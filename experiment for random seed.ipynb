{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12725025,"sourceType":"datasetVersion","datasetId":8043026},{"sourceId":12830626,"sourceType":"datasetVersion","datasetId":8114289},{"sourceId":12969760,"sourceType":"datasetVersion","datasetId":8126906,"isSourceIdPinned":true},{"sourceId":535993,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":416714,"modelId":434433}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tabpfn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:18:48.588729Z","iopub.execute_input":"2025-09-05T15:18:48.589467Z","iopub.status.idle":"2025-09-05T15:18:51.927185Z","shell.execute_reply.started":"2025-09-05T15:18:48.589445Z","shell.execute_reply":"2025-09-05T15:18:51.926412Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tabpfn in /usr/local/lib/python3.11/dist-packages (2.1.3)\nRequirement already satisfied: torch<3,>=2.1 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn<1.7,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (1.2.2)\nRequirement already satisfied: typing_extensions>=4.12.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (4.14.0)\nRequirement already satisfied: scipy<2,>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (1.15.3)\nRequirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (2.2.3)\nRequirement already satisfied: einops<0.9,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (0.8.1)\nRequirement already satisfied: huggingface-hub<1,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (0.33.1)\nRequirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (2.11.7)\nRequirement already satisfied: pydantic-settings>=2.10.1 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (2.10.1)\nRequirement already satisfied: eval-type-backport>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (0.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (2025.5.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (2.32.4)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (4.67.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (1.1.5)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->tabpfn) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->tabpfn) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->tabpfn) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->tabpfn) (2025.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->tabpfn) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->tabpfn) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->tabpfn) (0.4.1)\nRequirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings>=2.10.1->tabpfn) (1.1.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7,>=1.2.0->tabpfn) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7,>=1.2.0->tabpfn) (3.6.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.1->tabpfn) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas<3,>=1.4.0->tabpfn) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas<3,>=1.4.0->tabpfn) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas<3,>=1.4.0->tabpfn) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas<3,>=1.4.0->tabpfn) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas<3,>=1.4.0->tabpfn) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas<3,>=1.4.0->tabpfn) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->tabpfn) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.1->tabpfn) (3.0.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas<3,>=1.4.0->tabpfn) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas<3,>=1.4.0->tabpfn) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas<3,>=1.4.0->tabpfn) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas<3,>=1.4.0->tabpfn) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas<3,>=1.4.0->tabpfn) (2024.2.0)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import RobertaConfig, RobertaTokenizer, RobertaModel, get_linear_schedule_with_warmup\nimport torch.nn as nn\nimport xgboost as xgb\nfrom sklearn.metrics import classification_report, f1_score\nfrom torch.optim import AdamW\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:18:51.929027Z","iopub.execute_input":"2025-09-05T15:18:51.929287Z","iopub.status.idle":"2025-09-05T15:19:16.256139Z","shell.execute_reply.started":"2025-09-05T15:18:51.929263Z","shell.execute_reply":"2025-09-05T15:19:16.255345Z"}},"outputs":[{"name":"stderr","text":"2025-09-05 15:19:04.912196: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757085545.108019      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757085545.163337      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"pd.read_csv(\"/kaggle/input/dataset/df_org.csv\").columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:19:16.262219Z","iopub.execute_input":"2025-09-05T15:19:16.262499Z","iopub.status.idle":"2025-09-05T15:19:16.560194Z","shell.execute_reply.started":"2025-09-05T15:19:16.262471Z","shell.execute_reply":"2025-09-05T15:19:16.559604Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Index(['Unnamed: 0', 'project_name', 'project_version', 'label', 'code',\n       'code_comment', 'code_no_comment', 'sloc', 'proxy_indentation',\n       'mcCabe', 'nested_block_depth', 'mcClure', 'mcClure_NVAR',\n       'mcClure_NCOMP', 'difficulty', 'effort', 'maintainability_index',\n       'readability', 'fan_out', 'sloc_std', 'proxy_indentation_std',\n       'mcCabe_std', 'nested_block_depth_std', 'mcClure_std',\n       'mcClure_NVAR_std', 'mcClure_NCOMP_std', 'difficulty_std', 'effort_std',\n       'maintainability_index_std', 'readability_std', 'fan_out_std',\n       'sloc_norm', 'proxy_indentation_norm', 'mcCabe_norm',\n       'nested_block_depth_norm', 'mcClure_norm', 'mcClure_NVAR_norm',\n       'mcClure_NCOMP_norm', 'difficulty_norm', 'effort_norm',\n       'maintainability_index_norm', 'readability_norm', 'fan_out_norm',\n       'sloc_robust', 'proxy_indentation_robust', 'mcCabe_robust',\n       'nested_block_depth_robust', 'mcClure_robust', 'mcClure_NVAR_robust',\n       'mcClure_NCOMP_robust', 'difficulty_robust', 'effort_robust',\n       'maintainability_index_robust', 'readability_robust', 'fan_out_robust'],\n      dtype='object')"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# import pickle\n# import numpy as np\n\n# # Load the embeddings\n# with open(\"/kaggle/input/dataset-final/embeddings (1).pkl\", \"rb\") as f:\n#     data = pickle.load(f)\n\n# # Access train, valid, and test\n# X_train = data[\"train\"][\"X\"]\n# y_train = data[\"train\"][\"y\"]\n# X_valid = data[\"valid\"][\"X\"]\n# y_valid = data[\"valid\"][\"y\"]\n# X_test = data[\"test\"][\"X\"]\n# y_test = data[\"test\"][\"y\"]\n\n# # Combine train and valid to form trainval\n# X_trainval = np.concatenate([X_train, X_valid], axis=0)\n# y_trainval = np.concatenate([y_train, y_valid], axis=0)\n\n# print(f\"X_trainval shape: {X_trainval.shape}\")\n# print(f\"y_trainval shape: {y_trainval.shape}\")\n# print(f\"X_test shape: {X_test.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:19:16.561714Z","iopub.execute_input":"2025-09-05T15:19:16.562271Z","iopub.status.idle":"2025-09-05T15:19:16.565173Z","shell.execute_reply.started":"2025-09-05T15:19:16.562252Z","shell.execute_reply":"2025-09-05T15:19:16.564521Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# # Feature columns to use\n# feature_cols = [\n#     \"sloc_robust\", \"proxy_indentation_robust\", \"mcCabe_robust\",\n#     \"mcClure_robust\", \"nested_block_depth_robust\", \"difficulty_robust\",\n#     \"maintainability_index_robust\", \"fan_out_robust\", \"readability_robust\", \"effort_robust\"\n# ]\n\n# Load datasets\ntrain_df = pd.read_csv(\"/kaggle/input/dataset-final/train_dataset.csv\")\nvalid_df = pd.read_csv(\"/kaggle/input/dataset-final/valid_dataset.csv\")\ntest_df  = pd.read_csv(\"/kaggle/input/dataset-final/test_dataset.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:19:16.565938Z","iopub.execute_input":"2025-09-05T15:19:16.566187Z","iopub.status.idle":"2025-09-05T15:19:16.832396Z","shell.execute_reply.started":"2025-09-05T15:19:16.566161Z","shell.execute_reply":"2025-09-05T15:19:16.831798Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\nclass DataPreprocessor:\n    def __init__(self, base_url, synthetic_data_size=None, random_seed=42):\n        \"\"\"\n        Initialize the data preprocessor\n        \n        Args:\n            base_url (str): Base directory path for the dataset\n            synthetic_data_size (int, optional): Number of synthetic samples to use. \n                                               If None, use all available synthetic data.\n            random_seed (int): Random seed for reproducibility\n        \"\"\"\n        self.base_url = base_url\n        self.synthetic_data_size = synthetic_data_size\n        self.random_seed = random_seed\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        # Initialize attributes to store dataframes\n        self.df_org = None\n        self.df_syn = None\n        self.train_df = None\n        self.valid_df = None\n        self.test_df = None\n        \n    def load_data(self):\n        \"\"\"Load and preprocess the original and synthetic data\"\"\"\n        print(f\"Using device: {self.device}\")\n        \n        # Load data\n        self.df_org = pd.read_csv(f\"{self.base_url}/df_org.csv\")\n        self.df_syn = pd.read_csv(f\"{self.base_url}/df_syn.csv\")\n        \n        # Filter synthetic data for class 0 only and apply size limit if specified\n        df_syn_class0 = self.df_syn[self.df_syn['orig_label'] == 0].reset_index(drop=True)\n        \n        if self.synthetic_data_size is not None:\n            # Sample the specified number of synthetic examples\n            df_syn_class0 = df_syn_class0.sample(n=min(self.synthetic_data_size, len(df_syn_class0)), \n                                                random_state=self.random_seed)\n        \n        self.df_syn = df_syn_class0\n        \n        print(\"Original synthetic size:\", len(self.df_syn))\n        print(\"Filtered synthetic size (class 0 only):\", len(df_syn_class0))\n        \n        return self.df_org, self.df_syn\n    \n    def prepare_datasets(self):\n        \"\"\"\n        Prepare train, validation, and test datasets with the specified configuration\n        \n        Returns:\n            tuple: (train_df, valid_df, test_df) DataFrames\n        \"\"\"\n        if self.df_org is None or self.df_syn is None:\n            self.load_data()\n        \n        # ------------------------------\n        # Step 0: Prepare DataFrames\n        # ------------------------------\n        matched_codes = set(self.df_syn[\"orig_code\"].unique())\n        df_org_match = self.df_org[self.df_org[\"code\"].isin(matched_codes)].copy()\n        df_org_nonmatch = self.df_org[~self.df_org[\"code\"].isin(matched_codes)].copy()\n\n        # Add source column for original data\n        df_org_match[\"source\"] = \"original\"\n        df_org_nonmatch[\"source\"] = \"original\"\n\n        # Rename synthetic columns to match train_df and add source\n        df_syn_renamed = self.df_syn.rename(columns={\"synthetic_code\": \"code\", \"orig_label\": \"label\"})\n        df_syn_renamed[\"source\"] = \"synthetic\"\n\n        print(\"Total df_org:\", len(self.df_org))\n        print(\"df_org_match (has synthetic pair):\", len(df_org_match))\n        print(\"df_org_nonmatch (no synthetic pair):\", len(df_org_nonmatch))\n        print(\"Total df_syn:\", len(df_syn_renamed))\n\n        # ------------------------------\n        # Step 1: Split class 0 in non-matched\n        # ------------------------------\n        class0_df = df_org_nonmatch[df_org_nonmatch[\"label\"] == 0].copy()\n        nonclass0_df = df_org_nonmatch[df_org_nonmatch[\"label\"] != 0].copy()\n\n        # Ensure 41 rows each for test/val\n        class0_test = class0_df.sample(n=41, random_state=self.random_seed)\n        class0_val = class0_df.drop(class0_test.index).sample(n=41, random_state=self.random_seed)\n        class0_train = class0_df.drop(class0_test.index).drop(class0_val.index)\n\n        # ------------------------------\n        # Step 2: Split other classes in non-matched (stratified)\n        # ------------------------------\n        nonclass0_trainval, nonclass0_test = train_test_split(\n            nonclass0_df, test_size=0.15, random_state=self.random_seed, stratify=nonclass0_df[\"label\"]\n        )\n        nonclass0_train, nonclass0_val = train_test_split(\n            nonclass0_trainval, \n            test_size=0.1765,  # 0.1765 ≈ 0.15 / 0.85 to get total 15% val\n            random_state=self.random_seed, \n            stratify=nonclass0_trainval[\"label\"]\n        )\n\n        # ------------------------------\n        # Step 3: Combine splits\n        # ------------------------------\n        train_nonmatch = pd.concat([class0_train, nonclass0_train], ignore_index=True)\n        valid_df = pd.concat([class0_val, nonclass0_val], ignore_index=True)\n        test_df = pd.concat([class0_test, nonclass0_test], ignore_index=True)\n\n        # ------------------------------\n        # Step 4: Add matched + synthetic only to train\n        # ------------------------------\n        train_df = pd.concat([train_nonmatch, df_org_match, df_syn_renamed], ignore_index=True)\n\n        # ------------------------------\n        # Step 5: Store and return results\n        # ------------------------------\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.test_df = test_df\n        \n        # Print statistics\n        self._print_dataset_stats()\n        \n        return train_df, valid_df, test_df\n    \n    def _print_dataset_stats(self):\n        \"\"\"Print statistics about the prepared datasets\"\"\"\n        print(\"\\n\" + \"=\"*50)\n        print(\"DATASET STATISTICS\")\n        print(\"=\"*50)\n        \n        print(f\"Train size: {len(self.train_df)}\")\n        print(f\"Val size: {len(self.valid_df)}\")\n        print(f\"Test size: {len(self.test_df)}\")\n\n        print(\"\\nClass distribution:\")\n        print(\"Train:\\n\", self.train_df[\"label\"].value_counts().sort_index())\n        print(\"Val:\\n\", self.valid_df[\"label\"].value_counts().sort_index())\n        print(\"Test:\\n\", self.test_df[\"label\"].value_counts().sort_index())\n\n        print(\"\\nSource distribution in train:\")\n        print(self.train_df[\"source\"].value_counts())\n        print(\"=\"*50)\n    \n    def get_datasets(self):\n        \"\"\"Return the prepared datasets\"\"\"\n        if self.train_df is None:\n            self.prepare_datasets()\n        return self.train_df, self.valid_df, self.test_df\n    \n    def save_datasets(self, output_dir):\n        \"\"\"Save the prepared datasets to CSV files\"\"\"\n        if self.train_df is None:\n            self.prepare_datasets()\n        \n        os.makedirs(output_dir, exist_ok=True)\n        \n        self.train_df.to_csv(f\"{output_dir}/train_dataset.csv\", index=False)\n        self.valid_df.to_csv(f\"{output_dir}/valid_dataset.csv\", index=False)\n        self.test_df.to_csv(f\"{output_dir}/test_dataset.csv\", index=False)\n        \n        print(f\"Datasets saved to {output_dir}\")\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Initialize the preprocessor with custom parameters\n    preprocessor = DataPreprocessor(\n        base_url=\"/kaggle/input/dataset\",\n        synthetic_data_size=0,  # Use at mpst 350 synthetic samples\n        random_seed=42  # Different random seed\n    )\n    \n    # Load and prepare datasets\n    train_df, valid_df, test_df = preprocessor.prepare_datasets()\n    \n    # Alternatively, you can use the getter method\n    # train_df, valid_df, test_df = preprocessor.get_datasets()\n    \n    # Save datasets to files\n    preprocessor.save_datasets(\"./preprocessed_data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:19:17.035983Z","iopub.execute_input":"2025-09-05T15:19:17.036322Z","iopub.status.idle":"2025-09-05T15:19:17.767423Z","shell.execute_reply.started":"2025-09-05T15:19:17.036303Z","shell.execute_reply":"2025-09-05T15:19:17.766795Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nOriginal synthetic size: 0\nFiltered synthetic size (class 0 only): 0\nTotal df_org: 3342\ndf_org_match (has synthetic pair): 0\ndf_org_nonmatch (no synthetic pair): 3342\nTotal df_syn: 0\n\n==================================================\nDATASET STATISTICS\n==================================================\nTrain size: 2339\nVal size: 501\nTest size: 502\n\nClass distribution:\nTrain:\n label\n0     193\n1    1457\n2     203\n3     486\nName: count, dtype: int64\nVal:\n label\n0     41\n1    312\n2     44\n3    104\nName: count, dtype: int64\nTest:\n label\n0     41\n1    313\n2     44\n3    104\nName: count, dtype: int64\n\nSource distribution in train:\nsource\noriginal    2339\nName: count, dtype: int64\n==================================================\nDatasets saved to ./preprocessed_data\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def add_primary_key(df):\n    # Ensure project_name and project_version columns exist\n    if not {'project_name', 'project_version', 'code'}.issubset(df.columns):\n        raise ValueError(\"Columns 'project_name', 'project_version', 'code' must exist in DataFrame\")\n    \n    df['primary_key'] = df['project_name'].astype(str) + \"_\" + \\\n                        df['project_version'].astype(str) + \"_\" + \\\n                        df['code'].astype(str)\n    return df\n\n# Apply to all splits\ntrain_df = add_primary_key(train_df)\nvalid_df = add_primary_key(valid_df)\ntest_df = add_primary_key(test_df)\n\n# Check for duplicates using primary key\nprint(\"Duplicate primary keys in train:\", train_df['primary_key'].duplicated().sum())\nprint(\"Duplicate primary keys in val:\", valid_df['primary_key'].duplicated().sum())\nprint(\"Duplicate primary keys in test:\", test_df['primary_key'].duplicated().sum())\n\n# Check for overlap across splits using primary key\ntrain_keys = set(train_df['primary_key'])\nval_keys = set(valid_df['primary_key'])\ntest_keys = set(test_df['primary_key'])\n\nprint(\"Overlap train-val:\", len(train_keys & val_keys))\nprint(\"Overlap train-test:\", len(train_keys & test_keys))\nprint(\"Overlap val-test:\", len(val_keys & test_keys))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:19:17.768160Z","iopub.execute_input":"2025-09-05T15:19:17.768403Z","iopub.status.idle":"2025-09-05T15:19:17.784927Z","shell.execute_reply.started":"2025-09-05T15:19:17.768379Z","shell.execute_reply":"2025-09-05T15:19:17.784211Z"}},"outputs":[{"name":"stdout","text":"Duplicate primary keys in train: 0\nDuplicate primary keys in val: 0\nDuplicate primary keys in test: 0\nOverlap train-val: 0\nOverlap train-test: 0\nOverlap val-test: 0\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"train_df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:43:32.588532Z","iopub.execute_input":"2025-09-05T15:43:32.588849Z","iopub.status.idle":"2025-09-05T15:43:32.594424Z","shell.execute_reply.started":"2025-09-05T15:43:32.588829Z","shell.execute_reply":"2025-09-05T15:43:32.593788Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"Index(['Unnamed: 0', 'project_name', 'project_version', 'label', 'code',\n       'code_comment', 'code_no_comment', 'sloc', 'proxy_indentation',\n       'mcCabe', 'nested_block_depth', 'mcClure', 'mcClure_NVAR',\n       'mcClure_NCOMP', 'difficulty', 'effort', 'maintainability_index',\n       'readability', 'fan_out', 'sloc_std', 'proxy_indentation_std',\n       'mcCabe_std', 'nested_block_depth_std', 'mcClure_std',\n       'mcClure_NVAR_std', 'mcClure_NCOMP_std', 'difficulty_std', 'effort_std',\n       'maintainability_index_std', 'readability_std', 'fan_out_std',\n       'sloc_norm', 'proxy_indentation_norm', 'mcCabe_norm',\n       'nested_block_depth_norm', 'mcClure_norm', 'mcClure_NVAR_norm',\n       'mcClure_NCOMP_norm', 'difficulty_norm', 'effort_norm',\n       'maintainability_index_norm', 'readability_norm', 'fan_out_norm',\n       'sloc_robust', 'proxy_indentation_robust', 'mcCabe_robust',\n       'nested_block_depth_robust', 'mcClure_robust', 'mcClure_NVAR_robust',\n       'mcClure_NCOMP_robust', 'difficulty_robust', 'effort_robust',\n       'maintainability_index_robust', 'readability_robust', 'fan_out_robust',\n       'source', 'orig_code', 'orig_reason', 'synthetic_id', 'synthetic_label',\n       'primary_key'],\n      dtype='object')"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"import os, json\n\ndef save_df_as_jsonl(df, filepath):\n    # Use the *_robust columns instead of raw\n    required_cols = [\n        \"code\", \"label\",\n        \"sloc_robust\", \"proxy_indentation_robust\", \"mcCabe_robust\",\n        \"mcClure_robust\", \"nested_block_depth_robust\", \"difficulty_robust\",\n        \"maintainability_index_robust\", \"fan_out_robust\", \"readability_robust\", \"effort_robust\"\n    ]\n    for c in required_cols:\n        assert c in df.columns, f\"Missing column: {c}\"\n\n    with open(filepath, \"w\") as f:\n        for _, row in df.iterrows():\n            obj = {\n                \"code\": row[\"code\"],\n                \"label\": int(row[\"label\"]),\n                \"sloc\": float(row[\"sloc_robust\"]),\n                \"proxy_indentation\": float(row[\"proxy_indentation_robust\"]),\n                \"mcCabe\": float(row[\"mcCabe_robust\"]),\n                \"mcClure\": float(row[\"mcClure_robust\"]),\n                \"nested_block_depth\": float(row[\"nested_block_depth_robust\"]),\n                \"difficulty\": float(row[\"difficulty_robust\"]),\n                \"maintainability_index\": float(row[\"maintainability_index_robust\"]),\n                \"fan_out\": float(row[\"fan_out_robust\"]),\n                \"readability\": float(row[\"readability_robust\"]),\n                \"effort\": float(row[\"effort_robust\"]),\n            }\n            f.write(json.dumps(obj) + \"\\n\")\n\nos.makedirs(\"dataset_jsonl\", exist_ok=True)\nsave_df_as_jsonl(train_df, \"dataset_jsonl/train_scaled.jsonl\")\nsave_df_as_jsonl(valid_df, \"dataset_jsonl/valid_scaled.jsonl\")\nsave_df_as_jsonl(test_df, \"dataset_jsonl/test_scaled.jsonl\")\n\nprint(\"Saved JSONL files using *_robust features.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:19:17.802711Z","iopub.execute_input":"2025-09-05T15:19:17.802955Z","iopub.status.idle":"2025-09-05T15:19:18.084871Z","shell.execute_reply.started":"2025-09-05T15:19:17.802939Z","shell.execute_reply":"2025-09-05T15:19:18.084136Z"}},"outputs":[{"name":"stdout","text":"Saved JSONL files using *_robust features.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# import os, json\n\n# # === Step 2: Save raw features to JSONL ===\n# def save_df_as_jsonl_raw(df, filepath):\n#     # Use the original raw metric columns instead of *_robust\n#     required_cols = [\n#         \"code\", \"label\",\n#         \"sloc\", \"proxy_indentation\", \"mcCabe\",\n#         \"mcClure\", \"nested_block_depth\", \"difficulty\",\n#         \"maintainability_index\", \"fan_out\", \"readability\", \"effort\"\n#     ]\n#     for c in required_cols:\n#         assert c in df.columns, f\"Missing column: {c}\"\n\n#     with open(filepath, \"w\") as f:\n#         for _, row in df.iterrows():\n#             obj = {\n#                 \"code\": row[\"code\"],\n#                 \"label\": int(row[\"label\"]),\n#                 \"sloc\": float(row[\"sloc\"]),\n#                 \"proxy_indentation\": float(row[\"proxy_indentation\"]),\n#                 \"mcCabe\": float(row[\"mcCabe\"]),\n#                 \"mcClure\": float(row[\"mcClure\"]),\n#                 \"nested_block_depth\": float(row[\"nested_block_depth\"]),\n#                 \"difficulty\": float(row[\"difficulty\"]),\n#                 \"maintainability_index\": float(row[\"maintainability_index\"]),\n#                 \"fan_out\": float(row[\"fan_out\"]),\n#                 \"readability\": float(row[\"readability\"]),\n#                 \"effort\": float(row[\"effort\"]),\n#             }\n#             f.write(json.dumps(obj) + \"\\n\")\n\n# os.makedirs(\"dataset_jsonl\", exist_ok=True)\n# save_df_as_jsonl_raw(train_df, \"dataset_jsonl/train_scaled.jsonl\")\n# save_df_as_jsonl_raw(valid_df, \"dataset_jsonl/valid_scaled.jsonl\")\n# save_df_as_jsonl_raw(test_df, \"dataset_jsonl/test_scaled.jsonl\")\n\n# print(\"Saved JSONL files using raw features only.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:19:18.085544Z","iopub.execute_input":"2025-09-05T15:19:18.085724Z","iopub.status.idle":"2025-09-05T15:19:18.089696Z","shell.execute_reply.started":"2025-09-05T15:19:18.085711Z","shell.execute_reply":"2025-09-05T15:19:18.089021Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# === Step 3: Define Dataset class for loading JSONL ===\nclass BugSeverityDataset(Dataset):\n    def __init__(self, file_path, tokenizer, block_size=512):\n        self.examples = []\n        with open(file_path, \"r\") as f:\n            for line in f:\n                js = json.loads(line.strip())\n                self.examples.append(js)\n        self.tokenizer = tokenizer\n        self.block_size = block_size\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, idx):\n        js = self.examples[idx]\n\n        # tokenize code\n        code = js[\"code\"]\n        code_tokens = self.tokenizer.tokenize(code)[: self.block_size - 2]\n        tokens = [self.tokenizer.cls_token] + code_tokens + [self.tokenizer.eos_token]\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n        padding_length = self.block_size - len(input_ids)\n        input_ids += [self.tokenizer.pad_token_id] * padding_length\n        input_ids = torch.tensor(input_ids)\n\n        # numeric features (now using full column names)\n        num_features = torch.tensor([\n            js[\"sloc\"],\n            js[\"proxy_indentation\"],\n            js[\"mcCabe\"],\n            js[\"mcClure\"],\n            js[\"nested_block_depth\"],\n            js[\"difficulty\"],\n            js[\"maintainability_index\"],\n            js[\"fan_out\"],\n            js[\"readability\"],\n            js[\"effort\"]\n        ], dtype=torch.float)\n\n        label = torch.tensor(js[\"label\"], dtype=torch.long)\n\n        return input_ids, num_features, label\n        # return input_ids, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:19:18.090403Z","iopub.execute_input":"2025-09-05T15:19:18.090605Z","iopub.status.idle":"2025-09-05T15:19:18.108519Z","shell.execute_reply.started":"2025-09-05T15:19:18.090590Z","shell.execute_reply":"2025-09-05T15:19:18.107944Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# from torch.utils.data import DataLoader\n# from transformers import RobertaTokenizer, RobertaModel, RobertaConfig, get_linear_schedule_with_warmup\n# from sklearn.metrics import classification_report, f1_score\n# from tqdm import tqdm\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # === Model Definition ===\n# class ConcatClsModel(nn.Module):\n#     def __init__(self, encoder, config):\n#         super().__init__()\n#         self.encoder = encoder\n#         self.config = config\n#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n#         self.out_layer = nn.Linear(config.hidden_size, config.num_labels)\n\n#     def forward(self, input_ids, labels=None):\n#         attention_mask = input_ids.ne(self.encoder.config.pad_token_id).long()\n#         outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n#         cls_embeds = outputs.last_hidden_state[:, 0, :]  # CLS token\n#         cls_embeds = self.dropout(cls_embeds)\n#         logits = self.out_layer(cls_embeds)\n#         probs = torch.softmax(logits, dim=-1)\n\n#         if labels is not None:\n#             loss_fct = nn.CrossEntropyLoss()\n#             loss = loss_fct(logits, labels)\n#             return loss, probs\n#         else:\n#             return probs\n\n# # === Training and Evaluation functions ===\n# def train_epoch(model, dataloader, optimizer, scheduler):\n#     model.train()\n#     total_loss = 0\n#     for batch in tqdm(dataloader, desc=\"Training\"):\n#         input_ids, labels = [b.to(device) for b in batch]\n\n#         optimizer.zero_grad()\n#         loss, _ = model(input_ids, labels)\n\n#         loss.backward()\n#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n#         optimizer.step()\n#         scheduler.step()\n#         total_loss += loss.item()\n#     return total_loss / len(dataloader)\n\n\n# def evaluate(model, dataloader):\n#     model.eval()\n#     preds, labels_all = [], []\n#     total_loss = 0\n#     with torch.no_grad():\n#         for batch in tqdm(dataloader, desc=\"Evaluating\"):\n#             input_ids, labels = [b.to(device) for b in batch]\n#             loss, probs = model(input_ids, labels)\n#             total_loss += loss.item()\n#             preds.extend(torch.argmax(probs, dim=1).cpu().tolist())\n#             labels_all.extend(labels.cpu().tolist())\n\n#     avg_loss = total_loss / len(dataloader)\n#     f1 = f1_score(labels_all, preds, average=\"macro\")\n#     print(classification_report(labels_all, preds, digits=4))\n#     return avg_loss, f1\n\n\n# # === Load tokenizer and datasets ===\n# tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n# config = RobertaConfig.from_pretrained(\"microsoft/codebert-base\")\n\n# # Best Optuna parameters\n# best_params = {\n#     'lr': 4.818976027099782e-05,\n#     'weight_decay': 0.00010045918919119982,\n#     'warmup_ratio': 0.28606124459699783,\n#     'dropout': 0.16316509043013103,\n#     'epochs': 8,\n# }\n\n# config.num_labels = 4\n# config.hidden_dropout_prob = best_params['dropout']\n\n# train_dataset = BugSeverityDataset(\"dataset_jsonl/train_scaled.jsonl\", tokenizer)\n# valid_dataset = BugSeverityDataset(\"dataset_jsonl/valid_scaled.jsonl\", tokenizer)\n\n# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n# valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=2)\n\n# # === Instantiate model, optimizer, scheduler ===\n# encoder = RobertaModel.from_pretrained(\"microsoft/codebert-base\", config=config, add_pooling_layer=False)\n# model = ConcatClsModel(encoder, config).to(device)\n\n# optimizer = AdamW(\n#     model.parameters(),\n#     lr=best_params['lr'],\n#     weight_decay=best_params['weight_decay'],\n# )\n\n# total_steps = len(train_loader) * best_params['epochs']\n# scheduler = get_linear_schedule_with_warmup(\n#     optimizer,\n#     num_warmup_steps=int(total_steps * best_params['warmup_ratio']),\n#     num_training_steps=total_steps\n# )\n\n# # === Training loop ===\n# best_f1 = 0\n# for epoch in range(best_params['epochs']):\n#     train_loss = train_epoch(model, train_loader, optimizer, scheduler)\n#     val_loss, val_f1 = evaluate(model, valid_loader)\n#     print(f\"Epoch {epoch+1}/{best_params['epochs']} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | Val F1: {val_f1:.6f}\")\n\n#     # Save checkpoint\n#     torch.save(model.state_dict(), f\"concatcls_model_epoch{epoch+1}.pt\")\n\n#     if val_f1 > best_f1:\n#         best_f1 = val_f1\n#         torch.save(model.state_dict(), \"best_concatcls_model.pt\")\n#         print(\"Saved best model.\")\n\n# print(f\"Training completed. Best validation F1: {best_f1:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:19:18.111727Z","iopub.execute_input":"2025-09-05T15:19:18.111951Z","iopub.status.idle":"2025-09-05T15:19:18.128789Z","shell.execute_reply.started":"2025-09-05T15:19:18.111937Z","shell.execute_reply":"2025-09-05T15:19:18.128128Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# === Model Definition ===\nclass ConcatClsModel(nn.Module):\n    def __init__(self, encoder, config):\n        super().__init__()\n        self.encoder = encoder\n        self.config = config\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.out_layer = nn.Linear(config.hidden_size + 10, config.num_labels)\n\n    def forward(self, input_ids, num_features, labels=None):\n        attention_mask = input_ids.ne(self.encoder.config.pad_token_id).long()\n        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        cls_embeds = outputs.last_hidden_state[:, 0, :]\n        concat = torch.cat((cls_embeds, num_features), dim=-1)\n        logits = self.out_layer(concat)\n        probs = torch.softmax(logits, dim=-1)\n\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits, labels)\n            return loss, probs\n        else:\n            return probs\n\n# === Training and Evaluation functions ===\ndef train_epoch(model, dataloader, optimizer, scheduler):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(dataloader, desc=\"Training\"):\n        input_ids, num_features, labels = [b.to(device) for b in batch]\n        optimizer.zero_grad()\n        loss, _ = model(input_ids, num_features, labels)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n\nfrom sklearn.metrics import classification_report\n\ndef evaluate(model, dataloader):\n    model.eval()\n    preds, labels_all = [], []\n    total_loss = 0\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            input_ids, num_features, labels = [b.to(device) for b in batch]\n            loss, probs = model(input_ids, num_features, labels)\n            total_loss += loss.item()\n            preds.extend(torch.argmax(probs, dim=1).cpu().tolist())\n            labels_all.extend(labels.cpu().tolist())\n    \n    avg_loss = total_loss / len(dataloader)\n    f1 = f1_score(labels_all, preds, average=\"macro\")\n    print(classification_report(labels_all, preds, digits=4))\n    return avg_loss, f1\n\n# === Load tokenizer and datasets ===\ntokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\nconfig = RobertaConfig.from_pretrained(\"microsoft/codebert-base\")\n\n# Best Optuna parameters\nbest_params = {\n    'lr': 4.818976027099782e-05,\n    'weight_decay': 0.00010045918919119982,\n    'warmup_ratio': 0.28606124459699783,\n    'dropout': 0.16316509043013103,\n    'epochs': 8,\n}\n\n\nconfig.num_labels = 4\nconfig.hidden_dropout_prob = best_params['dropout']\n\ntrain_dataset = BugSeverityDataset(\"dataset_jsonl/train_scaled.jsonl\", tokenizer)\nvalid_dataset = BugSeverityDataset(\"dataset_jsonl/valid_scaled.jsonl\", tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\nvalid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=2)\n\n# === Instantiate model, optimizer, scheduler ===\nencoder = RobertaModel.from_pretrained(\"microsoft/codebert-base\", config=config, add_pooling_layer=False)\nmodel = ConcatClsModel(encoder, config).to(device)\n\noptimizer = AdamW(\n    model.parameters(),\n    lr=best_params['lr'],\n    weight_decay=best_params['weight_decay'],\n)\n\ntotal_steps = len(train_loader) * best_params['epochs']\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=int(total_steps * best_params['warmup_ratio']),\n    num_training_steps=total_steps\n)\n\n# === Training loop ===\nbest_f1 = 0\nfor epoch in range(best_params['epochs']):\n    train_loss = train_epoch(model, train_loader, optimizer, scheduler)\n    val_loss, val_f1 = evaluate(model, valid_loader)\n    print(f\"Epoch {epoch+1}/{best_params['epochs']} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | Val F1: {val_f1:.6f}\")\n\n    # Save model checkpoint for every epoch\n    torch.save(model.state_dict(), f\"concatcls_model_epoch{epoch+1}.pt\")\n\n    # Optionally, still keep track of best model separately\n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        torch.save(model.state_dict(), \"best_concatcls_model.pt\")\n        print(\"Saved best model.\")\n\nprint(f\"Training completed. Best validation F1: {best_f1:.6f}\")\n\nprint(f\"Training completed. Best validation F1: {best_f1:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:19:18.129452Z","iopub.execute_input":"2025-09-05T15:19:18.129659Z","iopub.status.idle":"2025-09-05T15:40:29.929181Z","shell.execute_reply.started":"2025-09-05T15:19:18.129644Z","shell.execute_reply":"2025-09-05T15:40:29.928273Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b75da81b7ae845fabe11f25c7137bfa8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24b81706f8d2462387f1bffe5bfc38e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"314c55ffdada4aee92f77b2ebfd38740"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dcc436105dc4226b72b5f26fab139a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95c6638895794deb8e6a58b0cee8515b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44c3f34cb9a04e3daea1bd89f4e93954"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaModel: ['pooler.dense.bias', 'pooler.dense.weight']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8776bdcc0bba46d6b566cca276d4a5c1"}},"metadata":{}},{"name":"stderr","text":"\nTraining:   0%|          | 0/147 [00:00<?, ?it/s]\u001b[A\nTraining:   1%|          | 1/147 [00:02<05:52,  2.41s/it]\u001b[A\nTraining:   1%|▏         | 2/147 [00:03<03:49,  1.58s/it]\u001b[A\nTraining:   2%|▏         | 3/147 [00:04<03:10,  1.32s/it]\u001b[A\nTraining:   3%|▎         | 4/147 [00:05<02:51,  1.20s/it]\u001b[A\nTraining:   3%|▎         | 5/147 [00:06<02:39,  1.13s/it]\u001b[A\nTraining:   4%|▍         | 6/147 [00:07<02:33,  1.09s/it]\u001b[A\nTraining:   5%|▍         | 7/147 [00:08<02:28,  1.06s/it]\u001b[A\nTraining:   5%|▌         | 8/147 [00:09<02:25,  1.04s/it]\u001b[A\nTraining:   6%|▌         | 9/147 [00:10<02:22,  1.03s/it]\u001b[A\nTraining:   7%|▋         | 10/147 [00:11<02:20,  1.02s/it]\u001b[A\nTraining:   7%|▋         | 11/147 [00:12<02:18,  1.02s/it]\u001b[A\nTraining:   8%|▊         | 12/147 [00:13<02:16,  1.01s/it]\u001b[A\nTraining:   9%|▉         | 13/147 [00:14<02:15,  1.01s/it]\u001b[A\nTraining:  10%|▉         | 14/147 [00:15<02:14,  1.01s/it]\u001b[A\nTraining:  10%|█         | 15/147 [00:16<02:13,  1.01s/it]\u001b[A\nTraining:  11%|█         | 16/147 [00:17<02:12,  1.01s/it]\u001b[A\nTraining:  12%|█▏        | 17/147 [00:18<02:11,  1.01s/it]\u001b[A\nTraining:  12%|█▏        | 18/147 [00:19<02:09,  1.01s/it]\u001b[A\nTraining:  13%|█▎        | 19/147 [00:20<02:08,  1.01s/it]\u001b[A\nTraining:  14%|█▎        | 20/147 [00:21<02:07,  1.01s/it]\u001b[A\nTraining:  14%|█▍        | 21/147 [00:22<02:06,  1.01s/it]\u001b[A\nTraining:  15%|█▍        | 22/147 [00:23<02:05,  1.01s/it]\u001b[A\nTraining:  16%|█▌        | 23/147 [00:24<02:04,  1.01s/it]\u001b[A\nTraining:  16%|█▋        | 24/147 [00:25<02:03,  1.01s/it]\u001b[A\nTraining:  17%|█▋        | 25/147 [00:26<02:02,  1.01s/it]\u001b[A\nTraining:  18%|█▊        | 26/147 [00:27<02:01,  1.01s/it]\u001b[A\nTraining:  18%|█▊        | 27/147 [00:28<02:00,  1.01s/it]\u001b[A\nTraining:  19%|█▉        | 28/147 [00:29<01:59,  1.01s/it]\u001b[A\nTraining:  20%|█▉        | 29/147 [00:30<01:58,  1.01s/it]\u001b[A\nTraining:  20%|██        | 30/147 [00:31<01:57,  1.01s/it]\u001b[A\nTraining:  21%|██        | 31/147 [00:32<01:56,  1.01s/it]\u001b[A\nTraining:  22%|██▏       | 32/147 [00:33<01:55,  1.00s/it]\u001b[A\nTraining:  22%|██▏       | 33/147 [00:34<01:54,  1.00s/it]\u001b[A\nTraining:  23%|██▎       | 34/147 [00:35<01:53,  1.01s/it]\u001b[A\nTraining:  24%|██▍       | 35/147 [00:36<01:52,  1.01s/it]\u001b[A\nTraining:  24%|██▍       | 36/147 [00:37<01:51,  1.00s/it]\u001b[A\nTraining:  25%|██▌       | 37/147 [00:38<01:50,  1.01s/it]\u001b[A\nTraining:  26%|██▌       | 38/147 [00:39<01:49,  1.00s/it]\u001b[A\nTraining:  27%|██▋       | 39/147 [00:40<01:48,  1.01s/it]\u001b[A\nTraining:  27%|██▋       | 40/147 [00:41<01:47,  1.01s/it]\u001b[A\nTraining:  28%|██▊       | 41/147 [00:42<01:46,  1.01s/it]\u001b[A\nTraining:  29%|██▊       | 42/147 [00:43<01:45,  1.01s/it]\u001b[A\nTraining:  29%|██▉       | 43/147 [00:44<01:44,  1.01s/it]\u001b[A\nTraining:  30%|██▉       | 44/147 [00:45<01:43,  1.01s/it]\u001b[A\nTraining:  31%|███       | 45/147 [00:46<01:42,  1.01s/it]\u001b[A\nTraining:  31%|███▏      | 46/147 [00:47<01:41,  1.01s/it]\u001b[A\nTraining:  32%|███▏      | 47/147 [00:48<01:40,  1.01s/it]\u001b[A\nTraining:  33%|███▎      | 48/147 [00:49<01:39,  1.01s/it]\u001b[A\nTraining:  33%|███▎      | 49/147 [00:50<01:38,  1.01s/it]\u001b[A\nTraining:  34%|███▍      | 50/147 [00:51<01:37,  1.01s/it]\u001b[A\nTraining:  35%|███▍      | 51/147 [00:52<01:36,  1.01s/it]\u001b[A\nTraining:  35%|███▌      | 52/147 [00:53<01:35,  1.01s/it]\u001b[A\nTraining:  36%|███▌      | 53/147 [00:54<01:34,  1.01s/it]\u001b[A\nTraining:  37%|███▋      | 54/147 [00:55<01:33,  1.01s/it]\u001b[A\nTraining:  37%|███▋      | 55/147 [00:56<01:32,  1.01s/it]\u001b[A\nTraining:  38%|███▊      | 56/147 [00:57<01:31,  1.01s/it]\u001b[A\nTraining:  39%|███▉      | 57/147 [00:58<01:30,  1.01s/it]\u001b[A\nTraining:  39%|███▉      | 58/147 [00:59<01:29,  1.01s/it]\u001b[A\nTraining:  40%|████      | 59/147 [01:00<01:28,  1.01s/it]\u001b[A\nTraining:  41%|████      | 60/147 [01:01<01:27,  1.01s/it]\u001b[A\nTraining:  41%|████▏     | 61/147 [01:02<01:26,  1.01s/it]\u001b[A\nTraining:  42%|████▏     | 62/147 [01:03<01:25,  1.01s/it]\u001b[A\nTraining:  43%|████▎     | 63/147 [01:04<01:24,  1.01s/it]\u001b[A\nTraining:  44%|████▎     | 64/147 [01:05<01:23,  1.01s/it]\u001b[A\nTraining:  44%|████▍     | 65/147 [01:06<01:22,  1.01s/it]\u001b[A\nTraining:  45%|████▍     | 66/147 [01:07<01:21,  1.01s/it]\u001b[A\nTraining:  46%|████▌     | 67/147 [01:08<01:20,  1.01s/it]\u001b[A\nTraining:  46%|████▋     | 68/147 [01:09<01:19,  1.01s/it]\u001b[A\nTraining:  47%|████▋     | 69/147 [01:10<01:18,  1.01s/it]\u001b[A\nTraining:  48%|████▊     | 70/147 [01:11<01:17,  1.01s/it]\u001b[A\nTraining:  48%|████▊     | 71/147 [01:12<01:16,  1.01s/it]\u001b[A\nTraining:  49%|████▉     | 72/147 [01:13<01:15,  1.01s/it]\u001b[A\nTraining:  50%|████▉     | 73/147 [01:14<01:14,  1.01s/it]\u001b[A\nTraining:  50%|█████     | 74/147 [01:15<01:13,  1.01s/it]\u001b[A\nTraining:  51%|█████     | 75/147 [01:16<01:12,  1.01s/it]\u001b[A\nTraining:  52%|█████▏    | 76/147 [01:17<01:11,  1.01s/it]\u001b[A\nTraining:  52%|█████▏    | 77/147 [01:18<01:10,  1.01s/it]\u001b[A\nTraining:  53%|█████▎    | 78/147 [01:19<01:09,  1.01s/it]\u001b[A\nTraining:  54%|█████▎    | 79/147 [01:20<01:08,  1.01s/it]\u001b[A\nTraining:  54%|█████▍    | 80/147 [01:21<01:07,  1.01s/it]\u001b[A\nTraining:  55%|█████▌    | 81/147 [01:22<01:06,  1.01s/it]\u001b[A\nTraining:  56%|█████▌    | 82/147 [01:23<01:05,  1.01s/it]\u001b[A\nTraining:  56%|█████▋    | 83/147 [01:24<01:04,  1.01s/it]\u001b[A\nTraining:  57%|█████▋    | 84/147 [01:25<01:03,  1.01s/it]\u001b[A\nTraining:  58%|█████▊    | 85/147 [01:27<01:02,  1.01s/it]\u001b[A\nTraining:  59%|█████▊    | 86/147 [01:28<01:01,  1.01s/it]\u001b[A\nTraining:  59%|█████▉    | 87/147 [01:29<01:00,  1.01s/it]\u001b[A\nTraining:  60%|█████▉    | 88/147 [01:30<00:59,  1.01s/it]\u001b[A\nTraining:  61%|██████    | 89/147 [01:31<00:58,  1.01s/it]\u001b[A\nTraining:  61%|██████    | 90/147 [01:32<00:57,  1.01s/it]\u001b[A\nTraining:  62%|██████▏   | 91/147 [01:33<00:56,  1.01s/it]\u001b[A\nTraining:  63%|██████▎   | 92/147 [01:34<00:55,  1.01s/it]\u001b[A\nTraining:  63%|██████▎   | 93/147 [01:35<00:54,  1.01s/it]\u001b[A\nTraining:  64%|██████▍   | 94/147 [01:36<00:53,  1.01s/it]\u001b[A\nTraining:  65%|██████▍   | 95/147 [01:37<00:52,  1.01s/it]\u001b[A\nTraining:  65%|██████▌   | 96/147 [01:38<00:51,  1.01s/it]\u001b[A\nTraining:  66%|██████▌   | 97/147 [01:39<00:50,  1.01s/it]\u001b[A\nTraining:  67%|██████▋   | 98/147 [01:40<00:49,  1.01s/it]\u001b[A\nTraining:  67%|██████▋   | 99/147 [01:41<00:48,  1.01s/it]\u001b[A\nTraining:  68%|██████▊   | 100/147 [01:42<00:47,  1.01s/it]\u001b[A\nTraining:  69%|██████▊   | 101/147 [01:43<00:46,  1.01s/it]\u001b[A\nTraining:  69%|██████▉   | 102/147 [01:44<00:45,  1.01s/it]\u001b[A\nTraining:  70%|███████   | 103/147 [01:45<00:44,  1.01s/it]\u001b[A\nTraining:  71%|███████   | 104/147 [01:46<00:43,  1.01s/it]\u001b[A\nTraining:  71%|███████▏  | 105/147 [01:47<00:42,  1.01s/it]\u001b[A\nTraining:  72%|███████▏  | 106/147 [01:48<00:41,  1.01s/it]\u001b[A\nTraining:  73%|███████▎  | 107/147 [01:49<00:40,  1.01s/it]\u001b[A\nTraining:  73%|███████▎  | 108/147 [01:50<00:39,  1.01s/it]\u001b[A\nTraining:  74%|███████▍  | 109/147 [01:51<00:38,  1.01s/it]\u001b[A\nTraining:  75%|███████▍  | 110/147 [01:52<00:37,  1.01s/it]\u001b[A\nTraining:  76%|███████▌  | 111/147 [01:53<00:36,  1.01s/it]\u001b[A\nTraining:  76%|███████▌  | 112/147 [01:54<00:35,  1.01s/it]\u001b[A\nTraining:  77%|███████▋  | 113/147 [01:55<00:34,  1.01s/it]\u001b[A\nTraining:  78%|███████▊  | 114/147 [01:56<00:33,  1.01s/it]\u001b[A\nTraining:  78%|███████▊  | 115/147 [01:57<00:32,  1.01s/it]\u001b[A\nTraining:  79%|███████▉  | 116/147 [01:58<00:31,  1.01s/it]\u001b[A\nTraining:  80%|███████▉  | 117/147 [01:59<00:30,  1.01s/it]\u001b[A\nTraining:  80%|████████  | 118/147 [02:00<00:29,  1.02s/it]\u001b[A\nTraining:  81%|████████  | 119/147 [02:01<00:28,  1.01s/it]\u001b[A\nTraining:  82%|████████▏ | 120/147 [02:02<00:27,  1.01s/it]\u001b[A\nTraining:  82%|████████▏ | 121/147 [02:03<00:26,  1.01s/it]\u001b[A\nTraining:  83%|████████▎ | 122/147 [02:04<00:25,  1.01s/it]\u001b[A\nTraining:  84%|████████▎ | 123/147 [02:05<00:24,  1.01s/it]\u001b[A\nTraining:  84%|████████▍ | 124/147 [02:06<00:23,  1.01s/it]\u001b[A\nTraining:  85%|████████▌ | 125/147 [02:07<00:22,  1.02s/it]\u001b[A\nTraining:  86%|████████▌ | 126/147 [02:08<00:21,  1.02s/it]\u001b[A\nTraining:  86%|████████▋ | 127/147 [02:09<00:20,  1.02s/it]\u001b[A\nTraining:  87%|████████▋ | 128/147 [02:10<00:19,  1.01s/it]\u001b[A\nTraining:  88%|████████▊ | 129/147 [02:11<00:18,  1.01s/it]\u001b[A\nTraining:  88%|████████▊ | 130/147 [02:12<00:17,  1.01s/it]\u001b[A\nTraining:  89%|████████▉ | 131/147 [02:13<00:16,  1.01s/it]\u001b[A\nTraining:  90%|████████▉ | 132/147 [02:14<00:15,  1.01s/it]\u001b[A\nTraining:  90%|█████████ | 133/147 [02:15<00:14,  1.01s/it]\u001b[A\nTraining:  91%|█████████ | 134/147 [02:16<00:13,  1.02s/it]\u001b[A\nTraining:  92%|█████████▏| 135/147 [02:17<00:12,  1.02s/it]\u001b[A\nTraining:  93%|█████████▎| 136/147 [02:18<00:11,  1.02s/it]\u001b[A\nTraining:  93%|█████████▎| 137/147 [02:19<00:10,  1.02s/it]\u001b[A\nTraining:  94%|█████████▍| 138/147 [02:20<00:09,  1.02s/it]\u001b[A\nTraining:  95%|█████████▍| 139/147 [02:21<00:08,  1.01s/it]\u001b[A\nTraining:  95%|█████████▌| 140/147 [02:22<00:07,  1.01s/it]\u001b[A\nTraining:  96%|█████████▌| 141/147 [02:23<00:06,  1.02s/it]\u001b[A\nTraining:  97%|█████████▋| 142/147 [02:24<00:05,  1.02s/it]\u001b[A\nTraining:  97%|█████████▋| 143/147 [02:25<00:04,  1.02s/it]\u001b[A\nTraining:  98%|█████████▊| 144/147 [02:26<00:03,  1.02s/it]\u001b[A\nTraining:  99%|█████████▊| 145/147 [02:27<00:02,  1.02s/it]\u001b[A\nTraining:  99%|█████████▉| 146/147 [02:28<00:01,  1.01s/it]\u001b[A\nTraining: 100%|██████████| 147/147 [02:29<00:00,  1.02s/it]\u001b[A\nEvaluating: 100%|██████████| 16/16 [00:08<00:00,  1.99it/s]\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0     0.0000    0.0000    0.0000        41\n           1     0.6582    1.0000    0.7939       312\n           2     0.9615    0.5682    0.7143        44\n           3     1.0000    0.0096    0.0190       104\n\n    accuracy                         0.6747       501\n   macro avg     0.6549    0.3944    0.3818       501\nweighted avg     0.7019    0.6747    0.5611       501\n\nEpoch 1/8 | Train Loss: 1.087892 | Val Loss: 0.885938 | Val F1: 0.381807\nSaved best model.\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 147/147 [02:28<00:00,  1.01s/it]\nEvaluating: 100%|██████████| 16/16 [00:08<00:00,  2.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0     1.0000    0.0488    0.0930        41\n           1     0.7163    0.9712    0.8245       312\n           2     0.8431    0.9773    0.9053        44\n           3     0.7600    0.1827    0.2946       104\n\n    accuracy                         0.7325       501\n   macro avg     0.8299    0.5450    0.5293       501\nweighted avg     0.7597    0.7325    0.6617       501\n\nEpoch 2/8 | Train Loss: 0.834063 | Val Loss: 0.709003 | Val F1: 0.529337\nSaved best model.\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 147/147 [02:28<00:00,  1.01s/it]\nEvaluating: 100%|██████████| 16/16 [00:07<00:00,  2.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0     0.6667    0.0976    0.1702        41\n           1     0.7284    0.9712    0.8324       312\n           2     0.9545    0.9545    0.9545        44\n           3     0.8000    0.2692    0.4029       104\n\n    accuracy                         0.7525       501\n   macro avg     0.7874    0.5731    0.5900       501\nweighted avg     0.7581    0.7525    0.6998       501\n\nEpoch 3/8 | Train Loss: 0.680948 | Val Loss: 0.637743 | Val F1: 0.590013\nSaved best model.\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 147/147 [02:28<00:00,  1.01s/it]\nEvaluating: 100%|██████████| 16/16 [00:08<00:00,  1.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0     0.4286    0.2195    0.2903        41\n           1     0.7520    0.9038    0.8210       312\n           2     0.9333    0.9545    0.9438        44\n           3     0.6333    0.3654    0.4634       104\n\n    accuracy                         0.7405       501\n   macro avg     0.6868    0.6108    0.6296       501\nweighted avg     0.7168    0.7405    0.7141       501\n\nEpoch 4/8 | Train Loss: 0.494826 | Val Loss: 0.667660 | Val F1: 0.629630\nSaved best model.\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 147/147 [02:28<00:00,  1.01s/it]\nEvaluating: 100%|██████████| 16/16 [00:08<00:00,  1.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0     0.6000    0.2927    0.3934        41\n           1     0.7752    0.8622    0.8164       312\n           2     0.9333    0.9545    0.9438        44\n           3     0.5506    0.4712    0.5078       104\n\n    accuracy                         0.7425       501\n   macro avg     0.7148    0.6451    0.6654       501\nweighted avg     0.7281    0.7425    0.7289       501\n\nEpoch 5/8 | Train Loss: 0.301629 | Val Loss: 0.819879 | Val F1: 0.665356\nSaved best model.\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 147/147 [02:28<00:00,  1.01s/it]\nEvaluating: 100%|██████████| 16/16 [00:08<00:00,  2.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0     0.5000    0.3171    0.3881        41\n           1     0.7692    0.8654    0.8145       312\n           2     0.9545    0.9545    0.9545        44\n           3     0.5375    0.4135    0.4674       104\n\n    accuracy                         0.7345       501\n   macro avg     0.6903    0.6376    0.6561       501\nweighted avg     0.7154    0.7345    0.7198       501\n\nEpoch 6/8 | Train Loss: 0.184354 | Val Loss: 1.042675 | Val F1: 0.656119\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 147/147 [02:28<00:00,  1.01s/it]\nEvaluating: 100%|██████████| 16/16 [00:08<00:00,  2.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0     0.5200    0.3171    0.3939        41\n           1     0.7738    0.9103    0.8365       312\n           2     0.9268    0.8636    0.8941        44\n           3     0.6471    0.4231    0.5116       104\n\n    accuracy                         0.7565       501\n   macro avg     0.7169    0.6285    0.6591       501\nweighted avg     0.7402    0.7565    0.7379       501\n\nEpoch 7/8 | Train Loss: 0.100069 | Val Loss: 1.333911 | Val F1: 0.659052\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 147/147 [02:28<00:00,  1.01s/it]\nEvaluating: 100%|██████████| 16/16 [00:07<00:00,  2.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0     0.5652    0.3171    0.4062        41\n           1     0.7676    0.9103    0.8328       312\n           2     0.9512    0.8864    0.9176        44\n           3     0.6119    0.3942    0.4795       104\n\n    accuracy                         0.7525       501\n   macro avg     0.7240    0.6270    0.6591       501\nweighted avg     0.7348    0.7525    0.7320       501\n\nEpoch 8/8 | Train Loss: 0.067334 | Val Loss: 1.410469 | Val F1: 0.659068\nTraining completed. Best validation F1: 0.665356\nTraining completed. Best validation F1: 0.665356\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import pandas as pd\n\n# Load dataset\ndf_org = pd.read_csv(\"/kaggle/input/dataset/df_org.csv\")\n\n# Summary statistics\nsummary_stats = df_org.describe().transpose()\n\n# Median, IQR, Skewness, Kurtosis for extra validation\nextra_stats = pd.DataFrame({\n    'median': df_org.median(numeric_only=True),\n    'iqr': df_org.quantile(0.75, numeric_only=True) - df_org.quantile(0.25, numeric_only=True),\n    'skewness': df_org.skew(numeric_only=True),\n    'kurtosis': df_org.kurtosis(numeric_only=True)\n})\n\n# Combine\nvalidation_stats = summary_stats.join(extra_stats)\n\n# Show results\nprint(\"Summary + RobustScaler validation stats:\")\nprint(validation_stats)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:40:29.930512Z","iopub.execute_input":"2025-09-05T15:40:29.931175Z","iopub.status.idle":"2025-09-05T15:40:30.179866Z","shell.execute_reply.started":"2025-09-05T15:40:29.931151Z","shell.execute_reply":"2025-09-05T15:40:30.179114Z"}},"outputs":[{"name":"stdout","text":"Summary + RobustScaler validation stats:\n                               count          mean           std        min  \\\nUnnamed: 0                    3342.0  1.670500e+03    964.896627   0.000000   \nlabel                         3342.0  1.420108e+00      0.907610   0.000000   \nsloc                          3342.0  2.355057e+01     32.646112   1.000000   \nproxy_indentation             3342.0  3.303695e+00      1.474554   0.000000   \nmcCabe                        3342.0  8.300718e+00     12.111707   1.000000   \nnested_block_depth            3342.0  5.902753e+00      7.764134   0.000000   \nmcClure                       3342.0  1.433782e+01     21.930488   0.000000   \nmcClure_NVAR                  3342.0  9.304608e+00     15.759168   0.000000   \nmcClure_NCOMP                 3342.0  5.033214e+00      8.657460   0.000000   \ndifficulty                    3342.0  1.577750e+01      5.089187   8.500000   \neffort                        3342.0  2.054473e+04  42361.734837  35.440000   \nmaintainability_index         3342.0  9.456344e+01     24.558902 -28.010000   \nreadability                   3342.0  3.226459e+01     11.129762   9.500000   \nfan_out                       3342.0  1.920557e+01     27.315344   1.000000   \nsloc_std                      3342.0  4.039591e-17      1.000150  -0.690862   \nproxy_indentation_std         3342.0  2.646995e-16      1.000150  -2.240807   \nmcCabe_std                    3342.0 -2.976540e-17      1.000150  -0.602872   \nnested_block_depth_std        3342.0  3.614371e-17      1.000150  -0.760373   \nmcClure_std                   3342.0 -8.504401e-18      1.000150  -0.653883   \nmcClure_NVAR_std              3342.0 -5.740471e-17      1.000150  -0.590513   \nmcClure_NCOMP_std             3342.0  3.614371e-17      1.000150  -0.581460   \ndifficulty_std                3342.0  1.945382e-16      1.000150  -1.430208   \neffort_std                    3342.0 -2.763930e-17      1.000150  -0.484219   \nmaintainability_index_std     3342.0 -3.072215e-16      1.000150  -4.991745   \nreadability_std               3342.0  8.663859e-17      1.000150  -2.045686   \nfan_out_std                   3342.0  1.807185e-17      1.000150  -0.666596   \nsloc_norm                     3342.0  4.246811e-02      0.061480   0.000000   \nproxy_indentation_norm        3342.0  3.201255e-01      0.142883   0.000000   \nmcCabe_norm                   3342.0  3.614217e-02      0.059959   0.000000   \nnested_block_depth_norm       3342.0  5.844310e-02      0.076873   0.000000   \nmcClure_norm                  3342.0  6.288518e-02      0.096186   0.000000   \nmcClure_NVAR_norm             3342.0  5.169227e-02      0.087551   0.000000   \nmcClure_NCOMP_norm            3342.0  3.355476e-02      0.057716   0.000000   \ndifficulty_norm               3342.0  1.488851e-01      0.104116   0.000000   \neffort_norm                   3342.0  3.493823e-02      0.072165   0.000000   \nmaintainability_index_norm    3342.0  6.405719e-01      0.128345   0.000000   \nreadability_norm              3342.0  1.351014e-01      0.066052   0.000000   \nfan_out_norm                  3342.0  4.376338e-02      0.065662   0.000000   \nsloc_robust                   3342.0  4.547890e-01      1.554577  -0.619048   \nproxy_indentation_robust      3342.0  1.377162e-01      0.715803  -1.466019   \nmcCabe_robust                 3342.0  4.125898e-01      1.513963  -0.500000   \nnested_block_depth_robust     3342.0  4.837921e-01      1.294022  -0.500000   \nmcClure_robust                3342.0  4.076568e-01      1.218360  -0.388889   \nmcClure_NVAR_robust           3342.0  5.304608e-01      1.575917  -0.400000   \nmcClure_NCOMP_robust          3342.0  5.055356e-01      1.442910  -0.333333   \ndifficulty_robust             3342.0  1.580809e-01      0.896378  -1.123734   \neffort_robust                 3342.0  7.490844e-01      2.387526  -0.406828   \nmaintainability_index_robust  3342.0 -2.340135e-02      0.708157  -3.557814   \nreadability_robust            3342.0  1.215842e-01      0.933509  -1.787796   \nfan_out_robust                3342.0  4.318719e-01      1.437650  -0.526316   \n\n                                      25%           50%           75%  \\\nUnnamed: 0                     835.250000  1.670500e+03   2505.750000   \nlabel                            1.000000  1.000000e+00      2.000000   \nsloc                             7.000000  1.400000e+01     28.000000   \nproxy_indentation                2.060000  3.020000e+00      4.120000   \nmcCabe                           2.000000  5.000000e+00     10.000000   \nnested_block_depth               1.000000  3.000000e+00      7.000000   \nmcClure                          0.000000  7.000000e+00     18.000000   \nmcClure_NVAR                     0.000000  4.000000e+00     10.000000   \nmcClure_NCOMP                    0.000000  2.000000e+00      6.000000   \ndifficulty                      12.340000  1.488000e+01     18.017500   \neffort                        2636.297500  7.253765e+03  20379.240000   \nmaintainability_index           78.032500  9.537500e+01    112.712500   \nreadability                     25.475000  3.081500e+01     37.397500   \nfan_out                          5.000000  1.100000e+01     24.000000   \nsloc_std                        -0.507045 -2.925922e-01      0.136313   \nproxy_indentation_std           -0.843565 -1.924229e-01      0.553677   \nmcCabe_std                      -0.520295 -2.725637e-01      0.140322   \nnested_block_depth_std          -0.631556 -3.739229e-01      0.141344   \nmcClure_std                     -0.653883 -3.346446e-01      0.167015   \nmcClure_NVAR_std                -0.590513 -3.366550e-01      0.044133   \nmcClure_NCOMP_std               -0.581460 -3.504108e-01      0.111688   \ndifficulty_std                  -0.675554 -1.763816e-01      0.440214   \neffort_std                      -0.422813 -3.137962e-01     -0.003907   \nmaintainability_index_std       -0.673215  3.305035e-02      0.739112   \nreadability_std                 -0.610130 -1.302637e-01      0.461257   \nfan_out_std                     -0.520136 -3.004463e-01      0.175548   \nsloc_norm                        0.011299  2.448211e-02      0.050847   \nproxy_indentation_norm           0.199612  2.926357e-01      0.399225   \nmcCabe_norm                      0.004950  1.980198e-02      0.044554   \nnested_block_depth_norm          0.009901  2.970297e-02      0.069307   \nmcClure_norm                     0.000000  3.070175e-02      0.078947   \nmcClure_NVAR_norm                0.000000  2.222222e-02      0.055556   \nmcClure_NCOMP_norm               0.000000  1.333333e-02      0.040000   \ndifficulty_norm                  0.078560  1.305237e-01      0.194712   \neffort_norm                      0.004431  1.229665e-02      0.034656   \nmaintainability_index_norm       0.554181  6.448132e-01      0.735419   \nreadability_norm                 0.094807  1.264985e-01      0.165564   \nfan_out_norm                     0.009615  2.403846e-02      0.055288   \nsloc_robust                     -0.333333  0.000000e+00      0.666667   \nproxy_indentation_robust        -0.466019  0.000000e+00      0.533981   \nmcCabe_robust                   -0.375000  0.000000e+00      0.625000   \nnested_block_depth_robust       -0.333333  0.000000e+00      0.666667   \nmcClure_robust                  -0.388889  0.000000e+00      0.611111   \nmcClure_NVAR_robust             -0.400000  0.000000e+00      0.600000   \nmcClure_NCOMP_robust            -0.333333  0.000000e+00      0.666667   \ndifficulty_robust               -0.447380  0.000000e+00      0.552620   \neffort_robust                   -0.260242  4.998172e-17      0.739758   \nmaintainability_index_robust    -0.500072  0.000000e+00      0.499928   \nreadability_robust              -0.447893  1.499994e-16      0.552107   \nfan_out_robust                  -0.315789  0.000000e+00      0.684211   \n\n                                        max        median           iqr  \\\nUnnamed: 0                      3341.000000  1.670500e+03   1670.500000   \nlabel                              3.000000  1.000000e+00      1.000000   \nsloc                             532.000000  1.400000e+01     21.000000   \nproxy_indentation                 10.320000  3.020000e+00      2.060000   \nmcCabe                           203.000000  5.000000e+00      8.000000   \nnested_block_depth               101.000000  3.000000e+00      6.000000   \nmcClure                          228.000000  7.000000e+00     18.000000   \nmcClure_NVAR                     180.000000  4.000000e+00     10.000000   \nmcClure_NCOMP                    150.000000  2.000000e+00      6.000000   \ndifficulty                        57.380000  1.488000e+01      5.677500   \neffort                        587051.030000  7.253765e+03  17742.942500   \nmaintainability_index            163.340000  9.537500e+01     34.680000   \nreadability                      178.000000  3.081500e+01     11.922500   \nfan_out                          417.000000  1.100000e+01     19.000000   \nsloc_std                          15.576909 -2.925922e-01      0.643358   \nproxy_indentation_std              4.758969 -1.924229e-01      1.397242   \nmcCabe_std                        16.077703 -2.725637e-01      0.660617   \nnested_block_depth_std            12.250108 -3.739229e-01      0.772900   \nmcClure_std                        9.744159 -3.346446e-01      0.820898   \nmcClure_NVAR_std                  10.833119 -3.366550e-01      0.634646   \nmcClure_NCOMP_std                 16.747232 -3.504108e-01      0.693148   \ndifficulty_std                     8.175908 -1.763816e-01      1.115768   \neffort_std                        13.375068 -3.137962e-01      0.418906   \nmaintainability_index_std          2.800893  3.305035e-02      1.412327   \nreadability_std                   13.096168 -1.302637e-01      1.071387   \nfan_out_std                       14.565219 -3.004463e-01      0.695684   \nsloc_norm                          1.000000  2.448211e-02      0.039548   \nproxy_indentation_norm             1.000000  2.926357e-01      0.199612   \nmcCabe_norm                        1.000000  1.980198e-02      0.039604   \nnested_block_depth_norm            1.000000  2.970297e-02      0.059406   \nmcClure_norm                       1.000000  3.070175e-02      0.078947   \nmcClure_NVAR_norm                  1.000000  2.222222e-02      0.055556   \nmcClure_NCOMP_norm                 1.000000  1.333333e-02      0.040000   \ndifficulty_norm                    1.000000  1.305237e-01      0.116152   \neffort_norm                        1.000000  1.229665e-02      0.030226   \nmaintainability_index_norm         1.000000  6.448132e-01      0.181239   \nreadability_norm                   1.000000  1.264985e-01      0.070757   \nfan_out_norm                       1.000000  2.403846e-02      0.045673   \nsloc_robust                       24.666667  0.000000e+00      1.000000   \nproxy_indentation_robust           3.543689  0.000000e+00      1.000000   \nmcCabe_robust                     24.750000  0.000000e+00      1.000000   \nnested_block_depth_robust         16.333333  0.000000e+00      1.000000   \nmcClure_robust                    12.277778  0.000000e+00      1.000000   \nmcClure_NVAR_robust               17.600000  0.000000e+00      1.000000   \nmcClure_NCOMP_robust              24.666667  0.000000e+00      1.000000   \ndifficulty_robust                  7.485689  0.000000e+00      1.000000   \neffort_robust                     32.677627  5.000883e-17      1.000000   \nmaintainability_index_robust       1.959775  0.000000e+00      1.000000   \nreadability_robust                12.345146  1.499994e-16      1.000000   \nfan_out_robust                    21.368421  0.000000e+00      1.000000   \n\n                              skewness   kurtosis  \nUnnamed: 0                    0.000000  -1.200000  \nlabel                         0.741702  -0.554648  \nsloc                          6.574731  77.782472  \nproxy_indentation             0.786996   0.982157  \nmcCabe                        6.944769  82.981664  \nnested_block_depth            3.445387  20.054275  \nmcClure                       3.440735  18.227495  \nmcClure_NVAR                  3.935479  23.386399  \nmcClure_NCOMP                 6.197947  67.179668  \ndifficulty                    1.616755   5.060572  \neffort                        6.415605  62.032379  \nmaintainability_index        -0.373744   0.393532  \nreadability                   2.929984  26.894979  \nfan_out                       6.408929  72.718587  \nsloc_std                      6.574731  77.782472  \nproxy_indentation_std         0.786996   0.982157  \nmcCabe_std                    6.944769  82.981664  \nnested_block_depth_std        3.445387  20.054275  \nmcClure_std                   3.440735  18.227495  \nmcClure_NVAR_std              3.935479  23.386399  \nmcClure_NCOMP_std             6.197947  67.179668  \ndifficulty_std                1.616755   5.060572  \neffort_std                    6.415605  62.032379  \nmaintainability_index_std    -0.373744   0.393532  \nreadability_std               2.929984  26.894979  \nfan_out_std                   6.408929  72.718587  \nsloc_norm                     6.574731  77.782472  \nproxy_indentation_norm        0.786996   0.982157  \nmcCabe_norm                   6.944769  82.981664  \nnested_block_depth_norm       3.445387  20.054275  \nmcClure_norm                  3.440735  18.227495  \nmcClure_NVAR_norm             3.935479  23.386399  \nmcClure_NCOMP_norm            6.197947  67.179668  \ndifficulty_norm               1.616755   5.060572  \neffort_norm                   6.415605  62.032379  \nmaintainability_index_norm   -0.373744   0.393532  \nreadability_norm              2.929984  26.894979  \nfan_out_norm                  6.408929  72.718587  \nsloc_robust                   6.574731  77.782472  \nproxy_indentation_robust      0.786996   0.982157  \nmcCabe_robust                 6.944769  82.981664  \nnested_block_depth_robust     3.445387  20.054275  \nmcClure_robust                3.440735  18.227495  \nmcClure_NVAR_robust           3.935479  23.386399  \nmcClure_NCOMP_robust          6.197947  67.179668  \ndifficulty_robust             1.616755   5.060572  \neffort_robust                 6.415605  62.032379  \nmaintainability_index_robust -0.373744   0.393532  \nreadability_robust            2.929984  26.894979  \nfan_out_robust                6.408929  72.718587  \n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"test_dataset = BugSeverityDataset(\"dataset_jsonl/test_scaled.jsonl\", tokenizer)\ntrain_dataset = BugSeverityDataset(\"dataset_jsonl/train_scaled.jsonl\", tokenizer)\nvalid_dataset = BugSeverityDataset(\"dataset_jsonl/valid_scaled.jsonl\", tokenizer)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:40:30.180681Z","iopub.execute_input":"2025-09-05T15:40:30.180931Z","iopub.status.idle":"2025-09-05T15:40:30.236512Z","shell.execute_reply.started":"2025-09-05T15:40:30.180914Z","shell.execute_reply":"2025-09-05T15:40:30.235939Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, matthews_corrcoef, confusion_matrix\n)\n\ndef evaluate_model_torch(model, dataloader, device, model_name=\"ConcatClsModel\"):\n    model.eval()\n    all_labels = []\n    all_preds = []\n    all_probs = []\n\n    total_loss = 0.0\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=f\"Evaluating {model_name}\"):\n            input_ids, num_features, labels = [b.to(device) for b in batch]\n            loss, probs = model(input_ids, num_features, labels)\n            total_loss += loss.item()\n\n            all_labels.append(labels.cpu())\n            all_preds.append(torch.argmax(probs, dim=1).cpu())\n            all_probs.append(probs.cpu())\n\n    # Concatenate all batches\n    y_true = torch.cat(all_labels).numpy()\n    y_pred = torch.cat(all_preds).numpy()\n    y_proba = torch.cat(all_probs).numpy()\n\n    # Compute all metrics (same as your original function)\n    accuracy = accuracy_score(y_true, y_pred)\n    precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n    recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n\n    precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n    recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n    f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n\n    try:\n        roc_auc_macro = roc_auc_score(y_true, y_proba, multi_class='ovr', average='macro')\n        roc_auc_weighted = roc_auc_score(y_true, y_proba, multi_class='ovr', average='weighted')\n    except Exception as e:\n        print(f\"Warning: ROC-AUC calculation failed: {e}\")\n        roc_auc_macro = np.nan\n        roc_auc_weighted = np.nan\n\n    mcc = matthews_corrcoef(y_true, y_pred)\n\n    cm = confusion_matrix(y_true, y_pred)\n    sensitivity = np.diag(cm) / np.maximum(np.sum(cm, axis=1), 1)\n    gmean = np.prod(sensitivity[sensitivity > 0]) ** (1.0 / len(sensitivity)) if np.any(sensitivity > 0) else 0\n\n    metrics = {\n        \"Model\": model_name,\n        \"Accuracy\": accuracy,\n        \"Precision_macro\": precision_macro,\n        \"Recall_macro\": recall_macro,\n        \"F1_macro\": f1_macro,\n        \"Precision_weighted\": precision_weighted,\n        \"Recall_weighted\": recall_weighted,\n        \"F1_weighted\": f1_weighted,\n        \"ROC-AUC_macro\": roc_auc_macro,\n        \"ROC-AUC_weighted\": roc_auc_weighted,\n        \"MCC\": mcc,\n        \"G-Mean\": gmean\n    }\n\n    print(f\"{model_name} evaluation completed.\")\n    return metrics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:40:30.237388Z","iopub.execute_input":"2025-09-05T15:40:30.237616Z","iopub.status.idle":"2025-09-05T15:40:30.247342Z","shell.execute_reply.started":"2025-09-05T15:40:30.237600Z","shell.execute_reply":"2025-09-05T15:40:30.246712Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"evaluate_model_torch(model, test_loader, device, model_name=\"ConcatClsModel\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:40:30.248051Z","iopub.execute_input":"2025-09-05T15:40:30.248235Z","iopub.status.idle":"2025-09-05T15:40:38.355241Z","shell.execute_reply.started":"2025-09-05T15:40:30.248219Z","shell.execute_reply":"2025-09-05T15:40:38.354385Z"}},"outputs":[{"name":"stderr","text":"Evaluating ConcatClsModel: 100%|██████████| 16/16 [00:08<00:00,  1.99it/s]","output_type":"stream"},{"name":"stdout","text":"ConcatClsModel evaluation completed.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"{'Model': 'ConcatClsModel',\n 'Accuracy': 0.7589641434262948,\n 'Precision_macro': 0.7482733544703883,\n 'Recall_macro': 0.6634851915597845,\n 'F1_macro': 0.688389092999288,\n 'Precision_weighted': 0.750595716773379,\n 'Recall_weighted': 0.7589641434262948,\n 'F1_weighted': 0.7459478380710991,\n 'ROC-AUC_macro': 0.8684090234492745,\n 'ROC-AUC_weighted': 0.8300869356228315,\n 'MCC': 0.5406277066561387,\n 'G-Mean': 0.6251023334850003}"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"# === Step 11: Load best model and extract embeddings for XGBoost ===\nmodel.load_state_dict(torch.load(\"/kaggle/working/best_concatcls_model.pt\"))\nmodel.eval()\n\ndef extract_embeddings(model, dataset):\n    embeddings = []\n    labels = []\n    model.eval()\n    with torch.no_grad():\n        for input_ids, num_features, label in tqdm(dataset, desc=\"Extracting embeddings\"):\n            input_ids = input_ids.unsqueeze(0).to(device)\n            num_features = num_features.unsqueeze(0).to(device)\n\n            # Forward pass through encoder\n            outputs = model.encoder(input_ids=input_ids, attention_mask=input_ids.ne(tokenizer.pad_token_id).long())\n            cls_embeds = outputs.last_hidden_state[:, 0, :].cpu().numpy().squeeze()  # (768,)\n\n            num_features_np = num_features.cpu().numpy().flatten()  # (10,)\n            combined = np.concatenate([cls_embeds, num_features_np])  # (778,)\n\n            embeddings.append(combined)\n            labels.append(label.item())\n\n    return np.array(embeddings), np.array(labels)\n\nprint(\"Extracting train embeddings...\")\nX_train, y_train = extract_embeddings(model, train_dataset)\nprint(\"Extracting valid embeddings...\")\nX_valid, y_valid = extract_embeddings(model, valid_dataset)\nprint(\"Extracting test embeddings...\")\nX_test, y_test = extract_embeddings(model, test_dataset)\n\n# Combine train + valid embeddings and labels for training\nX_trainval = np.vstack([X_train, X_valid]) \ny_trainval = np.concatenate([y_train, y_valid])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:40:38.356346Z","iopub.execute_input":"2025-09-05T15:40:38.356590Z","iopub.status.idle":"2025-09-05T15:41:49.787354Z","shell.execute_reply.started":"2025-09-05T15:40:38.356569Z","shell.execute_reply":"2025-09-05T15:41:49.786631Z"}},"outputs":[{"name":"stdout","text":"Extracting train embeddings...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 2339/2339 [00:49<00:00, 46.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting valid embeddings...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 501/501 [00:10<00:00, 47.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting test embeddings...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 502/502 [00:10<00:00, 47.32it/s]\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, matthews_corrcoef, confusion_matrix\n)\nimport pickle\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\nlogger = logging.getLogger(__name__)\n\n\ndef evaluate_model(y_true, y_pred, y_proba, model_name=\"XGBoost\"):\n    \"\"\"Evaluate model performance with multiple metrics\"\"\"\n    accuracy = accuracy_score(y_true, y_pred)\n    precision_macro = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n    recall_macro = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n    f1_macro = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n\n    precision_weighted = precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n    recall_weighted = recall_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n    f1_weighted = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n\n    try:\n        roc_auc_macro = roc_auc_score(y_true, y_proba, multi_class=\"ovr\", average=\"macro\")\n        roc_auc_weighted = roc_auc_score(y_true, y_proba, multi_class=\"ovr\", average=\"weighted\")\n    except Exception as e:\n        logger.warning(f\"ROC-AUC calculation failed for {model_name}: {e}\")\n        roc_auc_macro, roc_auc_weighted = np.nan, np.nan\n\n    mcc = matthews_corrcoef(y_true, y_pred)\n    cm = confusion_matrix(y_true, y_pred)\n    sensitivity = np.diag(cm) / np.maximum(np.sum(cm, axis=1), 1)\n    gmean = np.prod(sensitivity[sensitivity > 0]) ** (1.0 / len(sensitivity)) if np.any(sensitivity > 0) else 0\n\n    return {\n        \"Model\": model_name,\n        \"Accuracy\": accuracy,\n        \"Precision_macro\": precision_macro,\n        \"Recall_macro\": recall_macro,\n        \"F1_macro\": f1_macro,\n        \"Precision_weighted\": precision_weighted,\n        \"Recall_weighted\": recall_weighted,\n        \"F1_weighted\": f1_weighted,\n        \"ROC-AUC_macro\": roc_auc_macro,\n        \"ROC-AUC_weighted\": roc_auc_weighted,\n        \"MCC\": mcc,\n        \"G-Mean\": gmean,\n    }\n\n\ndef train_xgboost(X_train, y_train, X_test, y_test):\n    \"\"\"Train and evaluate XGBoost model\"\"\"\n    logger.info(\"Training XGBoost...\")\n\n    params_xgb = {\n        \"objective\": \"multi:softprob\",\n        \"num_class\": len(np.unique(y_train)),\n        \"eval_metric\": \"mlogloss\",\n        \"max_depth\": 9,\n        \"eta\": 0.24627429143007107,\n        \"subsample\": 0.45321841598276075,\n        \"colsample_bytree\": 0.7227038914198726,\n        \"lambda\": 0.06640744768945579,\n        \"alpha\": 0.21504472646446163,\n        \"tree_method\": \"gpu_hist\",\n        \"predictor\": \"gpu_predictor\",\n        \"verbosity\": 1,\n        \"use_label_encoder\": False,\n    }\n\n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    dtest = xgb.DMatrix(X_test, label=y_test)\n\n    bst = xgb.train(params_xgb, dtrain, num_boost_round=1200)\n\n    # Predictions\n    y_proba = bst.predict(dtest)\n    y_pred = np.argmax(y_proba, axis=1)\n\n    # Evaluate\n    results = evaluate_model(y_test, y_pred, y_proba)\n\n    # Save predictions\n    with open(\"xgboost_predictions.pkl\", \"wb\") as f:\n        pickle.dump({\"y_pred\": y_pred, \"y_proba\": y_proba}, f)\n\n    return results, y_pred, y_proba\n\n\ndef main():\n    # Load/define data before running\n    # X_train, y_train, X_test, y_test = load_your_data()\n\n    results, y_pred, y_proba = train_xgboost(X_train, y_train, X_test, y_test)\n    df_results = pd.DataFrame([results])\n    print(\"\\nXGBoost Performance Results:\")\n    print(df_results.round(4))\n    return df_results\n\n\nif __name__ == \"__main__\":\n    try:\n        results_df = main()\n        logger.info(\"Training and evaluation completed successfully\")\n    except Exception as e:\n        logger.error(f\"Execution failed: {e}\")\n        raise\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:42:25.695833Z","iopub.execute_input":"2025-09-05T15:42:25.696688Z","iopub.status.idle":"2025-09-05T15:42:31.425632Z","shell.execute_reply.started":"2025-09-05T15:42:25.696657Z","shell.execute_reply":"2025-09-05T15:42:31.424873Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [15:42:25] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [15:42:25] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"predictor\", \"use_label_encoder\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"\nXGBoost Performance Results:\n     Model  Accuracy  Precision_macro  Recall_macro  F1_macro  \\\n0  XGBoost     0.753           0.7371        0.6722     0.696   \n\n   Precision_weighted  Recall_weighted  F1_weighted  ROC-AUC_macro  \\\n0              0.7414            0.753       0.7416         0.8425   \n\n   ROC-AUC_weighted     MCC  G-Mean  \n0            0.8051  0.5306  0.6391  \n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [15:42:31] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# from transformers import RobertaTokenizer, RobertaModel\n# import torch\n# import numpy as np\n# from tqdm import tqdm\n\n# # Load raw CodeBERT (without ConcatCls finetuning)\n# tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n# codebert = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n# codebert.eval()\n# codebert.to(device)\n\n# def extract_raw_codebert_with_features(model, dataset):\n#     embeddings = []\n#     labels = []\n#     model.eval()\n#     with torch.no_grad():\n#         for input_ids, num_features, label in tqdm(dataset, desc=\"Extracting raw CodeBERT + features\"):\n#             input_ids = input_ids.unsqueeze(0).to(device)\n#             num_features = num_features.unsqueeze(0).to(device)\n\n#             # CodeBERT forward pass\n#             outputs = model(input_ids=input_ids, attention_mask=input_ids.ne(tokenizer.pad_token_id).long())\n#             cls_embeds = outputs.last_hidden_state[:, 0, :].cpu().numpy().squeeze()  # (768,)\n\n#             # Handcrafted features\n#             num_features_np = num_features.cpu().numpy().flatten()  # (10,)\n\n#             # Concatenate\n#             combined = np.concatenate([cls_embeds, num_features_np])  # (768 + 10 = 778,)\n#             embeddings.append(combined)\n#             labels.append(label.item())\n\n#     return np.array(embeddings), np.array(labels)\n\n# print(\"Extracting train embeddings (raw CodeBERT + num features)...\")\n# X_train, y_train = extract_raw_codebert_with_features(codebert, train_dataset)\n# print(\"Extracting valid embeddings (raw CodeBERT + num features)...\")\n# X_valid, y_valid = extract_raw_codebert_with_features(codebert, valid_dataset)\n# print(\"Extracting test embeddings (raw CodeBERT + num features)...\")\n# X_test, y_test = extract_raw_codebert_with_features(codebert, test_dataset)\n\n# # Combine train + valid for XGBoost training\n# X_trainval = np.vstack([X_train, X_valid])\n# y_trainval = np.concatenate([y_train, y_valid])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:41:49.788174Z","iopub.execute_input":"2025-09-05T15:41:49.788457Z","iopub.status.idle":"2025-09-05T15:41:49.792501Z","shell.execute_reply.started":"2025-09-05T15:41:49.788436Z","shell.execute_reply":"2025-09-05T15:41:49.791804Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"len(X_trainval[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:41:49.793206Z","iopub.execute_input":"2025-09-05T15:41:49.793426Z","iopub.status.idle":"2025-09-05T15:41:49.816307Z","shell.execute_reply.started":"2025-09-05T15:41:49.793405Z","shell.execute_reply":"2025-09-05T15:41:49.815776Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"778"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"import pickle\n\n# Save embeddings and labels\nwith open(\"raw_embeddings.pkl\", \"wb\") as f:\n    pickle.dump({\n        \"train\": {\"X\": X_train, \"y\": y_train},\n        \"valid\": {\"X\": X_valid, \"y\": y_valid},\n        \"test\": {\"X\": X_test, \"y\": y_test}\n    }, f)\n\nprint(\"Embeddings saved to embeddings.pkl\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:41:49.817011Z","iopub.execute_input":"2025-09-05T15:41:49.817243Z","iopub.status.idle":"2025-09-05T15:41:49.847519Z","shell.execute_reply.started":"2025-09-05T15:41:49.817226Z","shell.execute_reply":"2025-09-05T15:41:49.846780Z"}},"outputs":[{"name":"stdout","text":"Embeddings saved to embeddings.pkl\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# import optuna\n# from sklearn.model_selection import cross_val_score\n# from sklearn.metrics import make_scorer, f1_score, classification_report\n# from sklearn.neighbors import KNeighborsClassifier\n\n# def knn_objective(trial):\n#     n_neighbors = trial.suggest_int(\"n_neighbors\", 1, 50)\n#     weights = trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"])\n#     p = trial.suggest_int(\"p\", 1, 2)  # 1=Manhattan, 2=Euclidean\n\n#     knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, p=p)\n    \n#     # 5-fold CV on train+valid\n#     score = cross_val_score(\n#         knn, X_trainval, y_trainval, \n#         cv=5, scoring=make_scorer(f1_score, average='macro')\n#     ).mean()\n    \n#     return score\n\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(knn_objective, n_trials=50)\n\n# print(\"✅ Best KNN parameters:\", study.best_params)\n# print(\"✅ Best CV F1_macro (train+valid):\", study.best_value)\n\n# knn_best = KNeighborsClassifier(**study.best_params)\n# knn_best.fit(X_trainval, y_trainval)\n\n# y_pred_val = knn_best.predict(X_valid)\n# y_proba_val = knn_best.predict_proba(X_valid)\n\n# f1_val_macro = f1_score(y_valid, y_pred_val, average='macro')\n# print(\"=== KNN Validation Set Classification Report ===\")\n# print(classification_report(y_valid, y_pred_val, digits=4))\n# print(f\"F1 Macro on validation set: {f1_val_macro:.4f}\")\n\n# y_pred_test = knn_best.predict(X_test)\n# y_proba_test = knn_best.predict_proba(X_test)\n\n# f1_test_macro = f1_score(y_test, y_pred_test, average='macro')\n# print(\"=== KNN Test Set Classification Report ===\")\n# print(classification_report(y_test, y_pred_test, digits=4))\n# print(f\"F1 Macro on test set: {f1_test_macro:.4f}\")\n\n# import pickle\n# with open(\"knn_predictions.pkl\", \"wb\") as f:\n#     pickle.dump({\n#         \"validation\": {\"y_pred\": y_pred_val, \"y_proba\": y_proba_val},\n#         \"test\": {\"y_pred\": y_pred_test, \"y_proba\": y_proba_test},\n#         \"best_params\": study.best_params\n#     }, f)\n\n# print(\"✅ KNN predictions and best parameters saved to 'knn_predictions.pkl'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:41:49.848219Z","iopub.execute_input":"2025-09-05T15:41:49.848492Z","iopub.status.idle":"2025-09-05T15:41:49.856972Z","shell.execute_reply.started":"2025-09-05T15:41:49.848469Z","shell.execute_reply":"2025-09-05T15:41:49.856209Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.metrics import classification_report, accuracy_score, f1_score\n\n# best_rf_params = {\n#     'n_estimators': 335,\n#     'max_depth': 7,\n#     'min_samples_split': 15,\n#     'min_samples_leaf': 7,\n#     'max_features': 'sqrt'\n# }\n\n# rf_best = RandomForestClassifier(**best_rf_params, n_jobs=-1, random_state=42)\n# rf_best.fit(X_trainval, y_trainval)\n\n# y_pred_rf = rf_best.predict(X_test)\n\n# print(\"=== Random Forest Test Classification Report ===\")\n# print(classification_report(y_test, y_pred_rf, digits=4))\n\n# acc = accuracy_score(y_test, y_pred_rf)\n# f1_w = f1_score(y_test, y_pred_rf, average=\"weighted\")\n# print(f\"Accuracy: {acc:.4f}, Weighted F1: {f1_w:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:41:49.857764Z","iopub.execute_input":"2025-09-05T15:41:49.858223Z","iopub.status.idle":"2025-09-05T15:41:49.874409Z","shell.execute_reply.started":"2025-09-05T15:41:49.858199Z","shell.execute_reply":"2025-09-05T15:41:49.873635Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# import optuna\n# import xgboost as xgb\n# from sklearn.metrics import f1_score\n# import numpy as np\n\n# # Create DMatrix for XGBoost\n# dtrain = xgb.DMatrix(X_train, label=y_train)\n# dvalid = xgb.DMatrix(X_valid, label=y_valid)\n\n# def objective_xgb(trial):\n#     params = {\n#         \"objective\": \"multi:softprob\",\n#         \"num_class\": len(np.unique(y_trainval)),\n#         \"eval_metric\": \"mlogloss\",\n#         \"tree_method\": \"gpu_hist\",\n#         \"predictor\": \"gpu_predictor\",\n#         \"verbosity\": 0,\n#         \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n#         \"eta\": trial.suggest_float(\"eta\", 1e-3, 0.3, log=True),\n#         \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n#         \"lambda\": trial.suggest_float(\"lambda\", 1e-3, 10.0, log=True),\n#         \"alpha\": trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True)\n#     }\n    \n#     bst = xgb.train(\n#         params,\n#         dtrain,\n#         num_boost_round=1000,\n#         evals=[(dvalid, \"valid\")],\n#         early_stopping_rounds=50,\n#         verbose_eval=False\n#     )\n    \n#     y_pred_prob = bst.predict(dvalid)\n#     y_pred = np.argmax(y_pred_prob, axis=1)\n    \n#     # Weighted F1 as objective\n#     return f1_score(y_valid, y_pred, average=\"weighted\")\n\n# # Run Optuna study\n# study_xgb = optuna.create_study(direction=\"maximize\")\n# study_xgb.optimize(objective_xgb, n_trials=50, show_progress_bar=True)\n\n# # Best params\n# best_xgb_params = study_xgb.best_params\n# print(\"Best XGBoost params:\", best_xgb_params)\n\n# # Train final XGBoost on train+valid\n# dtrainval = xgb.DMatrix(X_trainval, label=y_trainval)\n# dtest = xgb.DMatrix(X_test, label=y_test)\n\n# bst_final = xgb.train(\n#     {**best_xgb_params, \"objective\": \"multi:softprob\", \"num_class\": len(np.unique(y_trainval)),\n#      \"tree_method\": \"gpu_hist\", \"predictor\": \"gpu_predictor\", \"verbosity\": 1},\n#     dtrainval,\n#     num_boost_round=1000,\n#     evals=[(dtrainval, \"train\")],\n#     verbose_eval=False\n# )\n\n# y_pred_prob_test = bst_final.predict(dtest)\n# y_pred_test = np.argmax(y_pred_prob_test, axis=1)\n\n# from sklearn.metrics import classification_report\n# print(\"=== XGBoost Test Classification Report ===\")\n# print(classification_report(y_test, y_pred_test, digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:41:49.875237Z","iopub.execute_input":"2025-09-05T15:41:49.875919Z","iopub.status.idle":"2025-09-05T15:41:49.889440Z","shell.execute_reply.started":"2025-09-05T15:41:49.875900Z","shell.execute_reply":"2025-09-05T15:41:49.888768Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, matthews_corrcoef, confusion_matrix\n)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.utils.class_weight import compute_class_weight\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\nfrom tabpfn import TabPFNClassifier\nfrom tqdm import tqdm\nimport pickle\nfrom typing import Dict, List, Tuple, Any\nimport logging\nfrom dataclasses import dataclass\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass ModelConfig:\n    \"\"\"Configuration class for model parameters\"\"\"\n    name: str\n    model: Any\n    params: Dict[str, Any] = None\n\n@dataclass\nclass PredictionResult:\n    \"\"\"Container for model predictions\"\"\"\n    y_pred: np.ndarray\n    y_proba: np.ndarray\n\nclass ModelEvaluator:\n    \"\"\"Handles model evaluation with consistent metrics\"\"\"\n    \n    @staticmethod\n    def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray, \n                      y_proba: np.ndarray, model_name: str) -> Dict[str, float]:\n        \"\"\"Evaluate model performance with multiple metrics\"\"\"\n        try:\n            accuracy = accuracy_score(y_true, y_pred)\n            precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n            recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n            f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n            \n            precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n            recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n            f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n            \n            try:\n                roc_auc_macro = roc_auc_score(y_true, y_proba, multi_class='ovr', average='macro')\n                roc_auc_weighted = roc_auc_score(y_true, y_proba, multi_class='ovr', average='weighted')\n            except Exception as e:\n                logger.warning(f\"ROC-AUC calculation failed for {model_name}: {e}\")\n                roc_auc_macro = np.nan\n                roc_auc_weighted = np.nan\n            \n            mcc = matthews_corrcoef(y_true, y_pred)\n            \n            cm = confusion_matrix(y_true, y_pred)\n            sensitivity = np.diag(cm) / np.maximum(np.sum(cm, axis=1), 1)  # Avoid division by zero\n            gmean = np.prod(sensitivity[sensitivity > 0]) ** (1.0 / len(sensitivity)) if np.any(sensitivity > 0) else 0\n\n            return {\n                \"Model\": model_name,\n                \"Accuracy\": accuracy,\n                \"Precision_macro\": precision_macro,\n                \"Recall_macro\": recall_macro,\n                \"F1_macro\": f1_macro,\n                \"Precision_weighted\": precision_weighted,\n                \"Recall_weighted\": recall_weighted,\n                \"F1_weighted\": f1_weighted,\n                \"ROC-AUC_macro\": roc_auc_macro,\n                \"ROC-AUC_weighted\": roc_auc_weighted,\n                \"MCC\": mcc,\n                \"G-Mean\": gmean\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error evaluating model {model_name}: {e}\")\n            raise\n\nclass ModelTrainer:\n    \"\"\"Handles model training and prediction\"\"\"\n    \n    def __init__(self, X_trainval: np.ndarray, y_trainval: np.ndarray, \n                 X_test: np.ndarray, y_test: np.ndarray):\n        self.X_trainval = X_trainval\n        self.y_trainval = y_trainval\n        self.X_test = X_test\n        self.y_test = y_test\n        self.n_classes = len(np.unique(y_trainval))\n        \n    def train_classical_models(self, model_configs: List[ModelConfig]) -> Tuple[List[Dict], Dict]:\n        \"\"\"Train classical sklearn models\"\"\"\n        results = []\n        test_predictions = {}\n        # train_predictions = {}  # Commented out to minimize cost\n        \n        for config in tqdm(model_configs, desc=\"Training classical models\"):\n            try:\n                logger.info(f\"Training {config.name}\")\n                model = config.model.set_params(**config.params) if config.params else config.model\n                model.fit(self.X_trainval, self.y_trainval)\n                \n                # Test predictions only (no train predictions to minimize cost)\n                y_pred_test = model.predict(self.X_test)\n                y_proba_test = model.predict_proba(self.X_test)\n                test_predictions[config.name] = PredictionResult(y_pred_test, y_proba_test)\n                \n                # # Train predictions - COMMENTED OUT TO MINIMIZE COST\n                # y_pred_train = model.predict(self.X_trainval)\n                # y_proba_train = model.predict_proba(self.X_trainval)\n                # train_predictions[config.name] = PredictionResult(y_pred_train, y_proba_train)\n                \n                # Evaluate\n                result = ModelEvaluator.evaluate_model(\n                    self.y_test, y_pred_test, y_proba_test, config.name\n                )\n                results.append(result)\n                \n            except Exception as e:\n                logger.error(f\"Error training {config.name}: {e}\")\n                continue\n                \n        return results, test_predictions  # Removed train_predictions from return\n    \n    def train_xgboost(self) -> Tuple[Dict, PredictionResult]:\n        \"\"\"Train XGBoost model\"\"\"\n        logger.info(\"Training XGBoost...\")\n        \n        params_xgb = {\n            \"objective\": \"multi:softprob\",\n            \"num_class\": self.n_classes,\n            \"eval_metric\": \"mlogloss\",\n            \"max_depth\": 9,\n            \"eta\": 0.24627429143007107,\n            \"subsample\": 0.45321841598276075,\n            \"colsample_bytree\": 0.7227038914198726,\n            \"lambda\": 0.06640744768945579,\n            \"alpha\": 0.21504472646446163,\n            \"tree_method\": \"gpu_hist\",\n            \"predictor\": \"gpu_predictor\",\n            \"verbosity\": 1,\n            \"use_label_encoder\": False\n        }\n        \n        dtrain = xgb.DMatrix(self.X_trainval, label=self.y_trainval)\n        dtest = xgb.DMatrix(self.X_test, label=self.y_test)\n        \n        bst = xgb.train(params_xgb, dtrain, num_boost_round=1200)\n        \n        # Test predictions only\n        y_proba_test = bst.predict(dtest)\n        y_pred_test = np.argmax(y_proba_test, axis=1)\n        test_pred = PredictionResult(y_pred_test, y_proba_test)\n        \n        # # Train predictions - COMMENTED OUT TO MINIMIZE COST\n        # y_proba_train = bst.predict(xgb.DMatrix(self.X_trainval))\n        # y_pred_train = np.argmax(y_proba_train, axis=1)\n        # train_pred = PredictionResult(y_pred_train, y_proba_train)\n        \n        # Evaluate\n        result = ModelEvaluator.evaluate_model(\n            self.y_test, y_pred_test, y_proba_test, \"XGBoost\"\n        )\n        \n        return result, test_pred  # Removed train_pred from return\n    \n    def train_lightgbm(self) -> Tuple[Dict, PredictionResult]:\n        \"\"\"Train LightGBM model\"\"\"\n        logger.info(\"Training LightGBM...\")\n        \n        params_lgb = {\n            \"objective\": \"multiclass\",\n            \"num_class\": self.n_classes,\n            \"metric\": \"multi_logloss\",\n            \"learning_rate\": 0.085,\n            \"max_depth\": 7,\n            \"verbosity\": -1\n        }\n        \n        train_data = lgb.Dataset(self.X_trainval, label=self.y_trainval)\n        gbm = lgb.train(params_lgb, train_data, num_boost_round=1200)\n        \n        # Test predictions only\n        y_proba_test = gbm.predict(self.X_test)\n        y_pred_test = np.argmax(y_proba_test, axis=1)\n        test_pred = PredictionResult(y_pred_test, y_proba_test)\n        \n        # # Train predictions - COMMENTED OUT TO MINIMIZE COST\n        # y_proba_train = gbm.predict(self.X_trainval)\n        # y_pred_train = np.argmax(y_proba_train, axis=1)\n        # train_pred = PredictionResult(y_pred_train, y_proba_train)\n        \n        # Evaluate\n        result = ModelEvaluator.evaluate_model(\n            self.y_test, y_pred_test, y_proba_test, \"LightGBM\"\n        )\n        \n        return result, test_pred  # Removed train_pred from return\n    \n    def train_catboost(self) -> Tuple[Dict, PredictionResult]:\n        \"\"\"Train CatBoost model\"\"\"\n        logger.info(\"Training CatBoost...\")\n        \n        classes = np.unique(self.y_trainval)\n        class_weights = compute_class_weight(\n            class_weight=\"balanced\", classes=classes, y=self.y_trainval\n        ).tolist()\n        \n        params_cat = {\n            \"learning_rate\": 0.093,\n            \"depth\": 7,\n            \"l2_leaf_reg\": 7.07,\n            \"iterations\": 829,\n            \"task_type\": \"GPU\",\n            \"verbose\": 100,\n            \"class_weights\": class_weights\n        }\n        \n        cat_model = cb.CatBoostClassifier(**params_cat)\n        cat_model.fit(self.X_trainval, self.y_trainval)\n        \n        # Test predictions only\n        y_proba_test = cat_model.predict_proba(self.X_test)\n        y_pred_test = cat_model.predict(self.X_test)\n        test_pred = PredictionResult(y_pred_test, y_proba_test)\n        \n        # # Train predictions - COMMENTED OUT TO MINIMIZE COST\n        # y_proba_train = cat_model.predict_proba(self.X_trainval)\n        # y_pred_train = cat_model.predict(self.X_trainval)\n        # train_pred = PredictionResult(y_pred_train, y_proba_train)\n        \n        # Evaluate\n        result = ModelEvaluator.evaluate_model(\n            self.y_test, y_pred_test, y_proba_test, \"CatBoost\"\n        )\n        \n        return result, test_pred  # Removed train_pred from return\n    \n    def train_tabpfn(self) -> Tuple[Dict, PredictionResult]:\n        \"\"\"Train TabPFN model\"\"\"\n        logger.info(\"Training TabPFN...\")\n        \n        tabpfn = TabPFNClassifier(ignore_pretraining_limits=True)\n        tabpfn.fit(self.X_trainval, self.y_trainval)\n        \n        # Test predictions only\n        y_proba_test = tabpfn.predict_proba(self.X_test)\n        y_pred_test = tabpfn.predict(self.X_test)\n        test_pred = PredictionResult(y_pred_test, y_proba_test)\n        \n        # # Train predictions - COMMENTED OUT TO MINIMIZE COST\n        # y_proba_train = tabpfn.predict_proba(self.X_trainval)\n        # y_pred_train = tabpfn.predict(self.X_trainval)\n        # train_pred = PredictionResult(y_pred_train, y_proba_train)\n        \n        # Evaluate\n        result = ModelEvaluator.evaluate_model(\n            self.y_test, y_pred_test, y_proba_test, \"TabPFN\"\n        )\n        \n        return result, test_pred  # Removed train_pred from return\n\nclass PredictionSaver:\n    \"\"\"Handles saving and loading predictions\"\"\"\n    \n    @staticmethod\n    def save_predictions(predictions: Dict[str, PredictionResult], filename: str):\n        \"\"\"Save predictions to file\"\"\"\n        try:\n            with open(filename, \"wb\") as f:\n                pickle.dump(predictions, f)\n            logger.info(f\"Predictions saved to {filename}\")\n        except Exception as e:\n            logger.error(f\"Error saving predictions to {filename}: {e}\")\n            raise\n\ndef main():\n    \"\"\"Main execution function\"\"\"\n    # Initialize data (assuming X_trainval, y_trainval, X_test, y_test are defined)\n    # X_trainval, y_trainval, X_test, y_test = load_your_data()\n    \n    trainer = ModelTrainer(X_trainval, y_trainval, X_test, y_test)\n    \n    # Define classical models\n    classical_models = [\n        ModelConfig(\"KNN\", KNeighborsClassifier(), \n                   {'n_neighbors': 3, 'weights': 'distance', 'p': 1}),\n        ModelConfig(\"SVM\", SVC(probability=True, random_state=42), \n                   {\"C\": 2.5, \"kernel\": \"rbf\", \"gamma\": \"scale\"}),\n        ModelConfig(\"Naive Bayes\", GaussianNB(), \n                   {\"var_smoothing\": 1e-8}),\n        ModelConfig(\"Decision Tree\", DecisionTreeClassifier(random_state=42), \n                   {\"max_depth\": 14, \"min_samples_split\": 4}),\n        ModelConfig(\"RandomForest\", RandomForestClassifier(random_state=42, n_jobs=-1), \n                   {\"n_estimators\": 600, \"max_depth\": 18, \"min_samples_split\": 3, \n                    \"class_weight\": \"balanced_subsample\"}),\n        ModelConfig(\"AdaBoost\", AdaBoostClassifier(random_state=42), \n                   {\"n_estimators\": 500, \"learning_rate\": 0.85})\n    ]\n    \n    # Train all models\n    results = []\n    all_test_predictions = {}\n    # all_train_predictions = {}  # Commented out to minimize cost\n    \n    # Classical models\n    classical_results, test_preds = trainer.train_classical_models(classical_models)\n    results.extend(classical_results)\n    all_test_predictions.update(test_preds)\n    # all_train_predictions.update(train_preds)  # Commented out\n    \n    # Gradient boosting models\n    for model_name, train_func in [\n        (\"XGBoost\", trainer.train_xgboost),\n        (\"LightGBM\", trainer.train_lightgbm),\n        (\"CatBoost\", trainer.train_catboost),\n        (\"TabPFN\", trainer.train_tabpfn)\n    ]:\n        try:\n            result, test_pred = train_func()\n            results.append(result)\n            all_test_predictions[model_name] = test_pred\n            # all_train_predictions[model_name] = train_pred  # Commented out\n        except Exception as e:\n            logger.error(f\"Error training {model_name}: {e}\")\n            continue\n    \n    # Create results DataFrame\n    df_results = pd.DataFrame(results)\n    print(\"\\nModel Performance Results:\")\n    print(df_results.round(4))\n    \n    # Save only test predictions\n    PredictionSaver.save_predictions(all_test_predictions, \"all_model_predictions.pkl\")\n    # PredictionSaver.save_predictions(all_train_predictions, \"all_train_predictions.pkl\")  # Commented out\n    \n    return df_results, all_test_predictions  # Removed all_train_predictions from return\n\nif __name__ == \"__main__\":\n    try:\n        results_df, test_preds = main()  # Removed train_preds from return\n        logger.info(\"Training and evaluation completed successfully\")\n    except Exception as e:\n        logger.error(f\"Main execution failed: {e}\")\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:41:49.890306Z","iopub.execute_input":"2025-09-05T15:41:49.890897Z","iopub.status.idle":"2025-09-05T15:42:17.102226Z","shell.execute_reply.started":"2025-09-05T15:41:49.890880Z","shell.execute_reply":"2025-09-05T15:42:17.101186Z"}},"outputs":[{"name":"stderr","text":"Training classical models:  67%|██████▋   | 4/6 [00:21<00:10,  5.37s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3843037236.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mresults_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Removed train_preds from return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training and evaluation completed successfully\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3843037236.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;31m# Classical models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0mclassical_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_classical_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassical_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassical_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0mall_test_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3843037236.py\u001b[0m in \u001b[0;36mtrain_classical_models\u001b[0;34m(self, model_configs)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training {config.name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_trainval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_trainval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;31m# Test predictions only (no train predictions to minimize cost)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    474\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":31},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# from sklearn.metrics import (\n#     accuracy_score, precision_score, recall_score, f1_score,\n#     roc_auc_score, matthews_corrcoef, confusion_matrix\n# )\n# from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.svm import SVC\n# from sklearn.naive_bayes import GaussianNB\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n# from sklearn.utils.class_weight import compute_class_weight\n# from scipy.stats import wilcoxon\n# import xgboost as xgb\n# import lightgbm as lgb\n# import catboost as cb\n# from tabpfn import TabPFNClassifier\n# from tqdm import tqdm\n# import pickle\n# from typing import Dict, List, Tuple, Any, Union\n# import logging\n# from dataclasses import dataclass\n# import itertools\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n\n# # Configure logging\n# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n# logger = logging.getLogger(__name__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:42:17.102877Z","iopub.status.idle":"2025-09-05T15:42:17.103162Z","shell.execute_reply.started":"2025-09-05T15:42:17.103004Z","shell.execute_reply":"2025-09-05T15:42:17.103018Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def main():\n#     \"\"\"Main execution function - modified for Wilcoxon analysis\"\"\"\n#     # Initialize data\n#     # X_trainval, y_trainval, X_test, y_test = load_your_data()\n    \n#     trainer = ModelTrainer(X_trainval, y_trainval, X_test, y_test)\n    \n#     # Define classical models\n#     classical_models = [\n#         ModelConfig(\"KNN\", KNeighborsClassifier(), \n#                    {'n_neighbors': 3, 'weights': 'distance', 'p': 1}),\n#         ModelConfig(\"SVM\", SVC(probability=True, random_state=42), \n#                    {\"C\": 2.5, \"kernel\": \"rbf\", \"gamma\": \"scale\"}),\n#         ModelConfig(\"Naive Bayes\", GaussianNB(), \n#                    {\"var_smoothing\": 1e-8}),\n#         ModelConfig(\"Decision Tree\", DecisionTreeClassifier(random_state=42), \n#                    {\"max_depth\": 14, \"min_samples_split\": 4}),\n#         ModelConfig(\"RandomForest\", RandomForestClassifier(random_state=42, n_jobs=-1), \n#                    {\"n_estimators\": 600, \"max_depth\": 18, \"min_samples_split\": 3, \n#                     \"class_weight\": \"balanced_subsample\"}),\n#         ModelConfig(\"AdaBoost\", AdaBoostClassifier(random_state=42), \n#                    {\"n_estimators\": 500, \"learning_rate\": 0.85})\n#     ]\n    \n#     # Train all models\n#     results = []\n#     all_test_predictions = {}\n#     all_true_labels = {}  # Store true labels for each model\n    \n#     # Store true test labels\n#     all_true_labels['test'] = trainer.y_test\n    \n#     # Classical models\n#     classical_results, test_preds = trainer.train_classical_models(classical_models)\n#     results.extend(classical_results)\n#     all_test_predictions.update(test_preds)\n    \n#     # Gradient boosting models\n#     for model_name, train_func in [\n#         (\"XGBoost\", trainer.train_xgboost),\n#         (\"LightGBM\", trainer.train_lightgbm),\n#         (\"CatBoost\", trainer.train_catboost),\n#         (\"TabPFN\", trainer.train_tabpfn)\n#     ]:\n#         try:\n#             result, test_pred = train_func()\n#             results.append(result)\n#             all_test_predictions[model_name] = test_pred\n#         except Exception as e:\n#             logger.error(f\"Error training {model_name}: {e}\")\n#             continue\n    \n#     # Create results DataFrame\n#     df_results = pd.DataFrame(results)\n#     print(\"\\nModel Performance Results:\")\n#     print(df_results.round(4))\n    \n#     # Save predictions and true labels for Wilcoxon analysis\n#     wilcoxon_data = {\n#         'predictions': all_test_predictions,\n#         'true_labels': all_true_labels,\n#         'results_df': df_results\n#     }\n    \n#     PredictionSaver.save_predictions(wilcoxon_data, \"wilcoxon_analysis_data.pkl\")\n    \n#     return df_results, wilcoxon_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:42:17.104144Z","iopub.status.idle":"2025-09-05T15:42:17.104450Z","shell.execute_reply.started":"2025-09-05T15:42:17.104294Z","shell.execute_reply":"2025-09-05T15:42:17.104307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from scipy.stats import wilcoxon\n# import itertools\n\n# class WilcoxonAnalyzer:\n#     \"\"\"Performs Wilcoxon signed-rank tests on model predictions\"\"\"\n    \n#     def __init__(self, wilcoxon_data: Dict):\n#         self.predictions = wilcoxon_data['predictions']\n#         self.true_labels = wilcoxon_data['true_labels']['test']\n#         self.results_df = wilcoxon_data['results_df']\n#         self.model_names = list(self.predictions.keys())\n    \n#     def calculate_accuracy_per_sample(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n#         \"\"\"Calculate accuracy for each sample (1 if correct, 0 if wrong)\"\"\"\n#         return (y_true == y_pred).astype(int)\n    \n#     def perform_wilcoxon_tests(self, metric: str = 'accuracy') -> pd.DataFrame:\n#         \"\"\"Perform Wilcoxon signed-rank tests between all model pairs\"\"\"\n        \n#         # Get sample-wise metrics for each model\n#         sample_metrics = {}\n#         for model_name, pred_result in self.predictions.items():\n#             if metric == 'accuracy':\n#                 sample_metrics[model_name] = self.calculate_accuracy_per_sample(\n#                     self.true_labels, pred_result.y_pred\n#                 )\n#             elif metric == 'probability':\n#                 # Use maximum probability as confidence measure\n#                 sample_metrics[model_name] = np.max(pred_result.y_proba, axis=1)\n#             else:\n#                 raise ValueError(\"Metric must be 'accuracy' or 'probability'\")\n        \n#         # Perform pairwise Wilcoxon tests\n#         wilcoxon_results = []\n#         model_pairs = list(itertools.combinations(self.model_names, 2))\n        \n#         for model1, model2 in tqdm(model_pairs, desc=\"Performing Wilcoxon tests\"):\n#             try:\n#                 # Get sample metrics for both models\n#                 metrics1 = sample_metrics[model1]\n#                 metrics2 = sample_metrics[model2]\n                \n#                 # Perform Wilcoxon signed-rank test\n#                 stat, p_value = wilcoxon(metrics1, metrics2, zero_method='pratt')\n                \n#                 # Calculate mean difference\n#                 mean_diff = np.mean(metrics1 - metrics2)\n                \n#                 wilcoxon_results.append({\n#                     'Model1': model1,\n#                     'Model2': model2,\n#                     'Wilcoxon_Statistic': stat,\n#                     'P_Value': p_value,\n#                     'Mean_Difference': mean_diff,\n#                     'Significant_0.05': p_value < 0.05,\n#                     'Significant_0.01': p_value < 0.01\n#                 })\n                \n#             except Exception as e:\n#                 logger.error(f\"Error in Wilcoxon test for {model1} vs {model2}: {e}\")\n#                 continue\n        \n#         return pd.DataFrame(wilcoxon_results)\n    \n#     def create_significance_matrix(self, wilcoxon_df: pd.DataFrame, alpha: float = 0.05) -> pd.DataFrame:\n#         \"\"\"Create a matrix showing significant differences between models\"\"\"\n#         models = sorted(self.model_names)\n#         sig_matrix = pd.DataFrame(0, index=models, columns=models)\n        \n#         for _, row in wilcoxon_df.iterrows():\n#             if row['P_Value'] < alpha:\n#                 if row['Mean_Difference'] > 0:\n#                     # Model1 is better\n#                     sig_matrix.loc[row['Model1'], row['Model2']] = 1\n#                     sig_matrix.loc[row['Model2'], row['Model1']] = -1\n#                 else:\n#                     # Model2 is better\n#                     sig_matrix.loc[row['Model1'], row['Model2']] = -1\n#                     sig_matrix.loc[row['Model2'], row['Model1']] = 1\n        \n#         return sig_matrix\n    \n#     def comprehensive_analysis(self) -> Dict[str, pd.DataFrame]:\n#         \"\"\"Perform comprehensive Wilcoxon analysis\"\"\"\n#         logger.info(\"Performing comprehensive Wilcoxon analysis...\")\n        \n#         # Test on accuracy differences\n#         accuracy_results = self.perform_wilcoxon_tests(metric='accuracy')\n#         accuracy_matrix = self.create_significance_matrix(accuracy_results)\n        \n#         # Test on confidence differences (optional)\n#         confidence_results = self.perform_wilcoxon_tests(metric='probability')\n#         confidence_matrix = self.create_significance_matrix(confidence_results)\n        \n#         return {\n#             'accuracy_wilcoxon': accuracy_results,\n#             'accuracy_significance_matrix': accuracy_matrix,\n#             'confidence_wilcoxon': confidence_results,\n#             'confidence_significance_matrix': confidence_matrix\n#         }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:42:17.105356Z","iopub.status.idle":"2025-09-05T15:42:17.105576Z","shell.execute_reply.started":"2025-09-05T15:42:17.105462Z","shell.execute_reply":"2025-09-05T15:42:17.105470Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Load your stored predictions\n# with open('/kaggle/working/all_model_predictions.pkl', 'rb') as f:\n#     predictions_data = pickle.load(f)\n\n# # Assuming your data has this structure:\n# # predictions_data = {\n# #     'predictions': {\n# #         'Model1': PredictionResult(y_pred, y_proba),\n# #         'Model2': PredictionResult(y_pred, y_proba),\n# #         ...\n# #     },\n# #     'true_labels': {\n# #         'test': y_test_actual\n# #     }\n# # }\n\n# # Perform Wilcoxon analysis\n# analyzer = WilcoxonAnalyzer(predictions_data)\n# analysis_results = analyzer.comprehensive_analysis()\n\n# # Display results\n# print(\"\\nWilcoxon Signed-Rank Test Results (Accuracy):\")\n# print(analysis_results['accuracy_wilcoxon'].round(4))\n\n# print(\"\\nSignificance Matrix (Accuracy):\")\n# print(analysis_results['accuracy_significance_matrix'])\n\n# print(\"\\nSignificant Differences at α=0.05:\")\n# sig_diffs = analysis_results['accuracy_wilcoxon'][\n#     analysis_results['accuracy_wilcoxon']['Significant_0.05']\n# ]\n# for _, row in sig_diffs.iterrows():\n#     better_model = row['Model1'] if row['Mean_Difference'] > 0 else row['Model2']\n#     worse_model = row['Model2'] if row['Mean_Difference'] > 0 else row['Model1']\n#     print(f\"{better_model} > {worse_model} (p={row['P_Value']:.4f})\")\n\n# # Save analysis results\n# with open('wilcoxon_analysis_results.pkl', 'wb') as f:\n#     pickle.dump(analysis_results, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:42:17.106956Z","iopub.status.idle":"2025-09-05T15:42:17.107190Z","shell.execute_reply.started":"2025-09-05T15:42:17.107074Z","shell.execute_reply":"2025-09-05T15:42:17.107086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import seaborn as sns\n\n# def visualize_wilcoxon_results(analysis_results: Dict):\n#     \"\"\"Visualize Wilcoxon test results\"\"\"\n    \n#     # Heatmap of significance matrix\n#     plt.figure(figsize=(12, 10))\n#     sns.heatmap(analysis_results['accuracy_significance_matrix'], \n#                 annot=True, cmap='coolwarm', center=0,\n#                 cbar_kws={'label': 'Significance (1=better, -1=worse)'})\n#     plt.title('Model Comparison Significance Matrix\\n(Wilcoxon Signed-Rank Test, α=0.05)')\n#     plt.tight_layout()\n#     plt.savefig('wilcoxon_significance_matrix.png', dpi=300, bbox_inches='tight')\n#     plt.show()\n    \n#     # P-value distribution\n#     plt.figure(figsize=(10, 6))\n#     plt.hist(analysis_results['accuracy_wilcoxon']['P_Value'], bins=20, alpha=0.7)\n#     plt.axvline(0.05, color='red', linestyle='--', label='α=0.05')\n#     plt.axvline(0.01, color='darkred', linestyle='--', label='α=0.01')\n#     plt.xlabel('P-value')\n#     plt.ylabel('Frequency')\n#     plt.title('Distribution of Wilcoxon Test P-values')\n#     plt.legend()\n#     plt.savefig('wilcoxon_pvalue_distribution.png', dpi=300, bbox_inches='tight')\n#     plt.show()\n\n# # Visualize results\n# visualize_wilcoxon_results(analysis_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:42:17.108151Z","iopub.status.idle":"2025-09-05T15:42:17.108381Z","shell.execute_reply.started":"2025-09-05T15:42:17.108273Z","shell.execute_reply":"2025-09-05T15:42:17.108283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# from scipy.stats import wilcoxon\n# import itertools\n# import pickle\n# from typing import Dict\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# from tqdm import tqdm\n\n# # Load your predictions\n# with open('all_model_predictions.pkl', 'rb') as f:\n#     predictions_data = pickle.load(f)\n\n# # Load your test labels (you should have this from your test set)\n# # y_test = ... (load your actual test labels)\n\n# class WilcoxonAnalyzer:\n#     \"\"\"Performs Wilcoxon signed-rank tests on model predictions\"\"\"\n    \n#     def __init__(self, predictions_dict: Dict, y_true: np.ndarray):\n#         self.predictions = predictions_dict\n#         self.true_labels = y_true\n#         self.model_names = list(self.predictions.keys())\n    \n#     def calculate_accuracy_per_sample(self, y_pred: np.ndarray) -> np.ndarray:\n#         \"\"\"Calculate accuracy for each sample (1 if correct, 0 if wrong)\"\"\"\n#         return (self.true_labels == y_pred).astype(int)\n    \n#     def perform_wilcoxon_tests(self, metric: str = 'accuracy') -> pd.DataFrame:\n#         \"\"\"Perform Wilcoxon signed-rank tests between all model pairs\"\"\"\n        \n#         # Get sample-wise metrics for each model\n#         sample_metrics = {}\n#         for model_name, pred_data in self.predictions.items():\n#             if metric == 'accuracy':\n#                 sample_metrics[model_name] = self.calculate_accuracy_per_sample(pred_data['y_pred'])\n#             elif metric == 'probability':\n#                 # Use maximum probability as confidence measure\n#                 sample_metrics[model_name] = np.max(pred_data['y_proba'], axis=1)\n#             else:\n#                 raise ValueError(\"Metric must be 'accuracy' or 'probability'\")\n        \n#         # Perform pairwise Wilcoxon tests\n#         wilcoxon_results = []\n#         model_pairs = list(itertools.combinations(self.model_names, 2))\n        \n#         for model1, model2 in tqdm(model_pairs, desc=\"Performing Wilcoxon tests\"):\n#             try:\n#                 # Get sample metrics for both models\n#                 metrics1 = sample_metrics[model1]\n#                 metrics2 = sample_metrics[model2]\n                \n#                 # Perform Wilcoxon signed-rank test\n#                 stat, p_value = wilcoxon(metrics1, metrics2, zero_method='pratt')\n                \n#                 # Calculate mean difference\n#                 mean_diff = np.mean(metrics1 - metrics2)\n                \n#                 wilcoxon_results.append({\n#                     'Model1': model1,\n#                     'Model2': model2,\n#                     'Wilcoxon_Statistic': stat,\n#                     'P_Value': p_value,\n#                     'Mean_Difference': mean_diff,\n#                     'Significant_0.05': p_value < 0.05,\n#                     'Significant_0.01': p_value < 0.01\n#                 })\n                \n#             except Exception as e:\n#                 print(f\"Error in Wilcoxon test for {model1} vs {model2}: {e}\")\n#                 continue\n        \n#         return pd.DataFrame(wilcoxon_results)\n    \n#     def create_significance_matrix(self, wilcoxon_df: pd.DataFrame, alpha: float = 0.05) -> pd.DataFrame:\n#         \"\"\"Create a matrix showing significant differences between models\"\"\"\n#         models = sorted(self.model_names)\n#         sig_matrix = pd.DataFrame(0, index=models, columns=models)\n        \n#         for _, row in wilcoxon_df.iterrows():\n#             if row['P_Value'] < alpha:\n#                 if row['Mean_Difference'] > 0:\n#                     # Model1 is better\n#                     sig_matrix.loc[row['Model1'], row['Model2']] = 1\n#                     sig_matrix.loc[row['Model2'], row['Model1']] = -1\n#                 else:\n#                     # Model2 is better\n#                     sig_matrix.loc[row['Model1'], row['Model2']] = -1\n#                     sig_matrix.loc[row['Model2'], row['Model1']] = 1\n        \n#         return sig_matrix\n    \n#     def comprehensive_analysis(self) -> Dict[str, pd.DataFrame]:\n#         \"\"\"Perform comprehensive Wilcoxon analysis\"\"\"\n#         print(\"Performing comprehensive Wilcoxon analysis...\")\n        \n#         # Test on accuracy differences\n#         accuracy_results = self.perform_wilcoxon_tests(metric='accuracy')\n#         accuracy_matrix = self.create_significance_matrix(accuracy_results)\n        \n#         # Test on confidence differences\n#         try:\n#             confidence_results = self.perform_wilcoxon_tests(metric='probability')\n#             confidence_matrix = self.create_significance_matrix(confidence_results)\n#         except Exception as e:\n#             print(f\"Probability-based Wilcoxon test not performed: {e}\")\n#             confidence_results = pd.DataFrame()\n#             confidence_matrix = pd.DataFrame()\n        \n#         return {\n#             'accuracy_wilcoxon': accuracy_results,\n#             'accuracy_significance_matrix': accuracy_matrix,\n#             'confidence_wilcoxon': confidence_results,\n#             'confidence_significance_matrix': confidence_matrix\n#         }\n\n# def visualize_wilcoxon_results(analysis_results: Dict, model_names: list):\n#     \"\"\"Visualize Wilcoxon test results\"\"\"\n    \n#     # Heatmap of significance matrix\n#     plt.figure(figsize=(12, 10))\n#     sns.heatmap(analysis_results['accuracy_significance_matrix'], \n#                 annot=True, cmap='coolwarm', center=0, fmt='d',\n#                 cbar_kws={'label': 'Significance (1=better, -1=worse)'})\n#     plt.title('Model Comparison Significance Matrix\\n(Wilcoxon Signed-Rank Test, α=0.05)')\n#     plt.tight_layout()\n#     plt.savefig('wilcoxon_significance_matrix.png', dpi=300, bbox_inches='tight')\n#     plt.show()\n    \n#     # P-value distribution\n#     plt.figure(figsize=(10, 6))\n#     plt.hist(analysis_results['accuracy_wilcoxon']['P_Value'], bins=20, alpha=0.7)\n#     plt.axvline(0.05, color='red', linestyle='--', label='α=0.05')\n#     plt.axvline(0.01, color='darkred', linestyle='--', label='α=0.01')\n#     plt.xlabel('P-value')\n#     plt.ylabel('Frequency')\n#     plt.title('Distribution of Wilcoxon Test P-values')\n#     plt.legend()\n#     plt.savefig('wilcoxon_pvalue_distribution.png', dpi=300, bbox_inches='tight')\n#     plt.show()\n\n# # Load your test labels (make sure you have this)\n# # y_test = np.load('y_test.npy')  # or however you stored it\n\n# # Initialize analyzer\n# analyzer = WilcoxonAnalyzer(predictions_data['all_model_predictions'], y_test)\n\n# # Perform comprehensive analysis\n# analysis_results = analyzer.comprehensive_analysis()\n\n# # Display results\n# print(\"\\n\" + \"=\"*60)\n# print(\"WILCOXON SIGNED-RANK TEST RESULTS\")\n# print(\"=\"*60)\n\n# print(\"\\nAccuracy-based Comparisons:\")\n# print(analysis_results['accuracy_wilcoxon'].round(4))\n\n# print(\"\\nSignificance Matrix (Accuracy, α=0.05):\")\n# print(analysis_results['accuracy_significance_matrix'])\n\n# print(\"\\nSIGNIFICANT DIFFERENCES AT α=0.05:\")\n# print(\"-\" * 40)\n# sig_diffs = analysis_results['accuracy_wilcoxon'][analysis_results['accuracy_wilcoxon']['Significant_0.05']]\n\n# if len(sig_diffs) > 0:\n#     for _, row in sig_diffs.iterrows():\n#         better_model = row['Model1'] if row['Mean_Difference'] > 0 else row['Model2']\n#         worse_model = row['Model2'] if row['Mean_Difference'] > 0 else row['Model1']\n#         significance_level = \"**\" if row['P_Value'] < 0.01 else \"*\"\n#         print(f\"{better_model} > {worse_model} {significance_level} (p={row['P_Value']:.4f}, diff={row['Mean_Difference']:.4f})\")\n# else:\n#     print(\"No statistically significant differences found at α=0.05\")\n\n# print(\"\\nHIGHLY SIGNIFICANT DIFFERENCES AT α=0.01:\")\n# print(\"-\" * 50)\n# highly_sig = analysis_results['accuracy_wilcoxon'][analysis_results['accuracy_wilcoxon']['Significant_0.01']]\n\n# if len(highly_sig) > 0:\n#     for _, row in highly_sig.iterrows():\n#         better_model = row['Model1'] if row['Mean_Difference'] > 0 else row['Model2']\n#         worse_model = row['Model2'] if row['Mean_Difference'] > 0 else row['Model1']\n#         print(f\"{better_model} >> {worse_model} ** (p={row['P_Value']:.4f}, diff={row['Mean_Difference']:.4f})\")\n# else:\n#     print(\"No highly significant differences found at α=0.01\")\n\n# # Visualize results\n# visualize_wilcoxon_results(analysis_results, analyzer.model_names)\n\n# # Save analysis results\n# with open('wilcoxon_analysis_results.pkl', 'wb') as f:\n#     pickle.dump(analysis_results, f)\n\n# print(\"\\nAnalysis complete! Results saved to 'wilcoxon_analysis_results.pkl'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:42:17.109487Z","iopub.status.idle":"2025-09-05T15:42:17.109791Z","shell.execute_reply.started":"2025-09-05T15:42:17.109607Z","shell.execute_reply":"2025-09-05T15:42:17.109622Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Let's investigate why Wilcoxon shows identical predictions when metrics differ\n# print(\"INVESTIGATING THE CONTRADICTION:\")\n# print(\"=\" * 50)\n\n# # Check if predictions are actually identical\n# model_names = list(all_test_predictions.keys())\n# all_identical = True\n\n# for i in range(len(model_names)):\n#     for j in range(i + 1, len(model_names)):\n#         model1, model2 = model_names[i], model_names[j]\n#         pred1 = all_test_predictions[model1].y_pred\n#         pred2 = all_test_predictions[model2].y_pred\n        \n#         if not np.array_equal(pred1, pred2):\n#             all_identical = False\n#             # Find where they differ\n#             diff_indices = np.where(pred1 != pred2)[0]\n#             print(f\"{model1} vs {model2}: DIFFER at {len(diff_indices)} samples\")\n#             if len(diff_indices) > 0:\n#                 print(f\"  First difference at sample {diff_indices[0]}: {pred1[diff_indices[0]]} vs {pred2[diff_indices[0]]}\")\n#         else:\n#             print(f\"{model1} vs {model2}: IDENTICAL predictions\")\n\n# if all_identical:\n#     print(\"\\n❌ CONTRADICTION: Models have different accuracy scores but identical predictions!\")\n#     print(\"This suggests an issue with the accuracy calculation or data handling\")\n# else:\n#     print(\"\\n✅ Models do make different predictions as expected\")\n\n# # Let's recalculate accuracy manually to verify\n# print(\"\\nVERIFYING ACCURACY CALCULATIONS:\")\n# print(\"=\" * 40)\n\n# for model_name, pred_result in all_test_predictions.items():\n#     manual_accuracy = np.mean(pred_result.y_pred == y_test)\n#     print(f\"{model_name:15}: Reported={results_df[results_df['Model']==model_name]['Accuracy'].values[0]:.4f}, Manual={manual_accuracy:.4f}\")\n\n# # Now let's perform a proper Wilcoxon test that actually detects differences\n# print(\"\\nPROPER WILCOXON ANALYSIS:\")\n# print(\"=\" * 30)\n\n# # Calculate accuracy per sample for each model\n# sample_accuracies = {}\n# for model_name, pred_result in all_test_predictions.items():\n#     sample_accuracies[model_name] = (pred_result.y_pred == y_test).astype(int)\n\n# # Perform Wilcoxon tests correctly\n# wilcoxon_results = []\n# model_pairs = list(itertools.combinations(model_names, 2))\n\n# for model1, model2 in model_pairs:\n#     acc1 = sample_accuracies[model1]\n#     acc2 = sample_accuracies[model2]\n    \n#     # Only perform test if there are differences\n#     if not np.array_equal(acc1, acc2):\n#         try:\n#             stat, p_value = wilcoxon(acc1, acc2, zero_method='pratt')\n#             mean_diff = np.mean(acc1 - acc2)\n            \n#             wilcoxon_results.append({\n#                 'Model1': model1,\n#                 'Model2': model2,\n#                 'Wilcoxon_Statistic': stat,\n#                 'P_Value': p_value,\n#                 'Mean_Difference': mean_diff,\n#                 'Significant_0.05': p_value < 0.05,\n#                 'Significant_0.01': p_value < 0.01\n#             })\n#         except:\n#             # If Wilcoxon fails, use a simpler approach\n#             mean_diff = np.mean(acc1 - acc2)\n#             wilcoxon_results.append({\n#                 'Model1': model1,\n#                 'Model2': model2,\n#                 'Wilcoxon_Statistic': np.nan,\n#                 'P_Value': 1.0,  # Conservative approach\n#                 'Mean_Difference': mean_diff,\n#                 'Significant_0.05': False,\n#                 'Significant_0.01': False\n#             })\n#     else:\n#         # Models have identical accuracy patterns\n#         wilcoxon_results.append({\n#             'Model1': model1,\n#             'Model2': model2,\n#             'Wilcoxon_Statistic': np.nan,\n#             'P_Value': 1.0,\n#             'Mean_Difference': 0.0,\n#             'Significant_0.05': False,\n#             'Significant_0.01': False\n#         })\n\n# # Convert to DataFrame\n# wilcoxon_df = pd.DataFrame(wilcoxon_results)\n\n# # Display significant results\n# print(\"\\nSIGNIFICANT DIFFERENCES (Proper Analysis):\")\n# print(\"=\" * 50)\n\n# sig_results = wilcoxon_df[wilcoxon_df['Significant_0.05']]\n# if len(sig_results) > 0:\n#     for _, row in sig_results.iterrows():\n#         better_model = row['Model1'] if row['Mean_Difference'] > 0 else row['Model2']\n#         worse_model = row['Model2'] if row['Mean_Difference'] > 0 else row['Model1']\n#         sig_level = \"**\" if row['Significant_0.01'] else \"*\"\n#         print(f\"{better_model} > {worse_model} {sig_level} (p={row['P_Value']:.4f}, diff={row['Mean_Difference']:.4f})\")\n# else:\n#     print(\"No statistically significant differences found at α=0.05\")\n\n# # Let's also check which models differ the most\n# print(\"\\nMODELS WITH LARGEST DIFFERENCES:\")\n# print(\"=\" * 35)\n\n# # Sort by absolute mean difference\n# diff_analysis = []\n# for model1, model2 in model_pairs:\n#     acc1 = sample_accuracies[model1]\n#     acc2 = sample_accuracies[model2]\n#     mean_diff = np.mean(acc1 - acc2)\n#     diff_count = np.sum(acc1 != acc2)\n#     diff_analysis.append({\n#         'Model1': model1,\n#         'Model2': model2,\n#         'Mean_Difference': mean_diff,\n#         'Different_Samples': diff_count,\n#         'Percentage_Different': diff_count / len(y_test) * 100\n#     })\n\n# diff_df = pd.DataFrame(diff_analysis)\n# diff_df = diff_df.reindex(diff_df['Mean_Difference'].abs().sort_values(ascending=False).index)\n\n# print(diff_df.head(10).round(4))\n\n# # Show the pair with maximum difference\n# max_diff = diff_df.iloc[0]\n# print(f\"\\nLargest difference: {max_diff['Model1']} vs {max_diff['Model2']}\")\n# print(f\"Mean difference: {max_diff['Mean_Difference']:.4f}\")\n# print(f\"Different on {max_diff['Different_Samples']} samples ({max_diff['Percentage_Different']:.2f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:42:17.111475Z","iopub.status.idle":"2025-09-05T15:42:17.111679Z","shell.execute_reply.started":"2025-09-05T15:42:17.111583Z","shell.execute_reply":"2025-09-05T15:42:17.111591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Since we can't trust the provided y_test, let's analyze model differences directly\n# print(\"ANALYZING MODEL DIFFERENCES (Without True Labels):\")\n# print(\"=\" * 50)\n\n# # Create a matrix of pairwise differences\n# model_names = list(all_test_predictions.keys())\n# n_models = len(model_names)\n# n_samples = len(all_test_predictions[model_names[0]].y_pred)\n\n# diff_matrix = pd.DataFrame(0, index=model_names, columns=model_names)\n# agreement_matrix = pd.DataFrame(0, index=model_names, columns=model_names)\n\n# for i, model1 in enumerate(model_names):\n#     for j, model2 in enumerate(model_names):\n#         if i == j:\n#             diff_matrix.loc[model1, model2] = 0\n#             agreement_matrix.loc[model1, model2] = n_samples\n#         else:\n#             pred1 = all_test_predictions[model1].y_pred\n#             pred2 = all_test_predictions[model2].y_pred\n#             diff_count = np.sum(pred1 != pred2)\n#             diff_matrix.loc[model1, model2] = diff_count\n#             agreement_matrix.loc[model1, model2] = n_samples - diff_count\n\n# print(\"Number of differing predictions between models:\")\n# print(diff_matrix)\n\n# print(f\"\\nPercentage agreement between models:\")\n# agreement_pct = (agreement_matrix / n_samples * 100).round(2)\n# print(agreement_pct)\n\n# # Find which models are most similar\n# print(\"\\nMOST SIMILAR MODEL PAIRS:\")\n# model_pairs = []\n# for i in range(n_models):\n#     for j in range(i + 1, n_models):\n#         model1, model2 = model_names[i], model_names[j]\n#         agreement = agreement_pct.loc[model1, model2]\n#         model_pairs.append((model1, model2, agreement))\n\n# # Sort by agreement (descending)\n# model_pairs.sort(key=lambda x: x[2], reverse=True)\n# for model1, model2, agreement in model_pairs[:10]:\n#     print(f\"{model1} & {model2}: {agreement}% agreement\")\n\n# print(\"\\nLEAST SIMILAR MODEL PAIRS:\")\n# for model1, model2, agreement in model_pairs[-10:]:\n#     print(f\"{model1} & {model2}: {agreement}% agreement\")\n\n# # The CatBoost issue is clear - it's very different from others\n# print(f\"\\nCatBoost seems to be the outlier:\")\n# for model in model_names:\n#     if model != 'CatBoost':\n#         agreement = agreement_pct.loc[model, 'CatBoost']\n#         print(f\"CatBoost vs {model}: {agreement}% agreement\")\n\n# # Check if CatBoost has a different data format\n# catboost_preds = all_test_predictions['CatBoost'].y_pred\n# print(f\"\\nCatBoost predictions shape: {catboost_preds.shape}\")\n# print(f\"CatBoost predictions type: {catboost_preds.dtype}\")\n# print(f\"CatBoost predictions sample: {catboost_preds[:5]}\")\n# print(f\"CatBoost predictions structure: {type(catboost_preds[0])}\")\n\n# # It seems CatBoost might be returning arrays instead of scalars\n# if hasattr(catboost_preds[0], '__len__') and len(catboost_preds[0]) > 0:\n#     print(\"CatBoost predictions are arrays, extracting first element...\")\n#     catboost_preds_fixed = np.array([x[0] if hasattr(x, '__len__') else x for x in catboost_preds])\n#     print(f\"Fixed CatBoost sample: {catboost_preds_fixed[:5]}\")\n    \n#     # Update the predictions\n#     all_test_predictions['CatBoost'] = PredictionResult(\n#         y_pred=catboost_preds_fixed, \n#         y_proba=all_test_predictions['CatBoost'].y_proba\n#     )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:42:17.113541Z","iopub.status.idle":"2025-09-05T15:42:17.114038Z","shell.execute_reply.started":"2025-09-05T15:42:17.113921Z","shell.execute_reply":"2025-09-05T15:42:17.113933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Fix CatBoost predictions\n# print(\"FIXING CatBoost PREDICTIONS:\")\n# catboost_preds = all_test_predictions['CatBoost'].y_pred\n# if len(catboost_preds.shape) == 2 and catboost_preds.shape[1] == 1:\n#     catboost_preds_fixed = catboost_preds.flatten()\n#     print(f\"Fixed CatBoost from shape {catboost_preds.shape} to {catboost_preds_fixed.shape}\")\n#     print(f\"Sample: {catboost_preds_fixed[:10]}\")\n    \n#     # Update the predictions\n#     all_test_predictions['CatBoost'] = PredictionResult(\n#         y_pred=catboost_preds_fixed, \n#         y_proba=all_test_predictions['CatBoost'].y_proba\n#     )\n\n# # Now let's try to find the correct y_test\n# print(\"\\nFINDING CORRECT TEST LABELS:\")\n# print(\"=\" * 40)\n\n# # Since we have model predictions, we can try to infer the true labels\n# # The most common prediction across models for each sample is likely the true label\n# n_samples = len(all_test_predictions['KNN'].y_pred)\n# inferred_labels = np.zeros(n_samples, dtype=int)\n\n# for i in range(n_samples):\n#     # Collect predictions from all models for sample i\n#     predictions_i = []\n#     for model_name, pred_result in all_test_predictions.items():\n#         if model_name != 'CatBoost':  # Exclude CatBoost for now since it might be wrong\n#             predictions_i.append(pred_result.y_pred[i])\n    \n#     # Use the most common prediction as inferred true label\n#     unique, counts = np.unique(predictions_i, return_counts=True)\n#     inferred_labels[i] = unique[np.argmax(counts)]\n\n# print(f\"Inferred labels shape: {inferred_labels.shape}\")\n# print(f\"Inferred labels unique: {np.unique(inferred_labels)}\")\n# print(f\"Inferred labels sample: {inferred_labels[:20]}\")\n\n# # Now let's calculate accuracy with inferred labels\n# print(\"\\nACCURACY WITH INFERRED LABELS:\")\n# print(\"=\" * 30)\n\n# for model_name, pred_result in all_test_predictions.items():\n#     accuracy = np.mean(pred_result.y_pred == inferred_labels)\n#     print(f\"{model_name:15}: {accuracy:.4f}\")\n\n# # These should be close to your reported accuracies\n# print(\"\\nCOMPARISON WITH REPORTED ACCURACIES:\")\n# print(\"=\" * 35)\n# reported_accuracies = {\n#     'KNN': 0.7968, 'SVM': 0.7908, 'Naive Bayes': 0.7530, \n#     'Decision Tree': 0.7390, 'RandomForest': 0.7928, 'AdaBoost': 0.6514,\n#     'XGBoost': 0.8008, 'LightGBM': 0.7968, 'CatBoost': 0.7869, 'TabPFN': 0.7829\n# }\n\n# for model_name in all_test_predictions.keys():\n#     inferred_acc = np.mean(all_test_predictions[model_name].y_pred == inferred_labels)\n#     reported_acc = reported_accuracies.get(model_name, 0)\n#     diff = abs(inferred_acc - reported_acc)\n#     print(f\"{model_name:15}: Inferred={inferred_acc:.4f}, Reported={reported_acc:.4f}, Diff={diff:.4f}\")\n\n# # Now perform Wilcoxon analysis with inferred labels\n# print(\"\\nWILCOXON ANALYSIS WITH INFERRED LABELS:\")\n# print(\"=\" * 40)\n\n# # Calculate accuracy per sample for each model\n# sample_accuracies = {}\n# for model_name, pred_result in all_test_predictions.items():\n#     sample_accuracies[model_name] = (pred_result.y_pred == inferred_labels).astype(int)\n\n# # Perform Wilcoxon tests\n# wilcoxon_results = []\n# model_names = list(all_test_predictions.keys())\n# model_pairs = list(itertools.combinations(model_names, 2))\n\n# for model1, model2 in tqdm(model_pairs, desc=\"Performing Wilcoxon tests\"):\n#     acc1 = sample_accuracies[model1]\n#     acc2 = sample_accuracies[model2]\n    \n#     # Only perform test if there are differences\n#     if not np.array_equal(acc1, acc2):\n#         try:\n#             stat, p_value = wilcoxon(acc1, acc2, zero_method='pratt')\n#             mean_diff = np.mean(acc1 - acc2)\n            \n#             wilcoxon_results.append({\n#                 'Model1': model1,\n#                 'Model2': model2,\n#                 'Wilcoxon_Statistic': stat,\n#                 'P_Value': p_value,\n#                 'Mean_Difference': mean_diff,\n#                 'Significant_0.05': p_value < 0.05,\n#                 'Significant_0.01': p_value < 0.01\n#             })\n#         except Exception as e:\n#             # If Wilcoxon fails, use mean difference only\n#             mean_diff = np.mean(acc1 - acc2)\n#             wilcoxon_results.append({\n#                 'Model1': model1,\n#                 'Model2': model2,\n#                 'Wilcoxon_Statistic': np.nan,\n#                 'P_Value': 1.0,\n#                 'Mean_Difference': mean_diff,\n#                 'Significant_0.05': False,\n#                 'Significant_0.01': False\n#             })\n#     else:\n#         # Models have identical accuracy patterns\n#         wilcoxon_results.append({\n#             'Model1': model1,\n#             'Model2': model2,\n#             'Wilcoxon_Statistic': np.nan,\n#             'P_Value': 1.0,\n#             'Mean_Difference': 0.0,\n#             'Significant_0.05': False,\n#             'Significant_0.01': False\n#         })\n\n# # Convert to DataFrame\n# wilcoxon_df = pd.DataFrame(wilcoxon_results)\n\n# # Display significant results\n# print(\"\\nSIGNIFICANT DIFFERENCES:\")\n# print(\"=\" * 25)\n\n# sig_results = wilcoxon_df[wilcoxon_df['Significant_0.05'] == True]\n# if len(sig_results) > 0:\n#     for _, row in sig_results.iterrows():\n#         better_model = row['Model1'] if row['Mean_Difference'] > 0 else row['Model2']\n#         worse_model = row['Model2'] if row['Mean_Difference'] > 0 else row['Model1']\n#         sig_level = \"**\" if row['Significant_0.01'] else \"*\"\n#         print(f\"{better_model} > {worse_model} {sig_level} (p={row['P_Value']:.4f}, diff={row['Mean_Difference']:.4f})\")\n# else:\n#     print(\"No statistically significant differences found at α=0.05\")\n\n# # Show all comparisons sorted by mean difference\n# print(\"\\nALL MODEL COMPARISONS (sorted by absolute difference):\")\n# print(\"=\" * 55)\n\n# comparison_df = wilcoxon_df.copy()\n# comparison_df['Abs_Difference'] = comparison_df['Mean_Difference'].abs()\n# comparison_df = comparison_df.sort_values('Abs_Difference', ascending=False)\n\n# for _, row in comparison_df.head(15).iterrows():\n#     better_model = row['Model1'] if row['Mean_Difference'] > 0 else row['Model2']\n#     worse_model = row['Model2'] if row['Mean_Difference'] > 0 else row['Model1']\n#     sig_mark = \"**\" if row['Significant_0.01'] else \"*\" if row['Significant_0.05'] else \"\"\n#     print(f\"{better_model} vs {worse_model}: diff={row['Mean_Difference']:.4f} {sig_mark} (p={row['P_Value']:.4f})\")\n\n# # Create a performance ranking\n# print(\"\\nMODEL PERFORMANCE RANKING:\")\n# print(\"=\" * 25)\n\n# model_performance = {}\n# for model_name in model_names:\n#     model_performance[model_name] = np.mean(sample_accuracies[model_name])\n\n# # Sort by performance\n# ranked_models = sorted(model_performance.items(), key=lambda x: x[1], reverse=True)\n# for rank, (model, perf) in enumerate(ranked_models, 1):\n#     print(f\"{rank:2d}. {model:15}: {perf:.4f}\")\n\n# # Save the inferred labels for future use\n# np.save('inferred_test_labels.npy', inferred_labels)\n# print(f\"\\nInferred labels saved to 'inferred_test_labels.npy'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:42:17.114705Z","iopub.status.idle":"2025-09-05T15:42:17.114955Z","shell.execute_reply.started":"2025-09-05T15:42:17.114859Z","shell.execute_reply":"2025-09-05T15:42:17.114868Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Fix CatBoost predictions\n# print(\"FIXING CatBoost PREDICTIONS:\")\n# catboost_preds = all_test_predictions['CatBoost'].y_pred\n# if len(catboost_preds.shape) == 2 and catboost_preds.shape[1] == 1:\n#     catboost_preds_fixed = catboost_preds.flatten()\n#     print(f\"Fixed CatBoost from shape {catboost_preds.shape} to {catboost_preds_fixed.shape}\")\n#     print(f\"Sample: {catboost_preds_fixed[:10]}\")\n    \n#     # Update the predictions\n#     all_test_predictions['CatBoost'] = PredictionResult(\n#         y_pred=catboost_preds_fixed, \n#         y_proba=all_test_predictions['CatBoost'].y_proba\n#     )\n\n# # Load the ACTUAL true test labels (you need to have these)\n# print(\"\\nUSING ACTUAL TEST LABELS:\")\n# print(\"=\" * 40)\n\n# # Replace this with how you load your actual y_test\n# # y_test = np.load('true_test_labels.npy')  # or however you access them\n# # y_test = your_data_loader_function() \n\n# # Since you don't seem to have the true labels, we'll use the reported results directly\n# print(\"Since true test labels are not available, using reported accuracies for analysis\")\n# print(\"This avoids the circular logic problem of inferring labels from predictions\")\n\n# # Create a DataFrame with the reported results\n# print(\"\\nREPORTED MODEL PERFORMANCE:\")\n# print(\"=\" * 30)\n\n# reported_results = {\n#     'Model': ['KNN', 'SVM', 'Naive Bayes', 'Decision Tree', 'RandomForest', \n#               'AdaBoost', 'XGBoost', 'LightGBM', 'CatBoost', 'TabPFN'],\n#     'Accuracy': [0.7968, 0.7908, 0.7530, 0.7390, 0.7928, 0.6514, 0.8008, 0.7968, 0.7869, 0.7829],\n#     'Precision_macro': [0.7761, 0.7893, 0.6993, 0.7062, 0.7951, 0.6439, 0.7772, 0.7758, 0.7535, 0.7641],\n#     'Recall_macro': [0.7119, 0.6982, 0.7073, 0.6713, 0.6985, 0.7022, 0.7155, 0.6989, 0.7152, 0.6982],\n#     'F1_macro': [0.7331, 0.7275, 0.7029, 0.6874, 0.7285, 0.6353, 0.7375, 0.7257, 0.7298, 0.7208]\n# }\n\n# results_df = pd.DataFrame(reported_results)\n# print(results_df.to_string(index=False))\n\n# # Performance ranking based on reported accuracy\n# print(\"\\nMODEL PERFORMANCE RANKING (Based on Reported Accuracy):\")\n# print(\"=\" * 50)\n\n# ranked_models = results_df.sort_values('Accuracy', ascending=False)\n# for rank, (_, row) in enumerate(ranked_models.iterrows(), 1):\n#     print(f\"{rank:2d}. {row['Model']:15}: {row['Accuracy']:.4f}\")\n\n# # Statistical analysis - we can't do Wilcoxon without true labels, but we can analyze patterns\n# print(\"\\nPERFORMANCE ANALYSIS:\")\n# print(\"=\" * 20)\n\n# print(\"Top Performers (Accuracy > 0.79):\")\n# top_models = results_df[results_df['Accuracy'] > 0.79]\n# print(top_models[['Model', 'Accuracy']].to_string(index=False))\n\n# print(\"\\nWeaker Performers (Accuracy < 0.75):\")\n# weak_models = results_df[results_df['Accuracy'] < 0.75]\n# print(weak_models[['Model', 'Accuracy']].to_string(index=False))\n\n# print(f\"\\nPerformance Range: {results_df['Accuracy'].min():.4f} - {results_df['Accuracy'].max():.4f}\")\n# print(f\"Average Accuracy: {results_df['Accuracy'].mean():.4f}\")\n# print(f\"Standard Deviation: {results_df['Accuracy'].std():.4f}\")\n\n# # Key insights\n# print(\"\\nKEY INSIGHTS:\")\n# print(\"=\" * 15)\n# print(\"1. XGBoost is the top performer (0.8008 accuracy)\")\n# print(\"2. Tree-based ensembles (XGBoost, LightGBM, RandomForest, CatBoost) perform well\")\n# print(\"3. AdaBoost is significantly worse than other models\")\n# print(\"4. Models are very competitive - top 6 within 2% of each other\")\n# print(\"5. Consider ensemble methods for best performance\")\n\n# # If you want to compare your predictions with reported results\n# print(\"\\nCOMPARING YOUR PREDICTIONS WITH REPORTED RESULTS:\")\n# print(\"=\" * 50)\n\n# # Calculate what the accuracy would be if your predictions matched reported results\n# # This is just for validation, not for actual performance measurement\n# model_accuracies = {}\n# for model_name in all_test_predictions.keys():\n#     # This is just to show the predicted class distribution\n#     pred_counts = np.bincount(all_test_predictions[model_name].y_pred)\n#     model_accuracies[model_name] = {\n#         'predicted_class_distribution': pred_counts,\n#         'n_predictions': len(all_test_predictions[model_name].y_pred)\n#     }\n#     print(f\"{model_name:15}: {len(pred_counts)} classes, distribution: {pred_counts}\")\n\n# print(\"\\nNote: Without true test labels, statistical tests like Wilcoxon cannot be performed\")\n# print(\"The reported results from your image are the ground truth for model performance\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:42:17.115851Z","iopub.status.idle":"2025-09-05T15:42:17.116120Z","shell.execute_reply.started":"2025-09-05T15:42:17.116006Z","shell.execute_reply":"2025-09-05T15:42:17.116019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Fix CatBoost predictions\n# print(\"FIXING CatBoost PREDICTIONS:\")\n# catboost_preds = all_test_predictions['CatBoost'].y_pred\n# if len(catboost_preds.shape) == 2 and catboost_preds.shape[1] == 1:\n#     catboost_preds_fixed = catboost_preds.flatten()\n#     print(f\"Fixed CatBoost from shape {catboost_preds.shape} to {catboost_preds_fixed.shape}\")\n#     print(f\"Sample: {catboost_preds_fixed[:10]}\")\n    \n#     # Update the predictions\n#     all_test_predictions['CatBoost'] = PredictionResult(\n#         y_pred=catboost_preds_fixed, \n#         y_proba=all_test_predictions['CatBoost'].y_proba\n#     )\n\n# # Calculate accuracy with TRUE test labels\n# print(f\"\\nACCURACY WITH TRUE TEST LABELS (y_test shape: {y_test.shape}):\")\n# print(\"=\" * 50)\n\n# # Calculate actual accuracy for each model\n# actual_accuracies = {}\n# for model_name, pred_result in all_test_predictions.items():\n#     accuracy = np.mean(pred_result.y_pred == y_test)\n#     actual_accuracies[model_name] = accuracy\n#     print(f\"{model_name:15}: {accuracy:.4f}\")\n\n# # Compare with reported accuracies\n# print(\"\\nCOMPARISON WITH REPORTED ACCURACIES:\")\n# print(\"=\" * 35)\n# reported_accuracies = {\n#     'KNN': 0.7968, 'SVM': 0.7908, 'Naive Bayes': 0.7530, \n#     'Decision Tree': 0.7390, 'RandomForest': 0.7928, 'AdaBoost': 0.6514,\n#     'XGBoost': 0.8008, 'LightGBM': 0.7968, 'CatBoost': 0.7869, 'TabPFN': 0.7829\n# }\n\n# print(f\"{'Model':15} {'Actual':8} {'Reported':8} {'Diff':8} {'Match':6}\")\n# print(\"-\" * 45)\n# for model_name in all_test_predictions.keys():\n#     actual_acc = actual_accuracies[model_name]\n#     reported_acc = reported_accuracies.get(model_name, 0)\n#     diff = abs(actual_acc - reported_acc)\n#     match = \"✓\" if abs(diff) < 0.01 else \"✗\"  # Allow small rounding differences\n#     print(f\"{model_name:15}: {actual_acc:.4f}  {reported_acc:.4f}  {diff:.4f}  {match}\")\n\n# # Now perform proper Wilcoxon analysis with TRUE labels\n# print(\"\\nWILCOXON ANALYSIS WITH TRUE LABELS:\")\n# print(\"=\" * 40)\n\n# # Calculate accuracy per sample for each model (1 if correct, 0 if wrong)\n# sample_accuracies = {}\n# for model_name, pred_result in all_test_predictions.items():\n#     sample_accuracies[model_name] = (pred_result.y_pred == y_test).astype(int)\n\n# # Perform Wilcoxon tests\n# wilcoxon_results = []\n# model_names = list(all_test_predictions.keys())\n# model_pairs = list(itertools.combinations(model_names, 2))\n\n# for model1, model2 in tqdm(model_pairs, desc=\"Performing Wilcoxon tests\"):\n#     acc1 = sample_accuracies[model1]\n#     acc2 = sample_accuracies[model2]\n    \n#     # Only perform test if there are differences\n#     if not np.array_equal(acc1, acc2):\n#         try:\n#             stat, p_value = wilcoxon(acc1, acc2, zero_method='pratt')\n#             mean_diff = np.mean(acc1 - acc2)\n            \n#             wilcoxon_results.append({\n#                 'Model1': model1,\n#                 'Model2': model2,\n#                 'Wilcoxon_Statistic': stat,\n#                 'P_Value': p_value,\n#                 'Mean_Difference': mean_diff,\n#                 'Significant_0.05': p_value < 0.05,\n#                 'Significant_0.01': p_value < 0.01\n#             })\n#         except Exception as e:\n#             # If Wilcoxon fails, use mean difference only\n#             mean_diff = np.mean(acc1 - acc2)\n#             wilcoxon_results.append({\n#                 'Model1': model1,\n#                 'Model2': model2,\n#                 'Wilcoxon_Statistic': np.nan,\n#                 'P_Value': 1.0,\n#                 'Mean_Difference': mean_diff,\n#                 'Significant_0.05': False,\n#                 'Significant_0.01': False\n#             })\n#     else:\n#         # Models have identical accuracy patterns\n#         wilcoxon_results.append({\n#             'Model1': model1,\n#             'Model2': model2,\n#             'Wilcoxon_Statistic': np.nan,\n#             'P_Value': 1.0,\n#             'Mean_Difference': 0.0,\n#             'Significant_0.05': False,\n#             'Significant_0.01': False\n#         })\n\n# # Convert to DataFrame\n# wilcoxon_df = pd.DataFrame(wilcoxon_results)\n\n# # Display significant results\n# print(\"\\nSTATISTICALLY SIGNIFICANT DIFFERENCES:\")\n# print(\"=\" * 40)\n\n# sig_results = wilcoxon_df[wilcoxon_df['Significant_0.05'] == True]\n# if len(sig_results) > 0:\n#     for _, row in sig_results.iterrows():\n#         better_model = row['Model1'] if row['Mean_Difference'] > 0 else row['Model2']\n#         worse_model = row['Model2'] if row['Mean_Difference'] > 0 else row['Model1']\n#         sig_level = \"**\" if row['Significant_0.01'] else \"*\"\n#         print(f\"{better_model} > {worse_model} {sig_level} (p={row['P_Value']:.4f}, diff={abs(row['Mean_Difference']):.4f})\")\n# else:\n#     print(\"No statistically significant differences found at α=0.05\")\n\n# # Show performance ranking\n# print(\"\\nMODEL PERFORMANCE RANKING (Based on True Labels):\")\n# print(\"=\" * 45)\n\n# # Sort by actual performance\n# ranked_models = sorted(actual_accuracies.items(), key=lambda x: x[1], reverse=True)\n# for rank, (model, perf) in enumerate(ranked_models, 1):\n#     print(f\"{rank:2d}. {model:15}: {perf:.4f}\")\n\n# # Additional performance metrics\n# print(\"\\nDETAILED PERFORMANCE ANALYSIS:\")\n# print(\"=\" * 30)\n\n# from sklearn.metrics import classification_report, confusion_matrix\n\n# print(\"Classification report for top performer:\")\n# top_model = ranked_models[0][0]\n# print(f\"\\n{top_model} Classification Report:\")\n# print(classification_report(y_test, all_test_predictions[top_model].y_pred))\n\n# # Check if results match reported values\n# print(\"\\nVALIDATION CHECK:\")\n# print(\"=\" * 20)\n# mismatches = []\n# for model_name in all_test_predictions.keys():\n#     actual = actual_accuracies[model_name]\n#     reported = reported_accuracies.get(model_name, 0)\n#     if abs(actual - reported) > 0.01:  # More than 1% difference\n#         mismatches.append((model_name, actual, reported, abs(actual - reported)))\n\n# if mismatches:\n#     print(\"Significant differences found between actual and reported accuracies:\")\n#     for model, actual, reported, diff in mismatches:\n#         print(f\"  {model}: Actual={actual:.4f}, Reported={reported:.4f}, Diff={diff:.4f}\")\n# else:\n#     print(\"All actual accuracies match reported values within 1% tolerance\")\n\n# # Save the results\n# print(f\"\\nTrue test labels shape: {y_test.shape}\")\n# print(f\"True labels unique values: {np.unique(y_test)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:42:17.117388Z","iopub.status.idle":"2025-09-05T15:42:17.117931Z","shell.execute_reply.started":"2025-09-05T15:42:17.117737Z","shell.execute_reply":"2025-09-05T15:42:17.117771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Fix CatBoost predictions if needed\n# print(\"FIXING CatBoost PREDICTIONS:\")\n# catboost_preds = all_test_predictions['CatBoost'].y_pred\n# if len(catboost_preds.shape) == 2 and catboost_preds.shape[1] == 1:\n#     catboost_preds_fixed = catboost_preds.flatten()\n#     print(f\"Fixed CatBoost from shape {catboost_preds.shape} to {catboost_preds_fixed.shape}\")\n    \n#     # Update the predictions\n#     all_test_predictions['CatBoost'] = PredictionResult(\n#         y_pred=catboost_preds_fixed, \n#         y_proba=all_test_predictions['CatBoost'].y_proba\n#     )\n\n# # Calculate comprehensive performance metrics\n# print(f\"\\nCOMPREHENSIVE PERFORMANCE METRICS (y_test shape: {y_test.shape}):\")\n# print(\"=\" * 65)\n\n# from sklearn.metrics import (accuracy_score, f1_score, precision_score, \n#                            recall_score, balanced_accuracy_score, \n#                            classification_report, confusion_matrix)\n\n# # Calculate all metrics for each model\n# performance_metrics = {}\n# for model_name, pred_result in all_test_predictions.items():\n#     y_pred = pred_result.y_pred\n    \n#     performance_metrics[model_name] = {\n#         'accuracy': accuracy_score(y_test, y_pred),\n#         'weighted_f1': f1_score(y_test, y_pred, average='weighted'),\n#         'macro_f1': f1_score(y_test, y_pred, average='macro'),\n#         'weighted_precision': precision_score(y_test, y_pred, average='weighted'),\n#         'weighted_recall': recall_score(y_test, y_pred, average='weighted'),\n#         'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n#         'micro_f1': f1_score(y_test, y_pred, average='micro')\n#     }\n\n# # Display all metrics\n# print(f\"{'Model':15} {'Acc':6} {'F1-Wtd':7} {'F1-Macro':8} {'Bal-Acc':8} {'F1-Micro':8}\")\n# print(\"-\" * 65)\n# for model_name, metrics in performance_metrics.items():\n#     print(f\"{model_name:15}: {metrics['accuracy']:.4f}  {metrics['weighted_f1']:.4f}  \"\n#           f\"{metrics['macro_f1']:.4f}    {metrics['balanced_accuracy']:.4f}    {metrics['micro_f1']:.4f}\")\n\n# # Class distribution analysis\n# print(f\"\\nCLASS DISTRIBUTION ANALYSIS:\")\n# print(\"=\" * 30)\n# unique_classes, class_counts = np.unique(y_test, return_counts=True)\n# class_info = dict(zip([f'Class_{c}' for c in unique_classes], class_counts))\n# for class_name, count in class_info.items():\n#     print(f\"{class_name}: {count} samples ({count/len(y_test)*100:.1f}%)\")\n# print(f\"Imbalance ratio: {max(class_counts)/min(class_counts):.2f}:1\")\n\n# # Performance ranking by weighted F1 (best for imbalanced data)\n# print(f\"\\nMODEL RANKING (By Weighted F1 Score):\")\n# print(\"=\" * 35)\n# ranked_models = sorted(performance_metrics.items(), \n#                       key=lambda x: x[1]['weighted_f1'], reverse=True)\n\n# for rank, (model, metrics) in enumerate(ranked_models, 1):\n#     print(f\"{rank:2d}. {model:15}: F1={metrics['weighted_f1']:.4f}, Acc={metrics['accuracy']:.4f}\")\n\n# # Wilcoxon statistical analysis\n# print(f\"\\nSTATISTICAL SIGNIFICANCE ANALYSIS (Wilcoxon):\")\n# print(\"=\" * 45)\n\n# # Calculate accuracy per sample for each model\n# sample_accuracies = {}\n# for model_name, pred_result in all_test_predictions.items():\n#     sample_accuracies[model_name] = (pred_result.y_pred == y_test).astype(int)\n\n# # Perform Wilcoxon tests\n# wilcoxon_results = []\n# model_pairs = list(itertools.combinations(performance_metrics.keys(), 2))\n\n# for model1, model2 in tqdm(model_pairs, desc=\"Performing Wilcoxon tests\"):\n#     acc1 = sample_accuracies[model1]\n#     acc2 = sample_accuracies[model2]\n    \n#     if not np.array_equal(acc1, acc2):\n#         try:\n#             stat, p_value = wilcoxon(acc1, acc2, zero_method='pratt')\n#             mean_diff = np.mean(acc1 - acc2)\n            \n#             wilcoxon_results.append({\n#                 'Model1': model1,\n#                 'Model2': model2,\n#                 'P_Value': p_value,\n#                 'Mean_Difference': mean_diff,\n#                 'Significant_0.05': p_value < 0.05,\n#                 'Significant_0.01': p_value < 0.01\n#             })\n#         except:\n#             mean_diff = np.mean(acc1 - acc2)\n#             wilcoxon_results.append({\n#                 'Model1': model1,\n#                 'Model2': model2,\n#                 'P_Value': 1.0,\n#                 'Mean_Difference': mean_diff,\n#                 'Significant_0.05': False,\n#                 'Significant_0.01': False\n#             })\n\n# # Display statistically significant differences\n# wilcoxon_df = pd.DataFrame(wilcoxon_results)\n# sig_results = wilcoxon_df[wilcoxon_df['Significant_0.05'] == True]\n\n# if len(sig_results) > 0:\n#     print(\"Statistically significant differences (α=0.05):\")\n#     for _, row in sig_results.iterrows():\n#         better_model = row['Model1'] if row['Mean_Difference'] > 0 else row['Model2']\n#         worse_model = row['Model2'] if row['Mean_Difference'] > 0 else row['Model1']\n#         sig_level = \"**\" if row['Significant_0.01'] else \"*\"\n#         f1_diff = abs(performance_metrics[better_model]['weighted_f1'] - \n#                      performance_metrics[worse_model]['weighted_f1'])\n#         print(f\"  {better_model} > {worse_model} {sig_level} (p={row['P_Value']:.4f}, ΔF1={f1_diff:.4f})\")\n# else:\n#     print(\"No statistically significant differences found\")\n\n# # Detailed analysis of top performer\n# top_model = ranked_models[0][0]\n# print(f\"\\nDETAILED ANALYSIS OF TOP PERFORMER ({top_model}):\")\n# print(\"=\" * 45)\n# print(classification_report(y_test, all_test_predictions[top_model].y_pred, \n#                            target_names=[f'Class_{c}' for c in unique_classes]))\n\n# # Confusion matrix for top model\n# print(\"Confusion Matrix:\")\n# cm = confusion_matrix(y_test, all_test_predictions[top_model].y_pred)\n# print(cm)\n\n# # Practical significance analysis\n# print(f\"\\nPRACTICAL SIGNIFICANCE (F1 differences > 0.01):\")\n# print(\"=\" * 40)\n# practical_diffs = []\n# for i in range(len(ranked_models)):\n#     for j in range(i+1, len(ranked_models)):\n#         model1, metrics1 = ranked_models[i]\n#         model2, metrics2 = ranked_models[j]\n#         f1_diff = metrics1['weighted_f1'] - metrics2['weighted_f1']\n#         if f1_diff > 0.01:  # 1% F1 difference considered practically significant\n#             practical_diffs.append((model1, model2, f1_diff))\n\n# for model1, model2, diff in sorted(practical_diffs, key=lambda x: x[2], reverse=True):\n#     print(f\"  {model1} outperforms {model2} by {diff:.4f} F1 points\")\n\n# # Final recommendations\n# print(f\"\\nFINAL RECOMMENDATIONS:\")\n# print(\"=\" * 20)\n# print(\"1. Best overall model: XGBoost (F1-weighted: 0.7916)\")\n# print(\"2. Consider model ensemble for potentially better performance\")\n# print(\"3. Address class imbalance (7.63:1 ratio) with:\")\n# print(\"   - Class weighting during training\")\n# print(\"   - Oversampling techniques for minority classes\")\n# print(\"   - Focus on improving Class_0 performance\")\n# print(\"4. Use weighted F1 as primary evaluation metric\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:42:17.118702Z","iopub.status.idle":"2025-09-05T15:42:17.119035Z","shell.execute_reply.started":"2025-09-05T15:42:17.118882Z","shell.execute_reply":"2025-09-05T15:42:17.118896Z"}},"outputs":[],"execution_count":null}]}