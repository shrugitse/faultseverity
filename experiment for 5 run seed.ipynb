{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12830626,"sourceType":"datasetVersion","datasetId":8114289},{"sourceId":12969760,"sourceType":"datasetVersion","datasetId":8126906},{"sourceId":535993,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":416714,"modelId":434433}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tabpfn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:56:09.155960Z","iopub.execute_input":"2025-09-05T15:56:09.156439Z","iopub.status.idle":"2025-09-05T15:57:22.882020Z","shell.execute_reply.started":"2025-09-05T15:56:09.156417Z","shell.execute_reply":"2025-09-05T15:57:22.881056Z"}},"outputs":[{"name":"stdout","text":"Collecting tabpfn\n  Downloading tabpfn-2.1.3-py3-none-any.whl.metadata (27 kB)\nRequirement already satisfied: torch<3,>=2.1 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn<1.7,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (1.2.2)\nRequirement already satisfied: typing_extensions>=4.12.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (4.14.0)\nRequirement already satisfied: scipy<2,>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (1.15.3)\nRequirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (2.2.3)\nRequirement already satisfied: einops<0.9,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (0.8.1)\nRequirement already satisfied: huggingface-hub<1,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (0.33.1)\nRequirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (2.11.7)\nCollecting pydantic-settings>=2.10.1 (from tabpfn)\n  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\nCollecting eval-type-backport>=0.2.2 (from tabpfn)\n  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (2025.5.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (2.32.4)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (4.67.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (1.1.5)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->tabpfn) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->tabpfn) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->tabpfn) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->tabpfn) (2025.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->tabpfn) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->tabpfn) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->tabpfn) (0.4.1)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings>=2.10.1->tabpfn)\n  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7,>=1.2.0->tabpfn) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7,>=1.2.0->tabpfn) (3.6.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.1->tabpfn)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.1->tabpfn)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.1->tabpfn)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.1->tabpfn)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.1->tabpfn)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.1->tabpfn)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.1->tabpfn)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.1->tabpfn)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.1->tabpfn)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.1->tabpfn)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.1->tabpfn) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas<3,>=1.4.0->tabpfn) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas<3,>=1.4.0->tabpfn) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas<3,>=1.4.0->tabpfn) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas<3,>=1.4.0->tabpfn) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas<3,>=1.4.0->tabpfn) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas<3,>=1.4.0->tabpfn) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->tabpfn) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.1->tabpfn) (3.0.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas<3,>=1.4.0->tabpfn) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas<3,>=1.4.0->tabpfn) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas<3,>=1.4.0->tabpfn) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas<3,>=1.4.0->tabpfn) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas<3,>=1.4.0->tabpfn) (2024.2.0)\nDownloading tabpfn-2.1.3-py3-none-any.whl (160 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.8/160.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\nDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\nInstalling collected packages: python-dotenv, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, eval-type-backport, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pydantic-settings, nvidia-cusolver-cu12, tabpfn\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed eval-type-backport-0.2.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydantic-settings-2.10.1 python-dotenv-1.1.1 tabpfn-2.1.3\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import RobertaConfig, RobertaTokenizer, RobertaModel, get_linear_schedule_with_warmup\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, matthews_corrcoef, confusion_matrix\n)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.utils.class_weight import compute_class_weight\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\nfrom tabpfn import TabPFNClassifier\nimport random\nimport logging\nfrom typing import Dict, List, Tuple, Any\nfrom dataclasses import dataclass\n\n# =============================================================================\n# STEP 1: GLOBAL CONFIGURATION AND SETUP\n# =============================================================================\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# --- Global Configuration ---\nBASE_URL = \"/kaggle/input/dataset\"\nOUTPUT_DIR = \"/kaggle/working/\"\nSYNTHETIC_DATA_SIZE = 0\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")\n\n# --- SEEDS FOR 5 RUNS ---\nSEEDS = [42, 83, 456, 789, 101]\n\ndef set_seed(seed_value):\n    \"\"\"Sets the seed for reproducibility for all relevant libraries.\"\"\"\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed_value)\n    logger.info(f\"Global seed set to {seed_value}\")\n\n# =============================================================================\n# STEP 2: DATA PREPARATION CLASSES AND FUNCTIONS\n# =============================================================================\nclass DataPreprocessor:\n    \"\"\"Handles loading and splitting of the dataset based on a given seed.\"\"\"\n    def __init__(self, base_url, synthetic_data_size, random_seed):\n        self.base_url = base_url\n        self.synthetic_data_size = synthetic_data_size\n        self.random_seed = random_seed\n\n    def prepare_datasets(self):\n        \"\"\"Loads and splits data into train, validation, and test sets.\"\"\"\n        logger.info(f\"Preparing datasets with seed: {self.random_seed}\")\n        df_org = pd.read_csv(f\"{self.base_url}/df_org.csv\")\n        df_syn = pd.read_csv(f\"{self.base_url}/df_syn.csv\")\n\n        df_syn_class0 = df_syn[df_syn['orig_label'] == 0].reset_index(drop=True)\n        if self.synthetic_data_size > 0:\n            df_syn_class0 = df_syn_class0.sample(\n                n=min(self.synthetic_data_size, len(df_syn_class0)),\n                random_state=self.random_seed\n            )\n        else:\n            df_syn_class0 = pd.DataFrame(columns=df_syn_class0.columns).rename(columns={\"synthetic_code\": \"code\", \"orig_label\": \"label\"})\n\n\n        # The rest of your data splitting logic\n        matched_codes = set(df_syn_class0[\"orig_code\"].unique()) if not df_syn_class0.empty else set()\n        df_org_match = df_org[df_org[\"code\"].isin(matched_codes)].copy()\n        df_org_nonmatch = df_org[~df_org[\"code\"].isin(matched_codes)].copy()\n\n        df_org_match[\"source\"] = \"original\"\n        df_org_nonmatch[\"source\"] = \"original\"\n\n        df_syn_renamed = df_syn_class0.rename(columns={\"synthetic_code\": \"code\", \"orig_label\": \"label\"})\n        df_syn_renamed[\"source\"] = \"synthetic\"\n\n        class0_df = df_org_nonmatch[df_org_nonmatch[\"label\"] == 0].copy()\n        nonclass0_df = df_org_nonmatch[df_org_nonmatch[\"label\"] != 0].copy()\n\n        class0_test = class0_df.sample(n=41, random_state=self.random_seed)\n        class0_val = class0_df.drop(class0_test.index).sample(n=41, random_state=self.random_seed)\n        class0_train = class0_df.drop(class0_test.index).drop(class0_val.index)\n\n        nonclass0_trainval, nonclass0_test = train_test_split(\n            nonclass0_df, test_size=0.15, random_state=self.random_seed, stratify=nonclass0_df[\"label\"]\n        )\n        nonclass0_train, nonclass0_val = train_test_split(\n            nonclass0_trainval, test_size=0.1765, random_state=self.random_seed, stratify=nonclass0_trainval[\"label\"]\n        )\n\n        train_nonmatch = pd.concat([class0_train, nonclass0_train], ignore_index=True)\n        valid_df = pd.concat([class0_val, nonclass0_val], ignore_index=True)\n        test_df = pd.concat([class0_test, nonclass0_test], ignore_index=True)\n\n        train_df = pd.concat([train_nonmatch, df_org_match, df_syn_renamed], ignore_index=True)\n        \n        logger.info(f\"Data prepared: Train size={len(train_df)}, Val size={len(valid_df)}, Test size={len(test_df)}\")\n        return train_df, valid_df, test_df\n\nclass BugSeverityDataset(Dataset):\n    \"\"\"Custom PyTorch Dataset for loading code, numerical features, and labels.\"\"\"\n    def __init__(self, data, tokenizer, block_size=512):\n        self.examples = data\n        self.tokenizer = tokenizer\n        self.block_size = block_size\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, idx):\n        js = self.examples[idx]\n        code = js[\"code\"]\n        code_tokens = self.tokenizer.tokenize(str(code))[:self.block_size - 2]\n        tokens = [self.tokenizer.cls_token] + code_tokens + [self.tokenizer.eos_token]\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n        padding_length = self.block_size - len(input_ids)\n        input_ids += [self.tokenizer.pad_token_id] * padding_length\n        \n        num_features = torch.tensor([\n            js.get(\"sloc_robust\", 0.0), js.get(\"proxy_indentation_robust\", 0.0),\n            js.get(\"mcCabe_robust\", 0.0), js.get(\"mcClure_robust\", 0.0),\n            js.get(\"nested_block_depth_robust\", 0.0), js.get(\"difficulty_robust\", 0.0),\n            js.get(\"maintainability_index_robust\", 0.0), js.get(\"fan_out_robust\", 0.0),\n            js.get(\"readability_robust\", 0.0), js.get(\"effort_robust\", 0.0)\n        ], dtype=torch.float)\n        \n        return (\n            torch.tensor(input_ids),\n            num_features,\n            torch.tensor(js[\"label\"], dtype=torch.long)\n        )\n\ndef convert_df_to_json_format(df):\n    \"\"\"Converts a DataFrame to the list of dicts format required by the Dataset.\"\"\"\n    return df.to_dict('records')\n\n# =============================================================================\n# STEP 3: PYTORCH MODEL, TRAINING, AND EVALUATION\n# =============================================================================\nclass ConcatClsModel(nn.Module):\n    \"\"\"CodeBERT encoder with a classification head for concatenated features.\"\"\"\n    def __init__(self, encoder, config):\n        super().__init__()\n        self.encoder = encoder\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.out_layer = nn.Linear(config.hidden_size + 10, config.num_labels)\n\n    def forward(self, input_ids, num_features, labels=None):\n        attention_mask = input_ids.ne(self.encoder.config.pad_token_id).long()\n        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        cls_embeds = outputs.last_hidden_state[:, 0, :]\n        concat = torch.cat((cls_embeds, num_features), dim=-1)\n        logits = self.out_layer(self.dropout(concat))\n        probs = torch.softmax(logits, dim=-1)\n\n        if labels is not None:\n            loss = nn.CrossEntropyLoss()(logits, labels)\n            return loss, probs\n        return probs\n\ndef run_pytorch_training(train_loader, valid_loader, seed, output_dir):\n    \"\"\"Runs the entire PyTorch training and evaluation process for one seed.\"\"\"\n    logger.info(f\"Starting PyTorch training for seed {seed}.\")\n    \n    # Using exact best parameters from your original script\n    best_params = {\n        'lr': 4.818976027099782e-05,\n        'weight_decay': 0.00010045918919119982,\n        'warmup_ratio': 0.28606124459699783,\n        'dropout': 0.16316509043013103,\n        'epochs': 8,\n    }\n    \n    config = RobertaConfig.from_pretrained(\"microsoft/codebert-base\", num_labels=4, hidden_dropout_prob=best_params['dropout'])\n    encoder = RobertaModel.from_pretrained(\"microsoft/codebert-base\", config=config, add_pooling_layer=False)\n    model = ConcatClsModel(encoder, config).to(DEVICE)\n\n    optimizer = AdamW(model.parameters(), lr=best_params['lr'], weight_decay=best_params['weight_decay'])\n    total_steps = len(train_loader) * best_params['epochs']\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=int(total_steps * best_params['warmup_ratio']), num_training_steps=total_steps\n    )\n    \n    best_f1 = 0\n    best_model_path = os.path.join(output_dir, f\"best_model_seed_{seed}.pt\")\n\n    for epoch in range(best_params['epochs']):\n        model.train()\n        for batch in train_loader:\n            input_ids, num_features, labels = [b.to(DEVICE) for b in batch]\n            optimizer.zero_grad()\n            loss, _ = model(input_ids, num_features, labels)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n        \n        model.eval()\n        preds, labels_all = [], []\n        with torch.no_grad():\n            for batch in valid_loader:\n                input_ids, num_features, labels = [b.to(DEVICE) for b in batch]\n                _, probs = model(input_ids, num_features, labels)\n                preds.extend(torch.argmax(probs, dim=1).cpu().tolist())\n                labels_all.extend(labels.cpu().tolist())\n        val_f1 = f1_score(labels_all, preds, average=\"macro\")\n        \n        logger.info(f\"Seed {seed} | Epoch {epoch+1}/{best_params['epochs']} | Val F1: {val_f1:.4f}\")\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            torch.save(model.state_dict(), best_model_path)\n            \n    return best_model_path\n    \n# =============================================================================\n# STEP 6: MAIN EXECUTION LOOP\n# =============================================================================\nall_runs_results = []\ntokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n\nfor seed in SEEDS:\n    try:\n        set_seed(seed)\n        \n        # --- Data Prep ---\n        preprocessor = DataPreprocessor(BASE_URL, SYNTHETIC_DATA_SIZE, seed)\n        train_df, valid_df, test_df = preprocessor.prepare_datasets()\n        \n        train_dataset = BugSeverityDataset(convert_df_to_json_format(train_df), tokenizer)\n        valid_dataset = BugSeverityDataset(convert_df_to_json_format(valid_df), tokenizer)\n        test_dataset = BugSeverityDataset(convert_df_to_json_format(test_df), tokenizer)\n        \n        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n        valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=2)\n\n        # --- PyTorch Training ---\n        best_model_path = run_pytorch_training(train_loader, valid_loader, seed, OUTPUT_DIR)\n\n    except Exception as e:\n        logger.error(f\"Run for seed {seed} failed: {e}\", exc_info=True)\n        continue","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T00:04:01.364663Z","iopub.execute_input":"2025-09-01T00:04:01.364982Z","iopub.status.idle":"2025-09-01T00:35:43.792599Z","shell.execute_reply.started":"2025-09-01T00:04:01.364957Z","shell.execute_reply":"2025-09-01T00:35:43.791432Z"}},"outputs":[{"name":"stderr","text":"2025-09-01 00:04:07.140225: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756685047.165605      91 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756685047.173374      91 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41f6d9ea1cb04346b93083ad91a21b7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37972685b8634b68b769fd6bb4cef94e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11d9de4898f842f69ef90b70d1b0c564"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca483b4d616e498bb3fe48b43bcec699"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd3c745c08144fae9d8be02eeded4d49"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_91/1788228459.py:112: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  train_df = pd.concat([train_nonmatch, df_org_match, df_syn_renamed], ignore_index=True)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d75e888b30d347aa97bdb315f3c83057"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaModel: ['pooler.dense.bias', 'pooler.dense.weight']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"979a5167c83d4fdeb23ad48a90f9b1ec"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaModel: ['pooler.dense.bias', 'pooler.dense.weight']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting embeddings:   0%|          | 0/2339 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting embeddings:   0%|          | 0/501 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting embeddings:   0%|          | 0/502 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Downstream Models (Seed 0):   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eed34cd7b36442a088bdf03ebf8c81c3"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [00:29:09] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [00:29:10] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [00:29:10] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025066 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 197500\n[LightGBM] [Info] Number of data points in the train set: 2840, number of used features: 778\n[LightGBM] [Info] Start training from score -2.496238\n[LightGBM] [Info] Start training from score -0.473390\n[LightGBM] [Info] Start training from score -2.442171\n[LightGBM] [Info] Start training from score -1.571437\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","output_type":"stream"},{"name":"stderr","text":"Warning: less than 75% GPU memory available for training. Free: 8863.125 Total: 16269.25\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tabpfn-v2-classifier-finetuned-zk73skhh.(…):   0%|          | 0.00/29.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e40742fe79b4fb4a26b0beed8769914"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/37.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16fefb6f794b4ad8ad1af41711e7fae6"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_91/1788228459.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;31m# Calculate mean and standard deviation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m     \u001b[0maggregated_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_results_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'std'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;31m# Format into \"mean ± std\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   9181\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to supply one of 'by' and 'level'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9183\u001b[0;31m         return DataFrameGroupBy(\n\u001b[0m\u001b[1;32m   9184\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9185\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgrouper\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1041\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Model'"],"ename":"KeyError","evalue":"'Model'","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import RobertaConfig, RobertaTokenizer, RobertaModel\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, matthews_corrcoef, confusion_matrix\n)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\nfrom tabpfn import TabPFNClassifier\nimport random\nimport logging\nfrom typing import Dict, List, Tuple, Any\nfrom dataclasses import dataclass\n\n# =============================================================================\n# STEP 1: GLOBAL CONFIGURATION AND SETUP\n# =============================================================================\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# --- Global Configuration ---\nBASE_URL = \"/kaggle/input/dataset\"\nOUTPUT_DIR = \"/kaggle/working/\"\nSYNTHETIC_DATA_SIZE = 0\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")\n\n# --- SEEDS AND SAVED MODELS ---\nSEEDS = [42, 83, 456, 789, 101]\nSAVED_MODEL_PATHS = {\n    42: \"/kaggle/working/best_model_seed_42.pt\",\n    83: \"/kaggle/working/best_model_seed_83.pt\", \n    456: \"/kaggle/working/best_model_seed_456.pt\",\n    789: \"/kaggle/working/best_model_seed_789.pt\",\n    101: \"/kaggle/working/best_model_seed_101.pt\"\n}\n\ndef set_seed(seed_value):\n    \"\"\"Sets the seed for reproducibility for all relevant libraries.\"\"\"\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed_value)\n    logger.info(f\"Global seed set to {seed_value}\")\n\n# =============================================================================\n# STEP 2: DATA PREPARATION CLASSES AND FUNCTIONS\n# =============================================================================\nclass DataPreprocessor:\n    \"\"\"Handles loading and splitting of the dataset based on a given seed.\"\"\"\n    def __init__(self, base_url, synthetic_data_size, random_seed):\n        self.base_url = base_url\n        self.synthetic_data_size = synthetic_data_size\n        self.random_seed = random_seed\n\n    def prepare_datasets(self):\n        \"\"\"Loads and splits data into train, validation, and test sets.\"\"\"\n        logger.info(f\"Preparing datasets with seed: {self.random_seed}\")\n        df_org = pd.read_csv(f\"{self.base_url}/df_org.csv\")\n        df_syn = pd.read_csv(f\"{self.base_url}/df_syn.csv\")\n\n        df_syn_class0 = df_syn[df_syn['orig_label'] == 0].reset_index(drop=True)\n        if self.synthetic_data_size > 0:\n            df_syn_class0 = df_syn_class0.sample(\n                n=min(self.synthetic_data_size, len(df_syn_class0)),\n                random_state=self.random_seed\n            )\n        else:\n            df_syn_class0 = pd.DataFrame(columns=df_syn_class0.columns).rename(columns={\"synthetic_code\": \"code\", \"orig_label\": \"label\"})\n\n        matched_codes = set(df_syn_class0[\"orig_code\"].unique()) if not df_syn_class0.empty else set()\n        df_org_match = df_org[df_org[\"code\"].isin(matched_codes)].copy()\n        df_org_nonmatch = df_org[~df_org[\"code\"].isin(matched_codes)].copy()\n\n        df_org_match[\"source\"] = \"original\"\n        df_org_nonmatch[\"source\"] = \"original\"\n\n        df_syn_renamed = df_syn_class0.rename(columns={\"synthetic_code\": \"code\", \"orig_label\": \"label\"})\n        df_syn_renamed[\"source\"] = \"synthetic\"\n\n        class0_df = df_org_nonmatch[df_org_nonmatch[\"label\"] == 0].copy()\n        nonclass0_df = df_org_nonmatch[df_org_nonmatch[\"label\"] != 0].copy()\n\n        class0_test = class0_df.sample(n=41, random_state=self.random_seed)\n        class0_val = class0_df.drop(class0_test.index).sample(n=41, random_state=self.random_seed)\n        class0_train = class0_df.drop(class0_test.index).drop(class0_val.index)\n\n        nonclass0_trainval, nonclass0_test = train_test_split(\n            nonclass0_df, test_size=0.15, random_state=self.random_seed, stratify=nonclass0_df[\"label\"]\n        )\n        nonclass0_train, nonclass0_val = train_test_split(\n            nonclass0_trainval, test_size=0.1765, random_state=self.random_seed, stratify=nonclass0_trainval[\"label\"]\n        )\n\n        train_nonmatch = pd.concat([class0_train, nonclass0_train], ignore_index=True)\n        valid_df = pd.concat([class0_val, nonclass0_val], ignore_index=True)\n        test_df = pd.concat([class0_test, nonclass0_test], ignore_index=True)\n\n        train_df = pd.concat([train_nonmatch, df_org_match, df_syn_renamed], ignore_index=True)\n        \n        logger.info(f\"Data prepared: Train size={len(train_df)}, Val size={len(valid_df)}, Test size={len(test_df)}\")\n        return train_df, valid_df, test_df\n\nclass BugSeverityDataset(Dataset):\n    \"\"\"Custom PyTorch Dataset for loading code, numerical features, and labels.\"\"\"\n    def __init__(self, data, tokenizer, block_size=512):\n        self.examples = data\n        self.tokenizer = tokenizer\n        self.block_size = block_size\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, idx):\n        js = self.examples[idx]\n        code = js[\"code\"]\n        code_tokens = self.tokenizer.tokenize(str(code))[:self.block_size - 2]\n        tokens = [self.tokenizer.cls_token] + code_tokens + [self.tokenizer.eos_token]\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n        padding_length = self.block_size - len(input_ids)\n        input_ids += [self.tokenizer.pad_token_id] * padding_length\n        \n        num_features = torch.tensor([\n            js.get(\"sloc_robust\", 0.0), js.get(\"proxy_indentation_robust\", 0.0),\n            js.get(\"mcCabe_robust\", 0.0), js.get(\"mcClure_robust\", 0.0),\n            js.get(\"nested_block_depth_robust\", 0.0), js.get(\"difficulty_robust\", 0.0),\n            js.get(\"maintainability_index_robust\", 0.0), js.get(\"fan_out_robust\", 0.0),\n            js.get(\"readability_robust\", 0.0), js.get(\"effort_robust\", 0.0)\n        ], dtype=torch.float)\n        \n        return (\n            torch.tensor(input_ids),\n            num_features,\n            torch.tensor(js[\"label\"], dtype=torch.long)\n        )\n\ndef convert_df_to_json_format(df):\n    \"\"\"Converts a DataFrame to the list of dicts format required by the Dataset.\"\"\"\n    return df.to_dict('records')\n\n# =============================================================================\n# STEP 3: PYTORCH MODEL\n# =============================================================================\nclass ConcatClsModel(nn.Module):\n    \"\"\"CodeBERT encoder with a classification head for concatenated features.\"\"\"\n    def __init__(self, encoder, config):\n        super().__init__()\n        self.encoder = encoder\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.out_layer = nn.Linear(config.hidden_size + 10, config.num_labels)\n\n    def forward(self, input_ids, num_features, labels=None):\n        attention_mask = input_ids.ne(self.encoder.config.pad_token_id).long()\n        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        cls_embeds = outputs.last_hidden_state[:, 0, :]\n        concat = torch.cat((cls_embeds, num_features), dim=-1)\n        logits = self.out_layer(self.dropout(concat))\n        probs = torch.softmax(logits, dim=-1)\n\n        if labels is not None:\n            loss = nn.CrossEntropyLoss()(logits, labels)\n            return loss, probs\n        return probs\n\ndef evaluate_codebert_model(model, dataloader, device, seed):\n    \"\"\"Evaluates the CodeBERT model and returns all metrics.\"\"\"\n    model.eval()\n    all_preds = []\n    all_probs = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=f\"Evaluating CodeBERT (Seed {seed})\", leave=False):\n            input_ids, num_features, labels = [b.to(device) for b in batch]\n            probs = model(input_ids, num_features)\n            preds = torch.argmax(probs, dim=1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_probs.extend(probs.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    # Calculate all metrics\n    metrics = {\n        \"Model\": \"CodeBERT\",\n        \"seed\": seed,\n        \"Accuracy\": accuracy_score(all_labels, all_preds),\n        \"Precision_macro\": precision_score(all_labels, all_preds, average='macro', zero_division=0),\n        \"Recall_macro\": recall_score(all_labels, all_preds, average='macro', zero_division=0),\n        \"F1_macro\": f1_score(all_labels, all_preds, average='macro', zero_division=0),\n        \"Precision_weighted\": precision_score(all_labels, all_preds, average='weighted', zero_division=0),\n        \"Recall_weighted\": recall_score(all_labels, all_preds, average='weighted', zero_division=0),\n        \"F1_weighted\": f1_score(all_labels, all_preds, average='weighted', zero_division=0),\n        \"ROC-AUC_macro\": roc_auc_score(all_labels, all_probs, multi_class='ovr', average='macro'),\n        \"ROC-AUC_weighted\": roc_auc_score(all_labels, all_probs, multi_class='ovr', average='weighted'),\n        \"MCC\": matthews_corrcoef(all_labels, all_preds)\n    }\n    \n    # Calculate G-Mean (geometric mean of recall for each class)\n    cm = confusion_matrix(all_labels, all_preds)\n    recalls = np.diag(cm) / np.sum(cm, axis=1)\n    recalls = recalls[~np.isnan(recalls)]  # Remove NaN values if any class has 0 samples\n    metrics[\"G-Mean\"] = np.exp(np.mean(np.log(recalls))) if len(recalls) > 0 else 0.0\n    \n    return metrics\n\n# =============================================================================\n# STEP 4: EMBEDDING EXTRACTION\n# =============================================================================\ndef extract_embeddings(model, dataset, tokenizer, device):\n    \"\"\"Extracts combined [CLS] and numerical embeddings for downstream models.\"\"\"\n    embeddings, labels = [], []\n    model.eval()\n    with torch.no_grad():\n        for item in tqdm(dataset, desc=\"Extracting embeddings\", leave=False):\n            input_ids, num_features, label = item\n            input_ids = input_ids.unsqueeze(0).to(device)\n            num_features = num_features.unsqueeze(0).to(device)\n\n            outputs = model.encoder(input_ids=input_ids, attention_mask=input_ids.ne(tokenizer.pad_token_id).long())\n            cls_embeds = outputs.last_hidden_state[:, 0, :].cpu().numpy().squeeze()\n            num_features_np = num_features.cpu().numpy().flatten()\n            \n            embeddings.append(np.concatenate([cls_embeds, num_features_np]))\n            labels.append(label.item())\n            \n    return np.array(embeddings), np.array(labels)\n\n# =============================================================================\n# STEP 5: CLASSICAL & NON-CLASSICAL MODEL TRAINING AND EVALUATION\n# =============================================================================\n@dataclass\nclass ModelConfig:\n    name: str\n    model: Any\n    params: Dict[str, Any] = None\n\ndef evaluate_predictions(y_true, y_pred, y_proba, model_name, seed):\n    \"\"\"Calculates a dictionary of metrics for a model's predictions.\"\"\"\n    metrics = {\n        \"Model\": model_name,\n        \"seed\": seed,\n        \"Accuracy\": accuracy_score(y_true, y_pred),\n        \"Precision_macro\": precision_score(y_true, y_pred, average='macro', zero_division=0),\n        \"Recall_macro\": recall_score(y_true, y_pred, average='macro', zero_division=0),\n        \"F1_macro\": f1_score(y_true, y_pred, average='macro', zero_division=0),\n        \"Precision_weighted\": precision_score(y_true, y_pred, average='weighted', zero_division=0),\n        \"Recall_weighted\": recall_score(y_true, y_pred, average='weighted', zero_division=0),\n        \"F1_weighted\": f1_score(y_true, y_pred, average='weighted', zero_division=0),\n        \"ROC-AUC_macro\": roc_auc_score(y_true, y_proba, multi_class='ovr', average='macro'),\n        \"ROC-AUC_weighted\": roc_auc_score(y_true, y_proba, multi_class='ovr', average='weighted'),\n        \"MCC\": matthews_corrcoef(y_true, y_pred)\n    }\n    \n    # Calculate G-Mean\n    cm = confusion_matrix(y_true, y_pred)\n    recalls = np.diag(cm) / np.sum(cm, axis=1)\n    recalls = recalls[~np.isnan(recalls)]\n    metrics[\"G-Mean\"] = np.exp(np.mean(np.log(recalls))) if len(recalls) > 0 else 0.0\n    \n    return metrics\n\ndef run_downstream_model_training(X_trainval, y_trainval, X_test, y_test, seed):\n    \"\"\"Trains and evaluates a suite of classical and non-classical models.\"\"\"\n    logger.info(f\"Starting downstream model training for seed {seed}.\")\n    results = []\n    \n    model_configs = [\n        ModelConfig(\"KNN\", KNeighborsClassifier(), {'n_neighbors': 3, 'weights': 'distance', 'p': 1}),\n        ModelConfig(\"SVM\", SVC(probability=True, random_state=seed), {\"C\": 2.5, \"kernel\": \"rbf\", \"gamma\": \"scale\"}),\n        ModelConfig(\"Naive Bayes\", GaussianNB(), {\"var_smoothing\": 1e-8}),\n        ModelConfig(\"Decision Tree\", DecisionTreeClassifier(random_state=seed), {\"max_depth\": 14, \"min_samples_split\": 4}),\n        ModelConfig(\"RandomForest\", RandomForestClassifier(random_state=seed, n_jobs=-1), {\"n_estimators\": 600, \"max_depth\": 18, \"min_samples_split\": 3, \"class_weight\": \"balanced_subsample\"}),\n        ModelConfig(\"AdaBoost\", AdaBoostClassifier(random_state=seed), {\"n_estimators\": 500, \"learning_rate\": 0.85}),\n        ModelConfig(\"XGBoost\", xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', seed=seed, tree_method=\"gpu_hist\"), {\n            \"max_depth\": 9, \"eta\": 0.24627429143007107, \"subsample\": 0.45321841598276075,\n            \"colsample_bytree\": 0.7227038914198726, \"lambda\": 0.06640744768945579, \"alpha\": 0.21504472646446163\n        }),\n        ModelConfig(\"LightGBM\", lgb.LGBMClassifier(random_state=seed), {\n            \"objective\": \"multiclass\", \"metric\": \"multi_logloss\", \"learning_rate\": 0.085, \"max_depth\": 7\n        }),\n        ModelConfig(\"CatBoost\", cb.CatBoostClassifier(random_seed=seed, verbose=0, task_type=\"GPU\"), {\n            \"learning_rate\": 0.093, \"depth\": 7, \"l2_leaf_reg\": 7.07, \"iterations\": 829\n        }),\n        ModelConfig(\"TabPFN\", TabPFNClassifier(device=DEVICE, ignore_pretraining_limits=True))\n    ]\n\n    for config in tqdm(model_configs, desc=f\"Training Downstream Models (Seed {seed})\"):\n        try:\n            model = config.model\n            if config.params:\n                model.set_params(**config.params)\n            \n            model.fit(X_trainval, y_trainval)\n            y_pred = model.predict(X_test)\n            y_proba = model.predict_proba(X_test)\n            \n            metrics = evaluate_predictions(y_test, y_pred, y_proba, config.name, seed)\n            results.append(metrics)\n        except Exception as e:\n            logger.error(f\"Failed to train {config.name} for seed {seed}: {e}\")\n            \n    return results\n\n# =============================================================================\n# STEP 6: MAIN EXECUTION LOOP\n# =============================================================================\nall_results = []  # This will store results for ALL models for ALL seeds\ntokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n\n# Initialize the base model architecture\nconfig = RobertaConfig.from_pretrained(\"microsoft/codebert-base\", num_labels=4)\nencoder = RobertaModel.from_pretrained(\"microsoft/codebert-base\", config=config, add_pooling_layer=False)\nbase_model = ConcatClsModel(encoder, config).to(DEVICE)\n\nfor seed in SEEDS:\n    try:\n        set_seed(seed)\n        \n        # --- Data Prep ---\n        preprocessor = DataPreprocessor(BASE_URL, SYNTHETIC_DATA_SIZE, seed)\n        train_df, valid_df, test_df = preprocessor.prepare_datasets()\n        \n        train_dataset = BugSeverityDataset(convert_df_to_json_format(train_df), tokenizer)\n        valid_dataset = BugSeverityDataset(convert_df_to_json_format(valid_df), tokenizer)\n        test_dataset = BugSeverityDataset(convert_df_to_json_format(test_df), tokenizer)\n        \n        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n\n        # --- Load Pre-trained Model ---\n        model_path = SAVED_MODEL_PATHS[seed]\n        if not os.path.exists(model_path):\n            logger.error(f\"Model not found: {model_path}. Skipping seed {seed}.\")\n            continue\n            \n        logger.info(f\"Loading saved model from {model_path} for seed {seed}.\")\n        base_model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n        model = base_model\n\n        # --- Evaluate CodeBERT Model Itself ---\n        logger.info(f\"Evaluating CodeBERT model performance for seed {seed}.\")\n        codebert_metrics = evaluate_codebert_model(model, test_loader, DEVICE, seed)\n        all_results.append(codebert_metrics)\n        logger.info(f\"CodeBERT Seed {seed} Results: Accuracy={codebert_metrics['Accuracy']:.4f}, F1_macro={codebert_metrics['F1_macro']:.4f}\")\n\n        # --- Embedding Extraction for Downstream Models ---\n        X_train, y_train = extract_embeddings(model, train_dataset, tokenizer, DEVICE)\n        X_valid, y_valid = extract_embeddings(model, valid_dataset, tokenizer, DEVICE)\n        X_test, y_test = extract_embeddings(model, test_dataset, tokenizer, DEVICE)\n        \n        X_trainval = np.vstack([X_train, X_valid])\n        y_trainval = np.concatenate([y_train, y_valid])\n\n        # --- Downstream Model Training & Evaluation ---\n        downstream_results = run_downstream_model_training(X_trainval, y_trainval, X_test, y_test, seed)\n        all_results.extend(downstream_results)\n        \n        logger.info(f\"Successfully completed evaluation for seed {seed}.\")\n\n    except Exception as e:\n        logger.error(f\"Run for seed {seed} failed: {e}\", exc_info=True)\n        continue\n\n# =============================================================================\n# STEP 7: AGGREGATE AND DISPLAY FINAL RESULTS\n# =============================================================================\nif all_results:\n    # Create a DataFrame from all results\n    final_results_df = pd.DataFrame(all_results)\n    \n    # 1. Display Individual Results for Each Seed\n    print(\"\\n\" + \"=\"*120)\n    print(\"INDIVIDUAL RESULTS FOR EACH SEED AND MODEL\")\n    print(\"=\"*120)\n    with pd.option_context('display.max_rows', None, 'display.width', None, 'display.max_columns', None):\n        print(final_results_df.round(4))\n    \n    # 2. Calculate and Display Averaged Results\n    print(\"\\n\\n\" + \"=\"*120)\n    print(\"AVERAGED RESULTS ACROSS ALL SEEDS (Mean ± Std)\")\n    print(\"=\"*120)\n    \n    # Group by model and calculate mean and std for all metrics\n    numeric_cols = final_results_df.columns.difference(['Model', 'seed'])\n    aggregated = final_results_df.groupby('Model')[numeric_cols].agg(['mean', 'std'])\n    \n    # Format results as \"mean ± std\"\n    formatted_results = pd.DataFrame(index=aggregated.index)\n    for metric in numeric_cols:\n        mean_vals = aggregated[(metric, 'mean')]\n        std_vals = aggregated[(metric, 'std')]\n        formatted_results[metric] = [f\"{m:.4f} ± {s:.4f}\" for m, s in zip(mean_vals, std_vals)]\n    \n    with pd.option_context('display.max_rows', None, 'display.width', None, 'display.max_columns', None):\n        print(formatted_results)\n    \n    # 3. Save results to files\n    final_results_df.to_csv(os.path.join(OUTPUT_DIR, \"all_seeds_individual_results.csv\"), index=False)\n    formatted_results.to_csv(os.path.join(OUTPUT_DIR, \"averaged_results_summary.csv\"))\n    \n    logger.info(f\"Results saved to {OUTPUT_DIR}\")\n    \n    # 4. Show best performing models for each metric\n    print(\"\\n\\n\" + \"=\"*80)\n    print(\"BEST PERFORMING MODELS FOR EACH METRIC (Based on Average)\")\n    print(\"=\"*80)\n    \n    best_models = {}\n    for metric in numeric_cols:\n        best_idx = aggregated[(metric, 'mean')].idxmax()\n        best_mean = aggregated.loc[best_idx, (metric, 'mean')]\n        best_std = aggregated.loc[best_idx, (metric, 'std')]\n        best_models[metric] = (best_idx, best_mean, best_std)\n        print(f\"{metric:20s}: {best_idx:15s} {best_mean:.4f} ± {best_std:.4f}\")\n\nelse:\n    logger.warning(\"No results were generated. Cannot create summary.\")\n\nprint(\"\\nEvaluation completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T04:51:01.271854Z","iopub.execute_input":"2025-08-27T04:51:01.272638Z","iopub.status.idle":"2025-08-27T05:41:55.008257Z","shell.execute_reply.started":"2025-08-27T04:51:01.272599Z","shell.execute_reply":"2025-08-27T05:41:55.007017Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaModel: ['pooler.dense.bias', 'pooler.dense.weight']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n/tmp/ipykernel_36/1199422901.py:114: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  train_df = pd.concat([train_nonmatch, df_org_match, df_syn_renamed], ignore_index=True)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating CodeBERT (Seed 42):   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting embeddings:   0%|          | 0/2339 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting embeddings:   0%|          | 0/501 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting embeddings:   0%|          | 0/502 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Downstream Models (Seed 42):   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe468f69e1a746949bdcd860f426ce66"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [04:54:57] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [04:54:58] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014589 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 197503\n[LightGBM] [Info] Number of data points in the train set: 2840, number of used features: 778\n[LightGBM] [Info] Start training from score -2.496238\n[LightGBM] [Info] Start training from score -0.473390\n[LightGBM] [Info] Start training from score -2.442171\n[LightGBM] [Info] Start training from score -1.571437\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","output_type":"stream"},{"name":"stderr","text":"Warning: less than 75% GPU memory available for training. Free: 3597.125 Total: 16269.25\n/tmp/ipykernel_36/1199422901.py:114: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  train_df = pd.concat([train_nonmatch, df_org_match, df_syn_renamed], ignore_index=True)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating CodeBERT (Seed 83):   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting embeddings:   0%|          | 0/2339 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting embeddings:   0%|          | 0/501 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting embeddings:   0%|          | 0/502 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Downstream Models (Seed 83):   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45cc882d56f74dcdbeff8a98e3b20c9a"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:05:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:05:09] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017053 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 197500\n[LightGBM] [Info] Number of data points in the train set: 2840, number of used features: 778\n[LightGBM] [Info] Start training from score -2.496238\n[LightGBM] [Info] Start training from score -0.473390\n[LightGBM] [Info] Start training from score -2.442171\n[LightGBM] [Info] Start training from score -1.571437\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","output_type":"stream"},{"name":"stderr","text":"Warning: less than 75% GPU memory available for training. Free: 3597.125 Total: 16269.25\n/tmp/ipykernel_36/1199422901.py:114: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  train_df = pd.concat([train_nonmatch, df_org_match, df_syn_renamed], ignore_index=True)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating CodeBERT (Seed 456):   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting embeddings:   0%|          | 0/2339 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting embeddings:   0%|          | 0/501 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting embeddings:   0%|          | 0/502 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Downstream Models (Seed 456):   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9830ca2020b747acbb3691022eeec478"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:15:20] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:15:21] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015065 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 197499\n[LightGBM] [Info] Number of data points in the train set: 2840, number of used features: 778\n[LightGBM] [Info] Start training from score -2.496238\n[LightGBM] [Info] Start training from score -0.473390\n[LightGBM] [Info] Start training from score -2.442171\n[LightGBM] [Info] Start training from score -1.571437\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","output_type":"stream"},{"name":"stderr","text":"Warning: less than 75% GPU memory available for training. Free: 3597.125 Total: 16269.25\n/tmp/ipykernel_36/1199422901.py:114: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  train_df = pd.concat([train_nonmatch, df_org_match, df_syn_renamed], ignore_index=True)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating CodeBERT (Seed 789):   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting embeddings:   0%|          | 0/2339 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting embeddings:   0%|          | 0/501 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting embeddings:   0%|          | 0/502 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Downstream Models (Seed 789):   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"942ccab121094e0ea415f57b3713c33a"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/1199422901.py:114: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  train_df = pd.concat([train_nonmatch, df_org_match, df_syn_renamed], ignore_index=True)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating CodeBERT (Seed 101):   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting embeddings:   0%|          | 0/2339 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting embeddings:   0%|          | 0/501 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting embeddings:   0%|          | 0/502 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Downstream Models (Seed 101):   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b1c8db69c8342ff8adae9915f72f575"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:35:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:35:39] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020257 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 197505\n[LightGBM] [Info] Number of data points in the train set: 2840, number of used features: 778\n[LightGBM] [Info] Start training from score -2.496238\n[LightGBM] [Info] Start training from score -0.473390\n[LightGBM] [Info] Start training from score -2.442171\n[LightGBM] [Info] Start training from score -1.571437\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","output_type":"stream"},{"name":"stderr","text":"Warning: less than 75% GPU memory available for training. Free: 3597.125 Total: 16269.25\n","output_type":"stream"},{"name":"stdout","text":"\n========================================================================================================================\nINDIVIDUAL RESULTS FOR EACH SEED AND MODEL\n========================================================================================================================\n            Model  seed  Accuracy  Precision_macro  Recall_macro  F1_macro  \\\n0        CodeBERT    42    0.7769           0.7348        0.7198    0.7232   \n1             KNN    42    0.7769           0.7281        0.7251    0.7246   \n2             SVM    42    0.7849           0.7487        0.7251    0.7324   \n3     Naive Bayes    42    0.7530           0.6958        0.7229    0.7073   \n4   Decision Tree    42    0.7470           0.6925        0.6904    0.6883   \n5    RandomForest    42    0.7869           0.7627        0.7210    0.7352   \n6        AdaBoost    42    0.6952           0.6430        0.6929    0.6556   \n7         XGBoost    42    0.7769           0.7386        0.7064    0.7177   \n8        LightGBM    42    0.7849           0.7537        0.7080    0.7245   \n9        CatBoost    42    0.7849           0.7479        0.7235    0.7309   \n10         TabPFN    42    0.7809           0.7401        0.7251    0.7277   \n11       CodeBERT    83    0.7689           0.7330        0.6800    0.7031   \n12            KNN    83    0.7610           0.7176        0.7045    0.7098   \n13            SVM    83    0.7689           0.7287        0.6988    0.7111   \n14    Naive Bayes    83    0.7510           0.6983        0.6956    0.6933   \n15  Decision Tree    83    0.7490           0.7156        0.6916    0.7025   \n16   RandomForest    83    0.7669           0.7303        0.6935    0.7083   \n17       AdaBoost    83    0.6355           0.6375        0.6734    0.6391   \n18        XGBoost    83    0.7669           0.7298        0.7053    0.7156   \n19       LightGBM    83    0.7749           0.7377        0.7138    0.7233   \n20       CatBoost    83    0.7729           0.7370        0.7130    0.7225   \n21         TabPFN    83    0.7669           0.7194        0.7114    0.7133   \n22       CodeBERT   456    0.7809           0.7316        0.6936    0.7090   \n23            KNN   456    0.7928           0.7615        0.7166    0.7329   \n24            SVM   456    0.7988           0.7626        0.7112    0.7267   \n25    Naive Bayes   456    0.7629           0.7072        0.7133    0.7073   \n26  Decision Tree   456    0.7729           0.7280        0.6755    0.6902   \n27   RandomForest   456    0.7948           0.7764        0.6902    0.7208   \n28       AdaBoost   456    0.6952           0.6991        0.6684    0.6729   \n29        XGBoost   456    0.7988           0.7845        0.7088    0.7341   \n30       LightGBM   456    0.8028           0.7806        0.7051    0.7316   \n31       CatBoost   456    0.7948           0.7557        0.6987    0.7207   \n32         TabPFN   456    0.7948           0.7585        0.7260    0.7376   \n33       CodeBERT   789    0.7869           0.7319        0.7542    0.7402   \n34            KNN   789    0.7988           0.7546        0.7595    0.7525   \n35            SVM   789    0.8028           0.7704        0.7473    0.7512   \n36    Naive Bayes   789    0.7749           0.7161        0.7564    0.7323   \n37  Decision Tree   789    0.7610           0.7114        0.7098    0.7091   \n38   RandomForest   789    0.7928           0.7596        0.7287    0.7380   \n39       AdaBoost   789    0.7709           0.7140        0.7073    0.7080   \n40        XGBoost   789    0.8008           0.7717        0.7367    0.7460   \n41       LightGBM   789    0.7988           0.7810        0.7279    0.7445   \n42       CatBoost   789    0.7928           0.7539        0.7336    0.7377   \n43         TabPFN   789    0.8008           0.7648        0.7587    0.7566   \n44       CodeBERT   101    0.7271           0.7227        0.6980    0.7092   \n45            KNN   101    0.7530           0.7339        0.6945    0.7107   \n46            SVM   101    0.7590           0.7527        0.6970    0.7195   \n47    Naive Bayes   101    0.7032           0.6741        0.6811    0.6755   \n48  Decision Tree   101    0.7191           0.6628        0.6578    0.6602   \n49   RandomForest   101    0.7530           0.7433        0.6934    0.7146   \n50       AdaBoost   101    0.7131           0.6755        0.6634    0.6692   \n51        XGBoost   101    0.7590           0.7471        0.7059    0.7236   \n52       LightGBM   101    0.7510           0.7390        0.6909    0.7116   \n53       CatBoost   101    0.7570           0.7407        0.7063    0.7213   \n54         TabPFN   101    0.7470           0.7179        0.7044    0.7099   \n\n    Precision_weighted  Recall_weighted  F1_weighted  ROC-AUC_macro  \\\n0               0.7676           0.7769       0.7693         0.8771   \n1               0.7699           0.7769       0.7720         0.8411   \n2               0.7767           0.7849       0.7773         0.8829   \n3               0.7523           0.7530       0.7517         0.8496   \n4               0.7429           0.7470       0.7424         0.7870   \n5               0.7782           0.7869       0.7772         0.8881   \n6               0.7212           0.6952       0.7032         0.8435   \n7               0.7682           0.7769       0.7690         0.8777   \n8               0.7763           0.7849       0.7756         0.8919   \n9               0.7763           0.7849       0.7769         0.8900   \n10              0.7724           0.7809       0.7727         0.8913   \n11              0.7602           0.7689       0.7617         0.8829   \n12              0.7567           0.7610       0.7578         0.8305   \n13              0.7618           0.7689       0.7627         0.8815   \n14              0.7554           0.7510       0.7516         0.8298   \n15              0.7435           0.7490       0.7452         0.7737   \n16              0.7618           0.7669       0.7610         0.8959   \n17              0.7058           0.6355       0.6528         0.8126   \n18              0.7615           0.7669       0.7624         0.8912   \n19              0.7696           0.7749       0.7700         0.8898   \n20              0.7677           0.7729       0.7681         0.8972   \n21              0.7625           0.7669       0.7629         0.9013   \n22              0.7722           0.7809       0.7737         0.9059   \n23              0.7860           0.7928       0.7865         0.8346   \n24              0.7907           0.7988       0.7906         0.8915   \n25              0.7705           0.7629       0.7656         0.8672   \n26              0.7705           0.7729       0.7677         0.7516   \n27              0.7878           0.7948       0.7854         0.9031   \n28              0.7286           0.6952       0.7038         0.8524   \n29              0.7954           0.7988       0.7924         0.8955   \n30              0.7959           0.8028       0.7940         0.9006   \n31              0.7866           0.7948       0.7876         0.9068   \n32              0.7876           0.7948       0.7890         0.9052   \n33              0.7826           0.7869       0.7835         0.8923   \n34              0.7921           0.7988       0.7928         0.8693   \n35              0.7953           0.8028       0.7940         0.8906   \n36              0.7780           0.7749       0.7748         0.8706   \n37              0.7553           0.7610       0.7567         0.7829   \n38              0.7859           0.7928       0.7848         0.9005   \n39              0.7640           0.7709       0.7651         0.8427   \n40              0.7944           0.8008       0.7921         0.8978   \n41              0.7932           0.7988       0.7888         0.9028   \n42              0.7853           0.7928       0.7850         0.8998   \n43              0.7935           0.8008       0.7938         0.8980   \n44              0.7298           0.7271       0.7281         0.8616   \n45              0.7470           0.7530       0.7485         0.8223   \n46              0.7497           0.7590       0.7513         0.8602   \n47              0.7117           0.7032       0.7066         0.8304   \n48              0.7151           0.7191       0.7170         0.7451   \n49              0.7438           0.7530       0.7460         0.8731   \n50              0.7157           0.7131       0.7143         0.7985   \n51              0.7512           0.7590       0.7533         0.8669   \n52              0.7435           0.7510       0.7453         0.8718   \n53              0.7515           0.7570       0.7532         0.8676   \n54              0.7387           0.7470       0.7420         0.8750   \n\n    ROC-AUC_weighted     MCC  G-Mean  \n0             0.8397  0.5862  0.6934  \n1             0.8169  0.5902  0.7044  \n2             0.8487  0.5997  0.7020  \n3             0.8035  0.5569  0.7048  \n4             0.7438  0.5348  0.6710  \n5             0.8549  0.6001  0.6947  \n6             0.7752  0.4821  0.6817  \n7             0.8445  0.5831  0.6814  \n8             0.8610  0.5957  0.6813  \n9             0.8573  0.5994  0.6994  \n10            0.8580  0.5931  0.6991  \n11            0.8548  0.5650  0.6552  \n12            0.8114  0.5605  0.6872  \n13            0.8534  0.5693  0.6749  \n14            0.8017  0.5498  0.6801  \n15            0.7359  0.5357  0.6744  \n16            0.8702  0.5647  0.6735  \n17            0.7217  0.4335  0.6664  \n18            0.8657  0.5681  0.6866  \n19            0.8672  0.5828  0.6956  \n20            0.8702  0.5791  0.6949  \n21            0.8735  0.5713  0.6900  \n22            0.8861  0.5898  0.6642  \n23            0.8203  0.6138  0.6908  \n24            0.8644  0.6244  0.6729  \n25            0.8421  0.5758  0.7027  \n26            0.7565  0.5828  0.6355  \n27            0.8816  0.6114  0.6549  \n28            0.7906  0.4825  0.6460  \n29            0.8728  0.6236  0.6825  \n30            0.8782  0.6283  0.6725  \n31            0.8821  0.6152  0.6683  \n32            0.8815  0.6194  0.7013  \n33            0.8650  0.6153  0.7359  \n34            0.8470  0.6318  0.7391  \n35            0.8611  0.6340  0.7219  \n36            0.8301  0.6005  0.7467  \n37            0.7399  0.5613  0.6936  \n38            0.8719  0.6139  0.7077  \n39            0.7637  0.5773  0.6903  \n40            0.8713  0.6289  0.7140  \n41            0.8802  0.6226  0.7043  \n42            0.8726  0.6157  0.7115  \n43            0.8720  0.6331  0.7372  \n44            0.8205  0.5044  0.6765  \n45            0.8011  0.5417  0.6714  \n46            0.8208  0.5467  0.6684  \n47            0.7915  0.4729  0.6632  \n48            0.7203  0.4883  0.6328  \n49            0.8336  0.5366  0.6664  \n50            0.7157  0.4815  0.6445  \n51            0.8355  0.5504  0.6833  \n52            0.8373  0.5344  0.6679  \n53            0.8329  0.5505  0.6829  \n54            0.8411  0.5334  0.6803  \n\n\n========================================================================================================================\nAVERAGED RESULTS ACROSS ALL SEEDS (Mean ± Std)\n========================================================================================================================\n                      Accuracy         F1_macro      F1_weighted  \\\nModel                                                              \nAdaBoost       0.7020 ± 0.0485  0.6689 ± 0.0256  0.7079 ± 0.0399   \nCatBoost       0.7805 ± 0.0157  0.7266 ± 0.0074  0.7741 ± 0.0140   \nCodeBERT       0.7681 ± 0.0238  0.7169 ± 0.0149  0.7632 ± 0.0212   \nDecision Tree  0.7498 ± 0.0200  0.6901 ± 0.0188  0.7458 ± 0.0190   \nKNN            0.7765 ± 0.0197  0.7261 ± 0.0177  0.7715 ± 0.0187   \nLightGBM       0.7825 ± 0.0208  0.7271 ± 0.0121  0.7747 ± 0.0191   \nNaive Bayes    0.7490 ± 0.0273  0.7031 ± 0.0209  0.7501 ± 0.0262   \nRandomForest   0.7789 ± 0.0182  0.7234 ± 0.0129  0.7709 ± 0.0170   \nSVM            0.7829 ± 0.0188  0.7282 ± 0.0152  0.7752 ± 0.0182   \nTabPFN         0.7781 ± 0.0218  0.7290 ± 0.0190  0.7721 ± 0.0209   \nXGBoost        0.7805 ± 0.0188  0.7274 ± 0.0126  0.7738 ± 0.0177   \n\n                        G-Mean              MCC  Precision_macro  \\\nModel                                                              \nAdaBoost       0.6658 ± 0.0206  0.4914 ± 0.0524  0.6738 ± 0.0336   \nCatBoost       0.6914 ± 0.0165  0.5920 ± 0.0276  0.7470 ± 0.0081   \nCodeBERT       0.6850 ± 0.0318  0.5722 ± 0.0419  0.7308 ± 0.0047   \nDecision Tree  0.6615 ± 0.0264  0.5406 ± 0.0354  0.7021 ± 0.0254   \nKNN            0.6986 ± 0.0255  0.5876 ± 0.0370  0.7391 ± 0.0184   \nLightGBM       0.6843 ± 0.0154  0.5928 ± 0.0377  0.7584 ± 0.0214   \nNaive Bayes    0.6995 ± 0.0314  0.5512 ± 0.0480  0.6983 ± 0.0157   \nRandomForest   0.6794 ± 0.0215  0.5853 ± 0.0336  0.7545 ± 0.0179   \nSVM            0.6880 ± 0.0231  0.5948 ± 0.0367  0.7526 ± 0.0158   \nTabPFN         0.7016 ± 0.0216  0.5900 ± 0.0396  0.7401 ± 0.0216   \nXGBoost        0.6896 ± 0.0138  0.5908 ± 0.0344  0.7544 ± 0.0230   \n\n              Precision_weighted    ROC-AUC_macro ROC-AUC_weighted  \\\nModel                                                                \nAdaBoost         0.7271 ± 0.0222  0.8299 ± 0.0231  0.7534 ± 0.0331   \nCatBoost         0.7735 ± 0.0145  0.8923 ± 0.0150  0.8630 ± 0.0190   \nCodeBERT         0.7625 ± 0.0200  0.8840 ± 0.0166  0.8532 ± 0.0249   \nDecision Tree    0.7455 ± 0.0203  0.7681 ± 0.0188  0.7393 ± 0.0131   \nKNN              0.7704 ± 0.0190  0.8396 ± 0.0180  0.8193 ± 0.0171   \nLightGBM         0.7757 ± 0.0212  0.8914 ± 0.0122  0.8648 ± 0.0173   \nNaive Bayes      0.7536 ± 0.0257  0.8495 ± 0.0194  0.8138 ± 0.0213   \nRandomForest     0.7715 ± 0.0186  0.8921 ± 0.0121  0.8624 ± 0.0187   \nSVM              0.7748 ± 0.0192  0.8813 ± 0.0126  0.8497 ± 0.0173   \nTabPFN           0.7710 ± 0.0218  0.8942 ± 0.0119  0.8652 ± 0.0159   \nXGBoost          0.7741 ± 0.0199  0.8858 ± 0.0131  0.8580 ± 0.0169   \n\n                  Recall_macro  Recall_weighted  \nModel                                            \nAdaBoost       0.6811 ± 0.0184  0.7020 ± 0.0485  \nCatBoost       0.7150 ± 0.0138  0.7805 ± 0.0157  \nCodeBERT       0.7091 ± 0.0290  0.7681 ± 0.0238  \nDecision Tree  0.6850 ± 0.0195  0.7498 ± 0.0200  \nKNN            0.7200 ± 0.0249  0.7765 ± 0.0197  \nLightGBM       0.7091 ± 0.0134  0.7825 ± 0.0208  \nNaive Bayes    0.7139 ± 0.0287  0.7490 ± 0.0273  \nRandomForest   0.7054 ± 0.0181  0.7789 ± 0.0182  \nSVM            0.7159 ± 0.0209  0.7829 ± 0.0188  \nTabPFN         0.7251 ± 0.0209  0.7781 ± 0.0218  \nXGBoost        0.7126 ± 0.0136  0.7805 ± 0.0188  \n\n\n================================================================================\nBEST PERFORMING MODELS FOR EACH METRIC (Based on Average)\n================================================================================\nAccuracy            : SVM             0.7829 ± 0.0188\nF1_macro            : TabPFN          0.7290 ± 0.0190\nF1_weighted         : SVM             0.7752 ± 0.0182\nG-Mean              : TabPFN          0.7016 ± 0.0216\nMCC                 : SVM             0.5948 ± 0.0367\nPrecision_macro     : LightGBM        0.7584 ± 0.0214\nPrecision_weighted  : LightGBM        0.7757 ± 0.0212\nROC-AUC_macro       : TabPFN          0.8942 ± 0.0119\nROC-AUC_weighted    : TabPFN          0.8652 ± 0.0159\nRecall_macro        : TabPFN          0.7251 ± 0.0209\nRecall_weighted     : SVM             0.7829 ± 0.0188\n\nEvaluation completed!\n","output_type":"stream"}],"execution_count":3}]}